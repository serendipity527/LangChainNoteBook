{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# LangChain 0.3 中的 LCEL 详细介绍",
   "id": "9768041cd9672ead"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "LCEL（LangChain Expression Language）是 LangChain 0.3 中的声明式编程框架，用于构建复杂的 AI 应用链。它基于 `Runnable` 接口，提供了强大的组合能力和优化的执行性能。\n",
    "\n",
    "LCEL 核心概念\n",
    "\n",
    "LCEL 的核心是 `Runnable` 接口，所有组件都实现了这个接口，支持：\n",
    "- **同步/异步执行**：`invoke()` 和 `ainvoke()`\n",
    "- **批量处理**：`batch()` 和 `abatch()`\n",
    "- **流式处理**：`stream()` 和 `astream()`\n",
    "- **并行执行**：`RunnableParallel`\n",
    "- **条件分支**：`RunnableBranch`"
   ],
   "id": "b812570ee8050851"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-22T10:07:41.005653Z",
     "start_time": "2025-07-22T10:07:39.811929Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "## 完整代码示例\n",
    "\n",
    "\"\"\"\n",
    "LangChain 0.3 LCEL 完整示例集合\n",
    "基于 LangChain 0.3.26 版本\n",
    "\"\"\"\n",
    "\n",
    "import asyncio\n",
    "import json\n",
    "from typing import Dict, List, Any, Optional\n",
    "from datetime import datetime\n",
    "\n",
    "# LangChain 核心组件\n",
    "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, PromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.output_parsers import StrOutputParser, JsonOutputParser\n",
    "from langchain_core.runnables import (\n",
    "    RunnablePassthrough,\n",
    "    RunnableLambda,\n",
    "    RunnableParallel,\n",
    "    RunnableBranch,\n",
    "    RunnableMap,\n",
    "    RunnableSequence\n",
    ")\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_core.chat_history import InMemoryChatMessageHistory\n",
    "from langchain_ollama import OllamaLLM, ChatOllama\n",
    "\n",
    "# 配置\n",
    "OLLAMA_BASE_URL = \"http://localhost:11434\"\n",
    "OLLAMA_MODEL = \"qwen2.5:3b\"\n",
    "\n",
    "def create_llm():\n",
    "    \"\"\"创建LLM实例\"\"\"\n",
    "    return OllamaLLM(\n",
    "        base_url=OLLAMA_BASE_URL,\n",
    "        model=OLLAMA_MODEL,\n",
    "        temperature=0.7\n",
    "    )\n",
    "\n",
    "def create_chat_llm():\n",
    "    \"\"\"创建Chat LLM实例\"\"\"\n",
    "    return ChatOllama(\n",
    "        base_url=OLLAMA_BASE_URL,\n",
    "        model=OLLAMA_MODEL,\n",
    "        temperature=0.7\n",
    "    )"
   ],
   "id": "b0fcac67b3af1b09",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 1. 基础 LCEL 链组合",
   "id": "3be2542ff4f73382"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-22T10:08:55.796441Z",
     "start_time": "2025-07-22T10:08:40.753584Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# 1. 基础 LCEL 链组合\n",
    "def basic_chain_example():\n",
    "    \"\"\"基础链组合示例\"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"1. 基础 LCEL 链组合\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    llm = create_llm()\n",
    "\n",
    "    # 创建提示模板\n",
    "    prompt = PromptTemplate.from_template(\"请用中文回答：{question}\")\n",
    "\n",
    "    # 创建输出解析器\n",
    "    output_parser = StrOutputParser()\n",
    "\n",
    "    # 使用 pipe 方法创建链\n",
    "    chain = prompt | llm | output_parser\n",
    "\n",
    "    # 调用链\n",
    "    result = chain.invoke({\"question\": \"什么是人工智能？\"})\n",
    "    print(f\"问题：什么是人工智能？\")\n",
    "    print(f\"回答：{result}\")\n",
    "\n",
    "    return chain\n",
    "basic_chain_example()\n"
   ],
   "id": "d26f82e34639b32e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "1. 基础 LCEL 链组合\n",
      "============================================================\n",
      "问题：什么是人工智能？\n",
      "回答：人工智能（Artificial Intelligence，简称AI）是指由人设计出的一套系统或算法，这套系统或算法能够模拟、扩展和增强人的智能。它让机器能够在没有人类明确编程的情况下学习、推理、解决问题以及完成各种任务。\n",
      "\n",
      "简单来说，人工智能是一种技术，通过计算机程序和算法来使机器具备理解环境并执行特定任务的能力。这些任务可以包括语音识别、图像处理、语言翻译、玩游戏、自动驾驶汽车等，甚至还能进行创造性活动如艺术创作或音乐生成。\n",
      "\n",
      "人工智能的核心在于让机器能够模仿人类的智能行为，包括学习能力、推理能力、问题解决能力和创造性的表达方式。它的发展目标是让计算机系统具备类人的认知和决策能力，以达到甚至超越人类的能力。\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, template='请用中文回答：{question}')\n",
       "| OllamaLLM(model='qwen2.5:3b', temperature=0.7, base_url='http://localhost:11434')\n",
       "| StrOutputParser()"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 2. RunnablePassthrough 使用",
   "id": "799b542518743694"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-22T10:09:46.016822Z",
     "start_time": "2025-07-22T10:09:41.863133Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "def passthrough_example():\n",
    "    \"\"\"RunnablePassthrough 示例\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"2. RunnablePassthrough 使用\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    llm = create_llm()\n",
    "\n",
    "    # 基础透传\n",
    "    def format_docs(docs):\n",
    "        return \"\\n\\n\".join([f\"文档{i+1}: {doc}\" for i, doc in enumerate(docs)])\n",
    "\n",
    "    # 创建链，保留原始输入并添加格式化文档\n",
    "    chain = (\n",
    "        RunnablePassthrough.assign(\n",
    "            formatted_docs=lambda x: format_docs(x[\"documents\"])\n",
    "        )\n",
    "        | RunnablePassthrough.assign(\n",
    "            prompt=lambda x: f\"基于以下文档回答问题：\\n{x['formatted_docs']}\\n\\n问题：{x['question']}\"\n",
    "        )\n",
    "        | (lambda x: x[\"prompt\"])\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "\n",
    "    # 测试数据\n",
    "    input_data = {\n",
    "        \"question\": \"什么是机器学习？\",\n",
    "        \"documents\": [\n",
    "            \"机器学习是人工智能的一个分支\",\n",
    "            \"它通过算法让计算机从数据中学习\",\n",
    "            \"常见的机器学习方法包括监督学习和无监督学习\"\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    result = chain.invoke(input_data)\n",
    "    print(f\"问题：{input_data['question']}\")\n",
    "    print(f\"回答：{result}\")\n",
    "\n",
    "    return chain\n",
    "passthrough_example()"
   ],
   "id": "b40eacc8dca14980",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "2. RunnablePassthrough 使用\n",
      "============================================================\n",
      "问题：什么是机器学习？\n",
      "回答：机器学习是人工智能的一个分支，它通过算法让计算机从数据中学习。常见的机器学习方法包括监督学习和无监督学习。\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RunnableAssign(mapper={\n",
       "  formatted_docs: RunnableLambda(lambda x: format_docs(x['documents']))\n",
       "})\n",
       "| RunnableAssign(mapper={\n",
       "    prompt: RunnableLambda(lambda x: f\"基于以下文档回答问题：\\n{x['formatted_docs']}\\n\\n问题：{x['question']}\")\n",
       "  })\n",
       "| RunnableLambda(...)\n",
       "| OllamaLLM(model='qwen2.5:3b', temperature=0.7, base_url='http://localhost:11434')\n",
       "| StrOutputParser()"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 3. RunnableParallel 并行处理",
   "id": "6d0df4232b263ae1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-22T10:10:33.226105Z",
     "start_time": "2025-07-22T10:10:21.428355Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def parallel_example():\n",
    "    \"\"\"RunnableParallel 并行处理示例\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"3. RunnableParallel 并行处理\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    llm = create_llm()\n",
    "\n",
    "    # 创建不同的分析提示\n",
    "    sentiment_prompt = PromptTemplate.from_template(\"分析以下文本的情感（积极/消极/中性）：{text}\")\n",
    "    topic_prompt = PromptTemplate.from_template(\"提取以下文本的主要话题：{text}\")\n",
    "    summary_prompt = PromptTemplate.from_template(\"用一句话总结以下文本：{text}\")\n",
    "\n",
    "    # 创建并行分析链\n",
    "    parallel_chain = RunnableParallel({\n",
    "        \"sentiment\": sentiment_prompt | llm | StrOutputParser(),\n",
    "        \"topic\": topic_prompt | llm | StrOutputParser(),\n",
    "        \"summary\": summary_prompt | llm | StrOutputParser(),\n",
    "        \"original\": RunnablePassthrough()\n",
    "    })\n",
    "\n",
    "    # 测试文本\n",
    "    text = \"今天天气真好，我和朋友们去公园玩了一整天，感觉非常开心和放松。\"\n",
    "\n",
    "    result = parallel_chain.invoke({\"text\": text})\n",
    "\n",
    "    print(f\"原文：{text}\")\n",
    "    print(f\"情感分析：{result['sentiment']}\")\n",
    "    print(f\"主题提取：{result['topic']}\")\n",
    "    print(f\"文本摘要：{result['summary']}\")\n",
    "\n",
    "    return parallel_chain\n",
    "parallel_example()"
   ],
   "id": "cd77a9950e23c0c3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "3. RunnableParallel 并行处理\n",
      "============================================================\n",
      "原文：今天天气真好，我和朋友们去公园玩了一整天，感觉非常开心和放松。\n",
      "情感分析：这个文本表达的是积极的情感。作者描述了美好的天气以及与朋友一起度过的一整天愉快时光，并且提到了自己感到很开心和放松的状态，这些都表明了积极的情绪体验。\n",
      "主题提取：这个文本的主要话题是关于作者和他的朋友们在公园度过的一天的愉快经历，重点在于天气良好以及他们感到高兴和放松的状态。主要话题可以总结为“户外休闲与愉悦”。\n",
      "文本摘要：今天在公园度过的一天因为好天气而格外愉快和放松。\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{\n",
       "  sentiment: PromptTemplate(input_variables=['text'], input_types={}, partial_variables={}, template='分析以下文本的情感（积极/消极/中性）：{text}')\n",
       "             | OllamaLLM(model='qwen2.5:3b', temperature=0.7, base_url='http://localhost:11434')\n",
       "             | StrOutputParser(),\n",
       "  topic: PromptTemplate(input_variables=['text'], input_types={}, partial_variables={}, template='提取以下文本的主要话题：{text}')\n",
       "         | OllamaLLM(model='qwen2.5:3b', temperature=0.7, base_url='http://localhost:11434')\n",
       "         | StrOutputParser(),\n",
       "  summary: PromptTemplate(input_variables=['text'], input_types={}, partial_variables={}, template='用一句话总结以下文本：{text}')\n",
       "           | OllamaLLM(model='qwen2.5:3b', temperature=0.7, base_url='http://localhost:11434')\n",
       "           | StrOutputParser(),\n",
       "  original: RunnablePassthrough()\n",
       "}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 4. RunnableBranch 条件分支",
   "id": "8f2196569f947267"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-22T10:11:24.597279Z",
     "start_time": "2025-07-22T10:10:55.455820Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def branch_example():\n",
    "    \"\"\"RunnableBranch 条件分支示例\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"4. RunnableBranch 条件分支\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    llm = create_llm()\n",
    "\n",
    "    # 定义条件函数\n",
    "    def is_question(x):\n",
    "        return x[\"text\"].strip().endswith(\"？\") or x[\"text\"].strip().endswith(\"?\")\n",
    "\n",
    "    def is_greeting(x):\n",
    "        greetings = [\"你好\", \"hello\", \"hi\", \"早上好\", \"晚上好\"]\n",
    "        return any(greeting in x[\"text\"].lower() for greeting in greetings)\n",
    "\n",
    "    # 创建不同的处理链\n",
    "    question_chain = PromptTemplate.from_template(\"请详细回答这个问题：{text}\") | llm\n",
    "    greeting_chain = PromptTemplate.from_template(\"友好地回应这个问候：{text}\") | llm\n",
    "    default_chain = PromptTemplate.from_template(\"请对以下内容进行评论：{text}\") | llm\n",
    "\n",
    "    # 创建分支链\n",
    "    branch_chain = RunnableBranch(\n",
    "        (is_question, question_chain),\n",
    "        (is_greeting, greeting_chain),\n",
    "        default_chain\n",
    "    ) | StrOutputParser()\n",
    "\n",
    "    # 测试不同类型的输入\n",
    "    test_inputs = [\n",
    "        {\"text\": \"你好！\"},\n",
    "        {\"text\": \"什么是深度学习？\"},\n",
    "        {\"text\": \"今天天气不错\"}\n",
    "    ]\n",
    "\n",
    "    for i, input_data in enumerate(test_inputs, 1):\n",
    "        result = branch_chain.invoke(input_data)\n",
    "        print(f\"输入{i}：{input_data['text']}\")\n",
    "        print(f\"输出{i}：{result}\\n\")\n",
    "\n",
    "    return branch_chain\n",
    "branch_example()\n"
   ],
   "id": "7f6691b06a1125b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "4. RunnableBranch 条件分支\n",
      "============================================================\n",
      "输入1：你好！\n",
      "输出1：你好！很高兴能帮助你。有什么我可以为你效劳的吗？\n",
      "\n",
      "输入2：什么是深度学习？\n",
      "输出2：深度学习是一种机器学习的子集，它允许计算机通过构建和训练多层神经网络来自动从大量数据中提取抽象特征。这些层次化的结构模仿了人脑处理信息的方式，即每个“层”可以识别越来越高级别的模式或特征。\n",
      "\n",
      "在深度学习模型中，输入的数据首先被传递到最底层的神经元，这些神经元会对数据进行初步的非线性变换，从而发现更复杂的、与目标相关的信息。然后，这个结果被传递给下一个层次的神经元，直到达到顶部，即输出层。这种结构被称为多层感知器（Multilayer Perceptron, MLP）。\n",
      "\n",
      "深度学习在训练过程中使用大量的标注数据来优化其模型参数，以提高预测准确率和泛化能力。常用的深度学习模型包括卷积神经网络（Convolutional Neural Networks, CNNs）、循环神经网络（Recurrent Neural Networks, RNNs）和生成对抗网络（Generative Adversarial Networks, GANs），其中最出名的是前两者。\n",
      "\n",
      "深度学习已经成为许多领域的关键技术，如自然语言处理、图像识别、语音识别以及预测分析等。通过使用大量的计算资源和高效的数据预处理技术来加速模型训练过程，深度学习已经在这些领域取得了显著的成果，并且正在持续发展之中。\n",
      "\n",
      "输入3：今天天气不错\n",
      "输出3：今天的天气确实不错，这通常意味着是一个适宜外出、享受户外活动的好日子。不过具体的感受还会受到其他因素的影响，比如温度、湿度和个人喜好等。总的来说，这是一个令人愉悦的天气条件。如果您有特定地点或具体的要求，请告诉我！\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RunnableBranch(branches=[(RunnableLambda(is_question), PromptTemplate(input_variables=['text'], input_types={}, partial_variables={}, template='请详细回答这个问题：{text}')\n",
       "| OllamaLLM(model='qwen2.5:3b', temperature=0.7, base_url='http://localhost:11434')), (RunnableLambda(is_greeting), PromptTemplate(input_variables=['text'], input_types={}, partial_variables={}, template='友好地回应这个问候：{text}')\n",
       "| OllamaLLM(model='qwen2.5:3b', temperature=0.7, base_url='http://localhost:11434'))], default=PromptTemplate(input_variables=['text'], input_types={}, partial_variables={}, template='请对以下内容进行评论：{text}')\n",
       "| OllamaLLM(model='qwen2.5:3b', temperature=0.7, base_url='http://localhost:11434'))\n",
       "| StrOutputParser()"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 5. RunnableLambda 自定义函数",
   "id": "761a6ec6d8622f22"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def lambda_example():\n",
    "    \"\"\"RunnableLambda 自定义函数示例\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"5. RunnableLambda 自定义函数\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    llm = create_llm()\n",
    "\n",
    "    # 自定义处理函数\n",
    "    def preprocess_text(x):\n",
    "        \"\"\"文本预处理\"\"\"\n",
    "        text = x[\"text\"]\n",
    "        # 清理文本\n",
    "        text = text.strip()\n",
    "        # 添加元数据\n",
    "        return {\n",
    "            \"processed_text\": text,\n",
    "            \"word_count\": len(text.split()),\n",
    "            \"char_count\": len(text),\n",
    "            \"timestamp\": datetime.now().isoformat()\n",
    "        }\n",
    "\n",
    "    def postprocess_result(x):\n",
    "        \"\"\"结果后处理\"\"\"\n",
    "        return {\n",
    "            \"response\": x,\n",
    "            \"processed_at\": datetime.now().isoformat(),\n",
    "            \"response_length\": len(x)\n",
    "        }\n",
    "\n",
    "    # 创建包含自定义函数的链\n",
    "    chain = (\n",
    "        RunnableLambda(preprocess_text)\n",
    "        | RunnablePassthrough.assign(\n",
    "            ai_response=lambda x: (\n",
    "                PromptTemplate.from_template(\"请分析以下文本（{word_count}词，{char_count}字符）：{processed_text}\")\n",
    "                | llm\n",
    "                | StrOutputParser()\n",
    "            ).invoke(x)\n",
    "        )\n",
    "        | RunnableLambda(lambda x: postprocess_result(x[\"ai_response\"]))\n",
    "    )\n",
    "\n",
    "    # 测试\n",
    "    input_text = {\"text\": \"人工智能正在改变我们的生活方式，从智能手机到自动驾驶汽车。\"}\n",
    "    result = chain.invoke(input_text)\n",
    "\n",
    "    print(f\"输入：{input_text['text']}\")\n",
    "    print(f\"AI回应：{result['response']}\")\n",
    "    print(f\"处理时间：{result['processed_at']}\")\n",
    "    print(f\"回应长度：{result['response_length']}字符\")\n",
    "\n",
    "    return chain\n",
    "lambda_example()"
   ],
   "id": "9adb7e218a250335",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 6. 带记忆的对话链",
   "id": "6f0f540a5c5014a2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def memory_chat_example():\n",
    "    \"\"\"带记忆的对话链示例\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"6. 带记忆的对话链\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    chat_llm = create_chat_llm()\n",
    "\n",
    "    # 创建聊天提示模板\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"你是一个友好的AI助手，能够记住之前的对话内容。\"),\n",
    "        MessagesPlaceholder(variable_name=\"history\"),\n",
    "        (\"human\", \"{input}\")\n",
    "    ])\n",
    "\n",
    "    # 创建基础链\n",
    "    chain = prompt | chat_llm | StrOutputParser()\n",
    "\n",
    "    # 创建记忆存储\n",
    "    memory = InMemoryChatMessageHistory()\n",
    "\n",
    "    # 创建带记忆的链\n",
    "    chain_with_memory = RunnableWithMessageHistory(\n",
    "        chain,\n",
    "        lambda session_id: memory,\n",
    "        input_messages_key=\"input\",\n",
    "        history_messages_key=\"history\"\n",
    "    )\n",
    "\n",
    "    # 模拟对话\n",
    "    conversations = [\n",
    "        \"我叫张三，今年25岁\",\n",
    "        \"我的爱好是什么？\",\n",
    "        \"我多大了？\",\n",
    "        \"能总结一下我们的对话吗？\"\n",
    "    ]\n",
    "\n",
    "    config = {\"configurable\": {\"session_id\": \"user_123\"}}\n",
    "\n",
    "    for i, message in enumerate(conversations, 1):\n",
    "        response = chain_with_memory.invoke(\n",
    "            {\"input\": message},\n",
    "            config=config\n",
    "        )\n",
    "        print(f\"用户{i}：{message}\")\n",
    "        print(f\"AI{i}：{response}\\n\")\n",
    "\n",
    "    return chain_with_memory"
   ],
   "id": "3c5d996866fe88bb",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 7. 复杂数据处理链",
   "id": "dfb3237e9961b2c8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def complex_data_processing():\n",
    "    \"\"\"复杂数据处理链示例\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"7. 复杂数据处理链\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    llm = create_llm()\n",
    "\n",
    "    # 模拟数据\n",
    "    data = {\n",
    "        \"users\": [\n",
    "            {\"name\": \"张三\", \"age\": 25, \"city\": \"北京\", \"interests\": [\"编程\", \"阅读\"]},\n",
    "            {\"name\": \"李四\", \"age\": 30, \"city\": \"上海\", \"interests\": [\"音乐\", \"旅行\"]},\n",
    "            {\"name\": \"王五\", \"age\": 28, \"city\": \"深圳\", \"interests\": [\"运动\", \"摄影\"]}\n",
    "        ],\n",
    "        \"query\": \"分析用户群体特征\"\n",
    "    }\n",
    "\n",
    "    # 数据处理函数\n",
    "    def analyze_users(x):\n",
    "        users = x[\"users\"]\n",
    "        total_users = len(users)\n",
    "        avg_age = sum(user[\"age\"] for user in users) / total_users\n",
    "        cities = list(set(user[\"city\"] for user in users))\n",
    "        all_interests = []\n",
    "        for user in users:\n",
    "            all_interests.extend(user[\"interests\"])\n",
    "\n",
    "        return {\n",
    "            \"total_users\": total_users,\n",
    "            \"average_age\": round(avg_age, 1),\n",
    "            \"cities\": cities,\n",
    "            \"common_interests\": list(set(all_interests)),\n",
    "            \"original_query\": x[\"query\"]\n",
    "        }\n",
    "\n",
    "    def format_analysis(x):\n",
    "        return f\"\"\"\n",
    "用户群体分析报告：\n",
    "- 总用户数：{x['total_users']}人\n",
    "- 平均年龄：{x['average_age']}岁\n",
    "- 分布城市：{', '.join(x['cities'])}\n",
    "- 兴趣爱好：{', '.join(x['common_interests'])}\n",
    "\n",
    "请基于以上数据回答：{x['original_query']}\n",
    "\"\"\"\n",
    "\n",
    "    # 创建处理链\n",
    "    processing_chain = (\n",
    "        RunnableLambda(analyze_users)\n",
    "        | RunnableLambda(format_analysis)\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "\n",
    "    result = processing_chain.invoke(data)\n",
    "    print(\"数据分析结果：\")\n",
    "    print(result)\n",
    "\n",
    "    return processing_chain"
   ],
   "id": "46f677ed3fd24199",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 8. 异步处理示例",
   "id": "d9d863aba76175f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "async def async_example():\n",
    "    \"\"\"异步处理示例\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"8. 异步处理示例\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    llm = create_llm()\n",
    "\n",
    "    # 创建异步处理链\n",
    "    prompt = PromptTemplate.from_template(\"请用中文简要回答：{question}\")\n",
    "    chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "    # 准备多个问题\n",
    "    questions = [\n",
    "        {\"question\": \"什么是机器学习？\"},\n",
    "        {\"question\": \"什么是深度学习？\"},\n",
    "        {\"question\": \"什么是自然语言处理？\"},\n",
    "        {\"question\": \"什么是计算机视觉？\"}\n",
    "    ]\n",
    "\n",
    "    print(\"开始异步处理多个问题...\")\n",
    "    start_time = datetime.now()\n",
    "\n",
    "    # 异步批量处理\n",
    "    results = await chain.abatch(questions)\n",
    "\n",
    "    end_time = datetime.now()\n",
    "    processing_time = (end_time - start_time).total_seconds()\n",
    "\n",
    "    print(f\"处理完成，耗时：{processing_time:.2f}秒\\n\")\n",
    "\n",
    "    for i, (question, result) in enumerate(zip(questions, results), 1):\n",
    "        print(f\"问题{i}：{question['question']}\")\n",
    "        print(f\"回答{i}：{result}\\n\")\n",
    "\n",
    "    return chain"
   ],
   "id": "403941e573edb342",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 9. 流式处理示例",
   "id": "63226c997facf053"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def streaming_example():\n",
    "    \"\"\"流式处理示例\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"9. 流式处理示例\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    llm = create_llm()\n",
    "\n",
    "    prompt = PromptTemplate.from_template(\"请详细解释：{topic}\")\n",
    "    chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "    print(\"开始流式生成回答...\")\n",
    "    print(\"问题：什么是人工智能？\")\n",
    "    print(\"回答：\", end=\"\", flush=True)\n",
    "\n",
    "    # 流式处理\n",
    "    for chunk in chain.stream({\"topic\": \"什么是人工智能\"}):\n",
    "        print(chunk, end=\"\", flush=True)\n",
    "\n",
    "    print(\"\\n\\n流式处理完成！\")\n",
    "\n",
    "    return chain"
   ],
   "id": "ca9ee5dd22fb97cb",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 10. JSON 输出解析示例",
   "id": "f28e6b115bbe78aa"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def json_output_example():\n",
    "    \"\"\"JSON输出解析示例\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"10. JSON输出解析示例\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    llm = create_llm()\n",
    "\n",
    "    # 创建JSON输出解析器\n",
    "    json_parser = JsonOutputParser()\n",
    "\n",
    "    # 创建提示模板，要求JSON格式输出\n",
    "    prompt = PromptTemplate.from_template(\"\"\"\n",
    "请分析以下文本并以JSON格式返回结果：\n",
    "文本：{text}\n",
    "\n",
    "请返回包含以下字段的JSON：\n",
    "- sentiment: 情感（positive/negative/neutral）\n",
    "- topics: 主要话题列表\n",
    "- summary: 一句话总结\n",
    "- word_count: 词数\n",
    "\n",
    "JSON格式：\n",
    "\"\"\")\n",
    "\n",
    "    # 创建链\n",
    "    chain = prompt | llm | json_parser\n",
    "\n",
    "    # 测试文本\n",
    "    test_text = \"今天的会议非常成功，我们讨论了新产品的开发计划和市场策略，团队成员都很积极参与。\"\n",
    "\n",
    "    try:\n",
    "        result = chain.invoke({\"text\": test_text})\n",
    "        print(f\"输入文本：{test_text}\")\n",
    "        print(\"JSON分析结果：\")\n",
    "        print(json.dumps(result, ensure_ascii=False, indent=2))\n",
    "    except Exception as e:\n",
    "        print(f\"JSON解析失败：{e}\")\n",
    "        # 降级处理\n",
    "        simple_chain = prompt | llm | StrOutputParser()\n",
    "        result = simple_chain.invoke({\"text\": test_text})\n",
    "        print(f\"文本结果：{result}\")\n",
    "\n",
    "    return chain"
   ],
   "id": "f8aa710fd4597b6",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "# ============================================================================\n",
    "# 主函数\n",
    "# ============================================================================\n",
    "\n",
    "def main():\n",
    "    \"\"\"运行所有示例\"\"\"\n",
    "    print(\"LangChain 0.3 LCEL 完整示例集合\")\n",
    "    print(\"基于 LangChain 0.3.26 版本\")\n",
    "    print(\"确保 Ollama 服务正在运行：http://localhost:11434\")\n",
    "\n",
    "    try:\n",
    "        # 运行所有同步示例\n",
    "        basic_chain_example()\n",
    "        passthrough_example()\n",
    "        parallel_example()\n",
    "        branch_example()\n",
    "        lambda_example()\n",
    "        memory_chat_example()\n",
    "        complex_data_processing()\n",
    "        streaming_example()\n",
    "        json_output_example()\n",
    "\n",
    "        # 运行异步示例\n",
    "        print(\"\\n开始运行异步示例...\")\n",
    "        asyncio.run(async_example())\n",
    "\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"所有示例运行完成！\")\n",
    "        print(\"=\" * 60)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"运行出错：{e}\")\n",
    "        print(\"请确保：\")\n",
    "        print(\"1. Ollama 服务正在运行\")\n",
    "        print(\"2. qwen2.5:3b 模型已下载\")\n",
    "        print(\"3. 网络连接正常\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ],
   "id": "c1539e699a47b62b",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 总结\n",
    "\n",
    "LCEL 是 LangChain 0.3 中的核心特性，提供了：\n",
    "\n",
    "1. **声明式编程**：简洁的链式语法\n",
    "2. **强大组合能力**：灵活的组件组合\n",
    "3. **性能优化**：自动并行化和批处理\n",
    "4. **流式支持**：实时响应能力\n",
    "5. **调试友好**：清晰的执行流程\n",
    "\n",
    "**选择建议**：\n",
    "- 简单应用：使用基础链组合\n",
    "- 复杂逻辑：使用 `RunnableBranch` 和 `RunnableParallel`\n",
    "- 高性能需求：利用批处理和并行特性\n",
    "- 复杂状态管理：考虑升级到 LangGraph\n",
    "\n",
    "所有示例代码都基于 LangChain 0.3.26 版本，确保与您的环境兼容。\n"
   ],
   "id": "19a72de9d062c3a4"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
