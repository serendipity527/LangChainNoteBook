{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# LangChain 0.3 中的 Chains（链）概念详解\n",
    "> 在 LangChain 0.3 中，Chains（链）是一种将多个组件（如 LLM、提示模板、记忆等）串联起来形成完整应用流程的方式。虽然 LangChain 0.3 更推荐使用 LCEL（LangChain Expression Language）构建应用，但传统的 Chain 类仍然被支持。\n",
    "\n",
    "传统 Chain 的主要类型\n",
    "根据提供的文档，LangChain 0.3 中的传统 Chain 主要包括：\n",
    "1. LLMChain - 最基础的链，结合提示模板和 LLM\n",
    "2. ConversationChain - 带有记忆功能的对话链\n",
    "3. ConversationalRetrievalQAChain - 用于与文档进行对话的链\n",
    "4. StuffDocumentsChain - 将多个文档合并到一个提示中\n",
    "5. MapReduceDocumentsChain - 先对每个文档单独处理，再合并结果\n",
    "6. RefineDocumentsChain - 通过逐步细化答案处理多个文档\n",
    "7. SqlDatabaseChain - 通过生成和执行 SQL 查询回答问题\n",
    "8. MultiPromptChain - 在多个提示之间路由输入"
   ],
   "id": "df7da42600484c05"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 示例 1: 基础 LLMChain",
   "id": "25e1ee0254bdd103"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-07-22T09:45:22.261661Z",
     "start_time": "2025-07-22T09:45:10.849601Z"
    }
   },
   "source": [
    "# 安装依赖\n",
    "# pip install langchain langchain-ollama\n",
    "\n",
    "from langchain_ollama import OllamaLLM\n",
    "from langchain.chains import LLMChain\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "# 初始化 LLM\n",
    "llm = OllamaLLM(model=\"qwen2.5:3b\")\n",
    "\n",
    "# 创建提示模板\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"topic\"],\n",
    "    template=\"请用简单的语言解释{topic}是什么？\"\n",
    ")\n",
    "\n",
    "# 创建 LLMChain\n",
    "chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=prompt,\n",
    "    # verbose=True  # 显示执行过程\n",
    ")\n",
    "\n",
    "# 运行链\n",
    "result = chain.run(topic=\"量子计算\")\n",
    "print(f\"结果: {result}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "结果: 量子计算是一种基于量子力学原理的计算方式，利用了量子位（也叫量子比特或qubit）的独特性质来进行信息处理。在经典计算机中，信息是以0和1两种状态来表示的；而在量子计算机中，量子位可以同时处于0和1的状态，这是由于量子力学中的叠加态特性。\n",
      "\n",
      "另外，还有一个特性叫做纠缠，意味着两个或多个量子位之间存在着一种特殊的关联，即使它们被分开很远。在量子计算中，可以通过处理这些叠加态和纠缠态来执行一些经典计算机无法完成的任务，尤其是那些涉及大量数据的复杂问题求解任务。\n",
      "\n",
      "简单来说，量子计算利用了量子力学的奇妙现象来存储和操作信息，这可能会让计算机的速度和效率大大提高。\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 示例 2: ConversationChain 实现对话记忆",
   "id": "f1a20e7d78167d0c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-22T09:50:35.775993Z",
     "start_time": "2025-07-22T09:50:32.145297Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_ollama import OllamaLLM\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "# 初始化 LLM\n",
    "llm = OllamaLLM(model=\"qwen2.5:3b\",num_predict=10)\n",
    "\n",
    "# 创建记忆\n",
    "memory = ConversationBufferMemory()\n",
    "\n",
    "# 创建对话链\n",
    "conversation = ConversationChain(\n",
    "    llm=llm,\n",
    "    memory=memory,\n",
    "    # verbose=True\n",
    ")\n",
    "\n",
    "# 多轮对话测试\n",
    "print(\"对话测试:\")\n",
    "response1 = conversation.predict(input=\"你好，我叫张三，是一名程序员\")\n",
    "print(f\"AI: {response1}\")\n",
    "\n",
    "response2 = conversation.predict(input=\"你还记得我的名字和职业吗？\")\n",
    "print(f\"AI: {response2}\")\n",
    "\n",
    "# 查看记忆内容\n",
    "print(f\"\\n记忆内容: {memory.buffer}\")"
   ],
   "id": "ba0469fe85f710f6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "对话测试:\n",
      "AI: 你好张先生，很高兴认识你！作为你的助\n",
      "AI: 当然记得，您叫张三，是一名程序员\n",
      "\n",
      "记忆内容: Human: 你好，我叫张三，是一名程序员\n",
      "AI: 你好张先生，很高兴认识你！作为你的助\n",
      "Human: 你还记得我的名字和职业吗？\n",
      "AI: 当然记得，您叫张三，是一名程序员\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 示例 3: 使用不同类型的记忆",
   "id": "cbdae343f53a7e23"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from langchain_ollama import OllamaLLM\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import (\n",
    "    ConversationBufferWindowMemory,\n",
    "    ConversationSummaryMemory\n",
    ")\n",
    "\n",
    "# 初始化 LLM\n",
    "llm = OllamaLLM(model=\"llama2\")\n",
    "\n",
    "# 1. 窗口记忆 - 只保留最近k轮对话\n",
    "print(\"=\" * 50)\n",
    "print(\"窗口记忆 (只保留最近2轮对话)\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "window_memory = ConversationBufferWindowMemory(k=2)\n",
    "window_chain = ConversationChain(\n",
    "    llm=llm,\n",
    "    memory=window_memory,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# 测试多轮对话\n",
    "conversations = [\n",
    "    \"我叫李四\",\n",
    "    \"我住在北京\",\n",
    "    \"我是医生\",\n",
    "    \"我的名字是什么？\",  # 应该记不住，因为超出窗口\n",
    "    \"我的职业是什么？\"  # 应该记得住\n",
    "]\n",
    "\n",
    "for i, msg in enumerate(conversations, 1):\n",
    "    print(f\"\\n第{i}轮对话:\")\n",
    "    response = window_chain.predict(input=msg)\n",
    "    print(f\"人类: {msg}\")\n",
    "    print(f\"AI: {response}\")\n",
    "\n",
    "# 2. 摘要记忆 - 自动总结对话历史\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"摘要记忆\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "summary_memory = ConversationSummaryMemory(llm=llm)\n",
    "summary_chain = ConversationChain(\n",
    "    llm=llm,\n",
    "    memory=summary_memory,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# 测试对话\n",
    "print(\"\\n摘要记忆测试:\")\n",
    "response1 = summary_chain.predict(input=\"我是王五，来自上海\")\n",
    "print(f\"AI: {response1}\")\n",
    "\n",
    "response2 = summary_chain.predict(input=\"我喜欢人工智能和机器学习\")\n",
    "print(f\"AI: {response2}\")\n",
    "\n",
    "response3 = summary_chain.predict(input=\"你能告诉我我来自哪里吗？\")\n",
    "print(f\"AI: {response3}\")\n",
    "\n",
    "# 查看摘要内容\n",
    "print(f\"\\n摘要内容: {summary_memory.moving_summary_buffer}\")"
   ],
   "id": "b123bdec68212a91"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 示例 4: 自定义提示模板的 LLMChain",
   "id": "4c25345acf2b9410"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from langchain_ollama import OllamaLLM\n",
    "from langchain.chains import LLMChain\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "# 初始化 LLM\n",
    "llm = OllamaLLM(model=\"llama2\")\n",
    "\n",
    "# 创建自定义提示模板\n",
    "custom_prompt = PromptTemplate(\n",
    "    input_variables=[\"history\", \"input\"],\n",
    "    template=\"\"\"你是一个友好的AI助手，能够记住对话历史。\n",
    "\n",
    "对话历史:\n",
    "{history}\n",
    "\n",
    "人类: {input}\n",
    "AI助手:\"\"\"\n",
    ")\n",
    "\n",
    "# 创建记忆\n",
    "custom_memory = ConversationBufferMemory(memory_key=\"history\")\n",
    "\n",
    "# 创建LLM链\n",
    "custom_chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=custom_prompt,\n",
    "    memory=custom_memory,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# 测试对话\n",
    "print(\"\\n测试对话:\")\n",
    "response1 = custom_chain.invoke(\"我喜欢看科幻电影，特别是《星际穿越》\")\n",
    "print(f\"AI: {response1['text']}\")\n",
    "\n",
    "response2 = custom_chain.invoke(\"我刚才提到了什么电影？\")\n",
    "print(f\"AI: {response2['text']}\")"
   ],
   "id": "a9b1fcb5435997d2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 示例 5: 基于令牌数量的记忆限制",
   "id": "110cfc6936027b0c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from langchain_ollama import OllamaLLM\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationTokenBufferMemory\n",
    "\n",
    "# 初始化 LLM\n",
    "llm = OllamaLLM(model=\"llama2\")\n",
    "\n",
    "# 创建基于令牌的记忆 (限制最大令牌数)\n",
    "token_memory = ConversationTokenBufferMemory(\n",
    "    llm=llm,\n",
    "    max_token_limit=100  # 设置最大令牌数\n",
    ")\n",
    "\n",
    "# 创建对话链\n",
    "token_chain = ConversationChain(\n",
    "    llm=llm,\n",
    "    memory=token_memory,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# 测试对话\n",
    "print(\"\\n基于令牌的记忆测试:\")\n",
    "response1 = token_chain.predict(input=\"我是一名软件工程师，专门从事人工智能和机器学习的研究与开发\")\n",
    "print(f\"AI: {response1}\")\n",
    "\n",
    "response2 = token_chain.predict(input=\"我目前在研究大语言模型的应用，特别关注对话系统的优化\")\n",
    "print(f\"AI: {response2}\")\n",
    "\n",
    "response3 = token_chain.predict(input=\"你知道我的专业领域吗？\")\n",
    "print(f\"AI: {response3}\")"
   ],
   "id": "4413f03ee24dd067"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 示例 6: 使用 StuffDocumentsChain 处理多个文档",
   "id": "bbac326b5308b131"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-22T09:56:23.718890Z",
     "start_time": "2025-07-22T09:56:11.127362Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_ollama import OllamaLLM\n",
    "from langchain.chains import StuffDocumentsChain, LLMChain\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "# 初始化 LLM\n",
    "llm = OllamaLLM(model=\"qwen2.5:3b\")\n",
    "\n",
    "# 创建文档提示模板\n",
    "document_prompt = PromptTemplate(\n",
    "    input_variables=[\"page_content\"],\n",
    "    template=\"{page_content}\"\n",
    ")\n",
    "\n",
    "# 创建提示模板\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=\"\"\"根据以下文档内容回答问题:\n",
    "\n",
    "{context}\n",
    "\n",
    "问题: {question}\n",
    "回答:\"\"\"\n",
    ")\n",
    "\n",
    "# 创建 LLM 链\n",
    "llm_chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "# 创建 StuffDocumentsChain\n",
    "stuff_chain = StuffDocumentsChain(\n",
    "    llm_chain=llm_chain,\n",
    "    document_variable_name=\"context\",\n",
    "    document_prompt=document_prompt,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# 创建示例文档\n",
    "documents = [\n",
    "    Document(page_content=\"Python是一种高级编程语言，以简洁、易读的语法著称。\"),\n",
    "    Document(page_content=\"Python支持多种编程范式，包括面向对象、命令式和函数式编程。\"),\n",
    "    Document(page_content=\"Python由Guido van Rossum于1991年创建，现在由Python软件基金会管理。\")\n",
    "]\n",
    "\n",
    "# 运行链\n",
    "result = stuff_chain.invoke({\n",
    "    \"question\": \"Python是什么语言？它有什么特点？\",\n",
    "    \"input_documents\": documents\n",
    "})\n",
    "\n",
    "print(f\"结果: {result['output_text']}\")"
   ],
   "id": "f122e5466f084ca9",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8k/msfr41dj559byywk1kcjpm9h0000gn/T/ipykernel_2402/413710473.py:30: LangChainDeprecationWarning: This class is deprecated. Use the `create_stuff_documents_chain` constructor instead. See migration guide here: https://python.langchain.com/docs/versions/migrating_chains/stuff_docs_chain/\n",
      "  stuff_chain = StuffDocumentsChain(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001B[1m> Entering new StuffDocumentsChain chain...\u001B[0m\n",
      "\n",
      "\u001B[1m> Finished chain.\u001B[0m\n",
      "结果: Python是一种高级编程语言。它的主要特点包括：\n",
      "\n",
      "1. 简洁：Python的语法设计简洁明了，这使得代码更易读。\n",
      "2. 易读性：Python语法易于理解和阅读，这对于快速编写和理解程序非常重要。\n",
      "3. 支持多种编程范式：Python兼容面向对象、命令式以及函数式编程风格。\n",
      "4. 由Guido van Rossum于1991年创建，并且现在由Python软件基金会进行管理。\n",
      "\n",
      "以上就是Python的特点。\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 示例 7: 使用 MapReduceDocumentsChain 处理大量文档",
   "id": "422e2e9308fe70b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from langchain_ollama import OllamaLLM\n",
    "from langchain.chains import MapReduceDocumentsChain, LLMChain, ReduceDocumentsChain\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "# 初始化 LLM\n",
    "llm = OllamaLLM(model=\"llama2\")\n",
    "\n",
    "# Map 提示模板 - 处理单个文档\n",
    "map_template = \"\"\"以下是一段关于Python的文本:\n",
    "{document}\n",
    "\n",
    "提取这段文本中关于Python的重要信息，并简洁总结:\"\"\"\n",
    "map_prompt = PromptTemplate.from_template(map_template)\n",
    "map_chain = LLMChain(llm=llm, prompt=map_prompt)\n",
    "\n",
    "# Reduce 提示模板 - 合并多个文档的结果\n",
    "reduce_template = \"\"\"以下是多段关于Python的总结:\n",
    "{doc_summaries}\n",
    "\n",
    "将这些信息整合成一个连贯的段落，回答问题: {question}\n",
    "最终答案:\"\"\"\n",
    "reduce_prompt = PromptTemplate.from_template(reduce_template)\n",
    "reduce_chain = LLMChain(llm=llm, prompt=reduce_prompt)\n",
    "\n",
    "# 创建 ReduceDocumentsChain\n",
    "reduce_documents_chain = ReduceDocumentsChain(\n",
    "    combine_documents_chain=reduce_chain,\n",
    "    collapse_documents_chain=reduce_chain,\n",
    "    token_max=4000,\n",
    ")\n",
    "\n",
    "# 创建 MapReduceDocumentsChain\n",
    "map_reduce_chain = MapReduceDocumentsChain(\n",
    "    llm_chain=map_chain,\n",
    "    reduce_documents_chain=reduce_documents_chain,\n",
    "    document_variable_name=\"document\",\n",
    "    return_intermediate_steps=True,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# 创建示例文档\n",
    "documents = [\n",
    "    Document(page_content=\"Python是一种高级编程语言，以简洁、易读的语法著称。\"),\n",
    "    Document(page_content=\"Python支持多种编程范式，包括面向对象、命令式和函数式编程。\"),\n",
    "    Document(page_content=\"Python由Guido van Rossum于1991年创建，现在由Python软件基金会管理。\"),\n",
    "    Document(page_content=\"Python的设计哲学强调代码的可读性和简洁的语法，使开发者能够用更少的代码表达想法。\"),\n",
    "    Document(page_content=\"Python有丰富的标准库，被称为'自带电池'的语言。\")\n",
    "]\n",
    "\n",
    "# 运行链\n",
    "result = map_reduce_chain.invoke({\n",
    "    \"input_documents\": documents,\n",
    "    \"question\": \"Python是什么，它有哪些主要特点？\"\n",
    "})\n",
    "\n",
    "print(\"\\n最终结果:\")\n",
    "print(result[\"output_text\"])\n",
    "\n",
    "print(\"\\n中间步骤:\")\n",
    "for i, step in enumerate(result[\"intermediate_steps\"], 1):\n",
    "    print(f\"步骤 {i}: {step}\")"
   ],
   "id": "c1bab1feaae23c72"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 示例 8: 使用 RefineDocumentsChain 逐步细化答案",
   "id": "71b835149a06957e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-22T09:57:56.286403Z",
     "start_time": "2025-07-22T09:57:56.160892Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_ollama import OllamaLLM\n",
    "from langchain.chains import RefineDocumentsChain, LLMChain\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "# 初始化 LLM\n",
    "llm = OllamaLLM(model=\"qwen2.5:3b\")\n",
    "\n",
    "# 初始提示模板 - 基于第一个文档生成初始答案\n",
    "initial_prompt = PromptTemplate.from_template(\n",
    "    \"根据以下信息回答问题: {question}\\n\\n{document}\\n\\n回答:\"\n",
    ")\n",
    "initial_chain = LLMChain(llm=llm, prompt=initial_prompt)\n",
    "\n",
    "# 细化提示模板 - 基于更多文档细化答案\n",
    "refine_prompt = PromptTemplate.from_template(\n",
    "    \"\"\"根据原始问题和之前的答案，结合新的文档内容，改进你的答案。\n",
    "\n",
    "问题: {question}\n",
    "之前的答案: {existing_answer}\n",
    "新文档内容: {document}\n",
    "\n",
    "改进后的答案:\"\"\"\n",
    ")\n",
    "refine_chain = LLMChain(llm=llm, prompt=refine_prompt)\n",
    "\n",
    "# 创建 RefineDocumentsChain\n",
    "refine_documents_chain = RefineDocumentsChain(\n",
    "    initial_llm_chain=initial_chain,\n",
    "    refine_llm_chain=refine_chain,\n",
    "    document_variable_name=\"document\",\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# 创建示例文档\n",
    "documents = [\n",
    "    Document(page_content=\"机器学习是人工智能的一个子领域，专注于开发能够从数据中学习的算法。\"),\n",
    "    Document(page_content=\"监督学习是机器学习的一种类型，其中模型从带标签的训练数据中学习。\"),\n",
    "    Document(page_content=\"无监督学习是另一种机器学习类型，模型从没有标签的数据中发现模式。\"),\n",
    "    Document(page_content=\"深度学习是机器学习的一个子集，使用多层神经网络处理复杂任务。\"),\n",
    "    Document(page_content=\"强化学习是一种机器学习方法，智能体通过与环境交互来学习最佳行为。\")\n",
    "]\n",
    "\n",
    "# 运行链\n",
    "result = refine_documents_chain.invoke({\n",
    "    \"input_documents\": documents,\n",
    "    \"question\": \"什么是机器学习及其主要类型？\"\n",
    "})\n",
    "\n",
    "print(f\"最终结果: {result['output_text']}\")"
   ],
   "id": "264c701d5508923f",
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for RefineDocumentsChain\ninitial_response_name\n  Field required [type=missing, input_value={'initial_llm_chain': LLM...ument', 'verbose': True}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.11/v/missing",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mValidationError\u001B[39m                           Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[9]\u001B[39m\u001B[32m, line 28\u001B[39m\n\u001B[32m     25\u001B[39m refine_chain = LLMChain(llm=llm, prompt=refine_prompt)\n\u001B[32m     27\u001B[39m \u001B[38;5;66;03m# 创建 RefineDocumentsChain\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m28\u001B[39m refine_documents_chain = \u001B[43mRefineDocumentsChain\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m     29\u001B[39m \u001B[43m    \u001B[49m\u001B[43minitial_llm_chain\u001B[49m\u001B[43m=\u001B[49m\u001B[43minitial_chain\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     30\u001B[39m \u001B[43m    \u001B[49m\u001B[43mrefine_llm_chain\u001B[49m\u001B[43m=\u001B[49m\u001B[43mrefine_chain\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     31\u001B[39m \u001B[43m    \u001B[49m\u001B[43mdocument_variable_name\u001B[49m\u001B[43m=\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mdocument\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m     32\u001B[39m \u001B[43m    \u001B[49m\u001B[43mverbose\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\n\u001B[32m     33\u001B[39m \u001B[43m)\u001B[49m\n\u001B[32m     35\u001B[39m \u001B[38;5;66;03m# 创建示例文档\u001B[39;00m\n\u001B[32m     36\u001B[39m documents = [\n\u001B[32m     37\u001B[39m     Document(page_content=\u001B[33m\"\u001B[39m\u001B[33m机器学习是人工智能的一个子领域，专注于开发能够从数据中学习的算法。\u001B[39m\u001B[33m\"\u001B[39m),\n\u001B[32m     38\u001B[39m     Document(page_content=\u001B[33m\"\u001B[39m\u001B[33m监督学习是机器学习的一种类型，其中模型从带标签的训练数据中学习。\u001B[39m\u001B[33m\"\u001B[39m),\n\u001B[32m   (...)\u001B[39m\u001B[32m     41\u001B[39m     Document(page_content=\u001B[33m\"\u001B[39m\u001B[33m强化学习是一种机器学习方法，智能体通过与环境交互来学习最佳行为。\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m     42\u001B[39m ]\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/FastAPIProject/.venv/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:222\u001B[39m, in \u001B[36mdeprecated.<locals>.deprecate.<locals>.finalize.<locals>.warn_if_direct_instance\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m    220\u001B[39m     warned = \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[32m    221\u001B[39m     emit_warning()\n\u001B[32m--> \u001B[39m\u001B[32m222\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mwrapped\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/FastAPIProject/.venv/lib/python3.11/site-packages/langchain_core/load/serializable.py:130\u001B[39m, in \u001B[36mSerializable.__init__\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m    128\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, *args: Any, **kwargs: Any) -> \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m    129\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"\"\"\"\u001B[39;00m  \u001B[38;5;66;03m# noqa: D419\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m130\u001B[39m     \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m.\u001B[49m\u001B[34;43m__init__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/FastAPIProject/.venv/lib/python3.11/site-packages/pydantic/main.py:253\u001B[39m, in \u001B[36mBaseModel.__init__\u001B[39m\u001B[34m(self, **data)\u001B[39m\n\u001B[32m    251\u001B[39m \u001B[38;5;66;03m# `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\u001B[39;00m\n\u001B[32m    252\u001B[39m __tracebackhide__ = \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m253\u001B[39m validated_self = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m__pydantic_validator__\u001B[49m\u001B[43m.\u001B[49m\u001B[43mvalidate_python\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mself_instance\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[32m    254\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m validated_self:\n\u001B[32m    255\u001B[39m     warnings.warn(\n\u001B[32m    256\u001B[39m         \u001B[33m'\u001B[39m\u001B[33mA custom validator is returning a value other than `self`.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m'\u001B[39m\n\u001B[32m    257\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mReturning anything other than `self` from a top level model validator isn\u001B[39m\u001B[33m'\u001B[39m\u001B[33mt supported when validating via `__init__`.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m\n\u001B[32m    258\u001B[39m         \u001B[33m'\u001B[39m\u001B[33mSee the `model_validator` docs (https://docs.pydantic.dev/latest/concepts/validators/#model-validators) for more details.\u001B[39m\u001B[33m'\u001B[39m,\n\u001B[32m    259\u001B[39m         stacklevel=\u001B[32m2\u001B[39m,\n\u001B[32m    260\u001B[39m     )\n",
      "\u001B[31mValidationError\u001B[39m: 1 validation error for RefineDocumentsChain\ninitial_response_name\n  Field required [type=missing, input_value={'initial_llm_chain': LLM...ument', 'verbose': True}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.11/v/missing"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 示例 9: 使用 ConstitutionalChain 实现有原则的回答",
   "id": "8a96e92080dd7aaa"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-22T09:59:55.134336Z",
     "start_time": "2025-07-22T09:58:44.960307Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_ollama import OllamaLLM\n",
    "from langchain.chains import ConstitutionalChain, LLMChain\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "# 初始化 LLM\n",
    "llm = OllamaLLM(model=\"qwen2.5:3b\")\n",
    "\n",
    "# 创建基础链\n",
    "prompt = PromptTemplate(\n",
    "    template=\"请回答以下问题: {question}\",\n",
    "    input_variables=[\"question\"]\n",
    ")\n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "# 定义宪法原则\n",
    "constitutional_principles = [\n",
    "    {\n",
    "        \"name\": \"科学准确性\",\n",
    "        \"critique_request\": \"评估回答是否基于科学事实，避免误导性信息。\",\n",
    "        \"revision_request\": \"修改回答，确保科学准确性，避免任何误导性陈述。\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"平衡观点\",\n",
    "        \"critique_request\": \"评估回答是否提供了平衡的观点，避免偏见。\",\n",
    "        \"revision_request\": \"修改回答，确保提供平衡的观点，考虑不同角度。\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# 创建宪法链\n",
    "constitutional_chain = ConstitutionalChain.from_llm(\n",
    "    chain=chain,\n",
    "    constitutional_principles=constitutional_principles,\n",
    "    llm=llm,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# 测试链\n",
    "question = \"全球变暖是真实的吗？\"\n",
    "result = constitutional_chain.invoke({\"question\": question})\n",
    "\n",
    "print(f\"最终回答: {result['output']}\")"
   ],
   "id": "e1a0940a0ba58206",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001B[1m> Entering new ConstitutionalChain chain...\u001B[0m\n",
      "\u001B[33;1m\u001B[1;3mInitial response: 全球变暖是一个被广泛科学研究和证实的现象。自工业革命以来，人类活动尤其是化石燃料的燃烧（如煤炭、石油和天然气）大大增加了大气中的温室气体浓度，特别是二氧化碳。这导致地球表面平均温度在过去一个世纪中显著上升，从而引发了全球气候系统的变化。\n",
      "\n",
      "科学家们通过各种数据来源和模型分析得出了这一结论，包括冰芯样本、海床沉积物以及长期的气温观测记录等。这些证据共同表明了人类活动对气候变化的影响，并且已经引起了全球范围内的环境变化，如极端天气事件增多、极地冰盖融化加速及海平面上升等。\n",
      "\n",
      "尽管存在一些关于气候模型预测精确性的讨论或不确定性，但全球变暖的整体趋势和主要驱动力是被广泛认可的。面对这一挑战，国际社会正在积极寻求减少温室气体排放以及适应气候变化的方法，以保护地球环境并保障未来世代的生活质量。\n",
      "\n",
      "\u001B[0m\n",
      "\u001B[1m> Finished chain.\u001B[0m\n",
      "最终回答: 全球变暖是一个被广泛科学研究和证实的现象。自工业革命以来，人类活动尤其是化石燃料的燃烧（如煤炭、石油和天然气）大大增加了大气中的温室气体浓度，特别是二氧化碳。这导致地球表面平均温度在过去一个世纪中显著上升，从而引发了全球气候系统的变化。\n",
      "\n",
      "科学家们通过各种数据来源和模型分析得出了这一结论，包括冰芯样本、海床沉积物以及长期的气温观测记录等。这些证据共同表明了人类活动对气候变化的影响，并且已经引起了全球范围内的环境变化，如极端天气事件增多、极地冰盖融化加速及海平面上升等。\n",
      "\n",
      "尽管存在一些关于气候模型预测精确性的讨论或不确定性，但全球变暖的整体趋势和主要驱动力是被广泛认可的。面对这一挑战，国际社会正在积极寻求减少温室气体排放以及适应气候变化的方法，以保护地球环境并保障未来世代的生活质量。\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 示例 10: 使用 MultiPromptChain 路由到不同专家",
   "id": "bfbb5244c5bf6737"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from langchain_ollama import OllamaLLM\n",
    "from langchain.chains import MultiPromptChain\n",
    "from langchain.chains.router.llm_router import LLMRouterChain, RouterOutputParser\n",
    "from langchain.chains.router.multi_prompt_prompt import MULTI_PROMPT_ROUTER_TEMPLATE\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "# 初始化 LLM\n",
    "llm = OllamaLLM(model=\"qwen2.5:3b\")\n",
    "\n",
    "# 定义不同领域的提示模板\n",
    "prompt_infos = [\n",
    "    {\n",
    "        \"name\": \"编程\",\n",
    "        \"description\": \"处理编程和软件开发相关问题\",\n",
    "        \"prompt_template\": \"你是一位编程专家。请回答以下编程问题:\\n\\n{input}\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"历史\",\n",
    "        \"description\": \"处理历史相关问题\",\n",
    "        \"prompt_template\": \"你是一位历史学家。请回答以下历史问题:\\n\\n{input}\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"科学\",\n",
    "        \"description\": \"处理科学相关问题\",\n",
    "        \"prompt_template\": \"你是一位科学家。请回答以下科学问题:\\n\\n{input}\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# 创建路由提示\n",
    "router_template = MULTI_PROMPT_ROUTER_TEMPLATE.format(\n",
    "    destinations=\", \".join([p[\"name\"] for p in prompt_infos]),\n",
    "    destinations_and_descriptions=\"\\n\".join(\n",
    "        [f\"{p['name']}: {p['description']}\" for p in prompt_infos]\n",
    "    )\n",
    ")\n",
    "router_prompt = PromptTemplate(\n",
    "    template=router_template,\n",
    "    input_variables=[\"input\"],\n",
    "    output_parser=RouterOutputParser()\n",
    ")\n",
    "\n",
    "# 创建路由链\n",
    "router_chain = LLMRouterChain.from_llm(llm, router_prompt)\n",
    "\n",
    "# 创建目标链\n",
    "destination_chains = {}\n",
    "for p_info in prompt_infos:\n",
    "    prompt = PromptTemplate(template=p_info[\"prompt_template\"], input_variables=[\"input\"])\n",
    "    destination_chains[p_info[\"name\"]] = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "# 创建默认链\n",
    "default_prompt = PromptTemplate(\n",
    "    template=\"你是一位通用知识专家。请回答以下问题:\\n\\n{input}\",\n",
    "    input_variables=[\"input\"]\n",
    ")\n",
    "default_chain = LLMChain(llm=llm, prompt=default_prompt)\n",
    "\n",
    "# 创建 MultiPromptChain\n",
    "multi_prompt_chain = MultiPromptChain(\n",
    "    router_chain=router_chain,\n",
    "    destination_chains=destination_chains,\n",
    "    default_chain=default_chain,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# 测试不同类型的问题\n",
    "questions = [\n",
    "    \"Python中如何创建一个列表？\",\n",
    "    \"第二次世界大战是什么时候开始的？\",\n",
    "    \"光速是多少？\",\n",
    "    \"如何做一道美味的披萨？\"  # 应该走默认链\n",
    "]\n",
    "\n",
    "for i, question in enumerate(questions, 1):\n",
    "    print(f\"\\n问题 {i}: {question}\")\n",
    "    result = multi_prompt_chain.invoke({\"input\": question})\n",
    "    print(f\"回答: {result['text']}\")"
   ],
   "id": "1d775ed32556560d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 总结\n",
    "\n",
    "## 传统 Chain 的优缺点\n",
    "\n",
    "### 优点\n",
    "1. **结构清晰** - 预定义的链结构使得应用逻辑清晰\n",
    "2. **易于使用** - 对于简单场景，使用现成的链可以快速构建应用\n",
    "3. **内置功能** - 许多链内置了特定功能，如记忆管理、文档处理等\n",
    "\n",
    "### 缺点\n",
    "1. **灵活性有限** - 相比 LCEL，传统链的自定义能力较弱\n",
    "2. **可观察性较差** - 传统链的调试和监控能力不如 LCEL\n",
    "3. **组合性较弱** - 难以像 LCEL 那样灵活组合不同组件\n",
    "\n",
    "## 选择建议\n",
    "\n",
    "1. **简单应用**: 使用 `LLMChain` 或 `ConversationChain`\n",
    "2. **内存限制**: 使用带有 `ConversationBufferWindowMemory` 或 `ConversationTokenBufferMemory` 的链\n",
    "3. **长对话**: 使用带有 `ConversationSummaryMemory` 或 `ConversationSummaryBufferMemory` 的链\n",
    "4. **文档处理**: 根据文档数量和大小选择 `StuffDocumentsChain`、`MapReduceDocumentsChain` 或 `RefineDocumentsChain`\n",
    "5. **复杂应用**: 考虑使用 LCEL 或 LangGraph 代替传统链\n",
    "\n",
    "虽然 LangChain 0.3 更推荐使用 LCEL 构建应用，但传统链仍然是快速构建简单应用的有效工具。对于更复杂的应用，建议考虑使用 LCEL 或 LangGraph。\n"
   ],
   "id": "ec3ce74966e30106"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
