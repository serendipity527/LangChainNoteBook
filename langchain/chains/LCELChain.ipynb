{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# LangChain 0.3 中的 LCEL 详细介绍",
   "id": "9768041cd9672ead"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "LCEL（LangChain Expression Language）是 LangChain 0.3 中的声明式编程框架，用于构建复杂的 AI 应用链。它基于 `Runnable` 接口，提供了强大的组合能力和优化的执行性能。\n",
    "\n",
    "LCEL 核心概念\n",
    "\n",
    "LCEL 的核心是 `Runnable` 接口，所有组件都实现了这个接口，支持：\n",
    "- **同步/异步执行**：`invoke()` 和 `ainvoke()`\n",
    "- **批量处理**：`batch()` 和 `abatch()`\n",
    "- **流式处理**：`stream()` 和 `astream()`\n",
    "- **并行执行**：`RunnableParallel`\n",
    "- **条件分支**：`RunnableBranch`"
   ],
   "id": "b812570ee8050851"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-22T10:07:41.005653Z",
     "start_time": "2025-07-22T10:07:39.811929Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "## 完整代码示例\n",
    "\n",
    "\"\"\"\n",
    "LangChain 0.3 LCEL 完整示例集合\n",
    "基于 LangChain 0.3.26 版本\n",
    "\"\"\"\n",
    "\n",
    "import asyncio\n",
    "import json\n",
    "from typing import Dict, List, Any, Optional\n",
    "from datetime import datetime\n",
    "\n",
    "# LangChain 核心组件\n",
    "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, PromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.output_parsers import StrOutputParser, JsonOutputParser\n",
    "from langchain_core.runnables import (\n",
    "    RunnablePassthrough,\n",
    "    RunnableLambda,\n",
    "    RunnableParallel,\n",
    "    RunnableBranch,\n",
    "    RunnableMap,\n",
    "    RunnableSequence\n",
    ")\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_core.chat_history import InMemoryChatMessageHistory\n",
    "from langchain_ollama import OllamaLLM, ChatOllama\n",
    "\n",
    "# 配置\n",
    "OLLAMA_BASE_URL = \"http://localhost:11434\"\n",
    "OLLAMA_MODEL = \"qwen2.5:3b\"\n",
    "\n",
    "def create_llm():\n",
    "    \"\"\"创建LLM实例\"\"\"\n",
    "    return OllamaLLM(\n",
    "        base_url=OLLAMA_BASE_URL,\n",
    "        model=OLLAMA_MODEL,\n",
    "        temperature=0.7\n",
    "    )\n",
    "\n",
    "def create_chat_llm():\n",
    "    \"\"\"创建Chat LLM实例\"\"\"\n",
    "    return ChatOllama(\n",
    "        base_url=OLLAMA_BASE_URL,\n",
    "        model=OLLAMA_MODEL,\n",
    "        temperature=0.7\n",
    "    )"
   ],
   "id": "b0fcac67b3af1b09",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 1. 基础 LCEL 链组合",
   "id": "3be2542ff4f73382"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-22T10:08:55.796441Z",
     "start_time": "2025-07-22T10:08:40.753584Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# 1. 基础 LCEL 链组合\n",
    "def basic_chain_example():\n",
    "    \"\"\"基础链组合示例\"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"1. 基础 LCEL 链组合\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    llm = create_llm()\n",
    "\n",
    "    # 创建提示模板\n",
    "    prompt = PromptTemplate.from_template(\"请用中文回答：{question}\")\n",
    "\n",
    "    # 创建输出解析器\n",
    "    output_parser = StrOutputParser()\n",
    "\n",
    "    # 使用 pipe 方法创建链\n",
    "    chain = prompt | llm | output_parser\n",
    "\n",
    "    # 调用链\n",
    "    result = chain.invoke({\"question\": \"什么是人工智能？\"})\n",
    "    print(f\"问题：什么是人工智能？\")\n",
    "    print(f\"回答：{result}\")\n",
    "\n",
    "    return chain\n",
    "basic_chain_example()\n"
   ],
   "id": "d26f82e34639b32e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "1. 基础 LCEL 链组合\n",
      "============================================================\n",
      "问题：什么是人工智能？\n",
      "回答：人工智能（Artificial Intelligence，简称AI）是指由人设计出的一套系统或算法，这套系统或算法能够模拟、扩展和增强人的智能。它让机器能够在没有人类明确编程的情况下学习、推理、解决问题以及完成各种任务。\n",
      "\n",
      "简单来说，人工智能是一种技术，通过计算机程序和算法来使机器具备理解环境并执行特定任务的能力。这些任务可以包括语音识别、图像处理、语言翻译、玩游戏、自动驾驶汽车等，甚至还能进行创造性活动如艺术创作或音乐生成。\n",
      "\n",
      "人工智能的核心在于让机器能够模仿人类的智能行为，包括学习能力、推理能力、问题解决能力和创造性的表达方式。它的发展目标是让计算机系统具备类人的认知和决策能力，以达到甚至超越人类的能力。\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, template='请用中文回答：{question}')\n",
       "| OllamaLLM(model='qwen2.5:3b', temperature=0.7, base_url='http://localhost:11434')\n",
       "| StrOutputParser()"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 2. RunnablePassthrough 使用",
   "id": "799b542518743694"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-22T10:09:46.016822Z",
     "start_time": "2025-07-22T10:09:41.863133Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "def passthrough_example():\n",
    "    \"\"\"RunnablePassthrough 示例\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"2. RunnablePassthrough 使用\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    llm = create_llm()\n",
    "\n",
    "    # 基础透传\n",
    "    def format_docs(docs):\n",
    "        return \"\\n\\n\".join([f\"文档{i+1}: {doc}\" for i, doc in enumerate(docs)])\n",
    "\n",
    "    # 创建链，保留原始输入并添加格式化文档\n",
    "    chain = (\n",
    "        RunnablePassthrough.assign(\n",
    "            formatted_docs=lambda x: format_docs(x[\"documents\"])\n",
    "        )\n",
    "        | RunnablePassthrough.assign(\n",
    "            prompt=lambda x: f\"基于以下文档回答问题：\\n{x['formatted_docs']}\\n\\n问题：{x['question']}\"\n",
    "        )\n",
    "        | (lambda x: x[\"prompt\"])\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "\n",
    "    # 测试数据\n",
    "    input_data = {\n",
    "        \"question\": \"什么是机器学习？\",\n",
    "        \"documents\": [\n",
    "            \"机器学习是人工智能的一个分支\",\n",
    "            \"它通过算法让计算机从数据中学习\",\n",
    "            \"常见的机器学习方法包括监督学习和无监督学习\"\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    result = chain.invoke(input_data)\n",
    "    print(f\"问题：{input_data['question']}\")\n",
    "    print(f\"回答：{result}\")\n",
    "\n",
    "    return chain\n",
    "passthrough_example()"
   ],
   "id": "b40eacc8dca14980",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "2. RunnablePassthrough 使用\n",
      "============================================================\n",
      "问题：什么是机器学习？\n",
      "回答：机器学习是人工智能的一个分支，它通过算法让计算机从数据中学习。常见的机器学习方法包括监督学习和无监督学习。\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RunnableAssign(mapper={\n",
       "  formatted_docs: RunnableLambda(lambda x: format_docs(x['documents']))\n",
       "})\n",
       "| RunnableAssign(mapper={\n",
       "    prompt: RunnableLambda(lambda x: f\"基于以下文档回答问题：\\n{x['formatted_docs']}\\n\\n问题：{x['question']}\")\n",
       "  })\n",
       "| RunnableLambda(...)\n",
       "| OllamaLLM(model='qwen2.5:3b', temperature=0.7, base_url='http://localhost:11434')\n",
       "| StrOutputParser()"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 3. RunnableParallel 并行处理",
   "id": "6d0df4232b263ae1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-22T10:10:33.226105Z",
     "start_time": "2025-07-22T10:10:21.428355Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def parallel_example():\n",
    "    \"\"\"RunnableParallel 并行处理示例\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"3. RunnableParallel 并行处理\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    llm = create_llm()\n",
    "\n",
    "    # 创建不同的分析提示\n",
    "    sentiment_prompt = PromptTemplate.from_template(\"分析以下文本的情感（积极/消极/中性）：{text}\")\n",
    "    topic_prompt = PromptTemplate.from_template(\"提取以下文本的主要话题：{text}\")\n",
    "    summary_prompt = PromptTemplate.from_template(\"用一句话总结以下文本：{text}\")\n",
    "\n",
    "    # 创建并行分析链\n",
    "    parallel_chain = RunnableParallel({\n",
    "        \"sentiment\": sentiment_prompt | llm | StrOutputParser(),\n",
    "        \"topic\": topic_prompt | llm | StrOutputParser(),\n",
    "        \"summary\": summary_prompt | llm | StrOutputParser(),\n",
    "        \"original\": RunnablePassthrough()\n",
    "    })\n",
    "\n",
    "    # 测试文本\n",
    "    text = \"今天天气真好，我和朋友们去公园玩了一整天，感觉非常开心和放松。\"\n",
    "\n",
    "    result = parallel_chain.invoke({\"text\": text})\n",
    "\n",
    "    print(f\"原文：{text}\")\n",
    "    print(f\"情感分析：{result['sentiment']}\")\n",
    "    print(f\"主题提取：{result['topic']}\")\n",
    "    print(f\"文本摘要：{result['summary']}\")\n",
    "\n",
    "    return parallel_chain\n",
    "parallel_example()"
   ],
   "id": "cd77a9950e23c0c3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "3. RunnableParallel 并行处理\n",
      "============================================================\n",
      "原文：今天天气真好，我和朋友们去公园玩了一整天，感觉非常开心和放松。\n",
      "情感分析：这个文本表达的是积极的情感。作者描述了美好的天气以及与朋友一起度过的一整天愉快时光，并且提到了自己感到很开心和放松的状态，这些都表明了积极的情绪体验。\n",
      "主题提取：这个文本的主要话题是关于作者和他的朋友们在公园度过的一天的愉快经历，重点在于天气良好以及他们感到高兴和放松的状态。主要话题可以总结为“户外休闲与愉悦”。\n",
      "文本摘要：今天在公园度过的一天因为好天气而格外愉快和放松。\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{\n",
       "  sentiment: PromptTemplate(input_variables=['text'], input_types={}, partial_variables={}, template='分析以下文本的情感（积极/消极/中性）：{text}')\n",
       "             | OllamaLLM(model='qwen2.5:3b', temperature=0.7, base_url='http://localhost:11434')\n",
       "             | StrOutputParser(),\n",
       "  topic: PromptTemplate(input_variables=['text'], input_types={}, partial_variables={}, template='提取以下文本的主要话题：{text}')\n",
       "         | OllamaLLM(model='qwen2.5:3b', temperature=0.7, base_url='http://localhost:11434')\n",
       "         | StrOutputParser(),\n",
       "  summary: PromptTemplate(input_variables=['text'], input_types={}, partial_variables={}, template='用一句话总结以下文本：{text}')\n",
       "           | OllamaLLM(model='qwen2.5:3b', temperature=0.7, base_url='http://localhost:11434')\n",
       "           | StrOutputParser(),\n",
       "  original: RunnablePassthrough()\n",
       "}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 4. RunnableBranch 条件分支",
   "id": "8f2196569f947267"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-22T10:11:24.597279Z",
     "start_time": "2025-07-22T10:10:55.455820Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def branch_example():\n",
    "    \"\"\"RunnableBranch 条件分支示例\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"4. RunnableBranch 条件分支\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    llm = create_llm()\n",
    "\n",
    "    # 定义条件函数\n",
    "    def is_question(x):\n",
    "        return x[\"text\"].strip().endswith(\"？\") or x[\"text\"].strip().endswith(\"?\")\n",
    "\n",
    "    def is_greeting(x):\n",
    "        greetings = [\"你好\", \"hello\", \"hi\", \"早上好\", \"晚上好\"]\n",
    "        return any(greeting in x[\"text\"].lower() for greeting in greetings)\n",
    "\n",
    "    # 创建不同的处理链\n",
    "    question_chain = PromptTemplate.from_template(\"请详细回答这个问题：{text}\") | llm\n",
    "    greeting_chain = PromptTemplate.from_template(\"友好地回应这个问候：{text}\") | llm\n",
    "    default_chain = PromptTemplate.from_template(\"请对以下内容进行评论：{text}\") | llm\n",
    "\n",
    "    # 创建分支链\n",
    "    branch_chain = RunnableBranch(\n",
    "        (is_question, question_chain),\n",
    "        (is_greeting, greeting_chain),\n",
    "        default_chain\n",
    "    ) | StrOutputParser()\n",
    "\n",
    "    # 测试不同类型的输入\n",
    "    test_inputs = [\n",
    "        {\"text\": \"你好！\"},\n",
    "        {\"text\": \"什么是深度学习？\"},\n",
    "        {\"text\": \"今天天气不错\"}\n",
    "    ]\n",
    "\n",
    "    for i, input_data in enumerate(test_inputs, 1):\n",
    "        result = branch_chain.invoke(input_data)\n",
    "        print(f\"输入{i}：{input_data['text']}\")\n",
    "        print(f\"输出{i}：{result}\\n\")\n",
    "\n",
    "    return branch_chain\n",
    "branch_example()\n"
   ],
   "id": "7f6691b06a1125b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "4. RunnableBranch 条件分支\n",
      "============================================================\n",
      "输入1：你好！\n",
      "输出1：你好！很高兴能帮助你。有什么我可以为你效劳的吗？\n",
      "\n",
      "输入2：什么是深度学习？\n",
      "输出2：深度学习是一种机器学习的子集，它允许计算机通过构建和训练多层神经网络来自动从大量数据中提取抽象特征。这些层次化的结构模仿了人脑处理信息的方式，即每个“层”可以识别越来越高级别的模式或特征。\n",
      "\n",
      "在深度学习模型中，输入的数据首先被传递到最底层的神经元，这些神经元会对数据进行初步的非线性变换，从而发现更复杂的、与目标相关的信息。然后，这个结果被传递给下一个层次的神经元，直到达到顶部，即输出层。这种结构被称为多层感知器（Multilayer Perceptron, MLP）。\n",
      "\n",
      "深度学习在训练过程中使用大量的标注数据来优化其模型参数，以提高预测准确率和泛化能力。常用的深度学习模型包括卷积神经网络（Convolutional Neural Networks, CNNs）、循环神经网络（Recurrent Neural Networks, RNNs）和生成对抗网络（Generative Adversarial Networks, GANs），其中最出名的是前两者。\n",
      "\n",
      "深度学习已经成为许多领域的关键技术，如自然语言处理、图像识别、语音识别以及预测分析等。通过使用大量的计算资源和高效的数据预处理技术来加速模型训练过程，深度学习已经在这些领域取得了显著的成果，并且正在持续发展之中。\n",
      "\n",
      "输入3：今天天气不错\n",
      "输出3：今天的天气确实不错，这通常意味着是一个适宜外出、享受户外活动的好日子。不过具体的感受还会受到其他因素的影响，比如温度、湿度和个人喜好等。总的来说，这是一个令人愉悦的天气条件。如果您有特定地点或具体的要求，请告诉我！\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RunnableBranch(branches=[(RunnableLambda(is_question), PromptTemplate(input_variables=['text'], input_types={}, partial_variables={}, template='请详细回答这个问题：{text}')\n",
       "| OllamaLLM(model='qwen2.5:3b', temperature=0.7, base_url='http://localhost:11434')), (RunnableLambda(is_greeting), PromptTemplate(input_variables=['text'], input_types={}, partial_variables={}, template='友好地回应这个问候：{text}')\n",
       "| OllamaLLM(model='qwen2.5:3b', temperature=0.7, base_url='http://localhost:11434'))], default=PromptTemplate(input_variables=['text'], input_types={}, partial_variables={}, template='请对以下内容进行评论：{text}')\n",
       "| OllamaLLM(model='qwen2.5:3b', temperature=0.7, base_url='http://localhost:11434'))\n",
       "| StrOutputParser()"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 5. RunnableLambda 自定义函数",
   "id": "761a6ec6d8622f22"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def lambda_example():\n",
    "    \"\"\"RunnableLambda 自定义函数示例\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"5. RunnableLambda 自定义函数\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    llm = create_llm()\n",
    "\n",
    "    # 自定义处理函数\n",
    "    def preprocess_text(x):\n",
    "        \"\"\"文本预处理\"\"\"\n",
    "        text = x[\"text\"]\n",
    "        # 清理文本\n",
    "        text = text.strip()\n",
    "        # 添加元数据\n",
    "        return {\n",
    "            \"processed_text\": text,\n",
    "            \"word_count\": len(text.split()),\n",
    "            \"char_count\": len(text),\n",
    "            \"timestamp\": datetime.now().isoformat()\n",
    "        }\n",
    "\n",
    "    def postprocess_result(x):\n",
    "        \"\"\"结果后处理\"\"\"\n",
    "        return {\n",
    "            \"response\": x,\n",
    "            \"processed_at\": datetime.now().isoformat(),\n",
    "            \"response_length\": len(x)\n",
    "        }\n",
    "\n",
    "    # 创建包含自定义函数的链\n",
    "    chain = (\n",
    "        RunnableLambda(preprocess_text)\n",
    "        | RunnablePassthrough.assign(\n",
    "            ai_response=lambda x: (\n",
    "                PromptTemplate.from_template(\"请分析以下文本（{word_count}词，{char_count}字符）：{processed_text}\")\n",
    "                | llm\n",
    "                | StrOutputParser()\n",
    "            ).invoke(x)\n",
    "        )\n",
    "        | RunnableLambda(lambda x: postprocess_result(x[\"ai_response\"]))\n",
    "    )\n",
    "\n",
    "    # 测试\n",
    "    input_text = {\"text\": \"人工智能正在改变我们的生活方式，从智能手机到自动驾驶汽车。\"}\n",
    "    result = chain.invoke(input_text)\n",
    "\n",
    "    print(f\"输入：{input_text['text']}\")\n",
    "    print(f\"AI回应：{result['response']}\")\n",
    "    print(f\"处理时间：{result['processed_at']}\")\n",
    "    print(f\"回应长度：{result['response_length']}字符\")\n",
    "\n",
    "    return chain\n",
    "lambda_example()"
   ],
   "id": "9adb7e218a250335",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 6. 带记忆的对话链",
   "id": "6f0f540a5c5014a2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def memory_chat_example():\n",
    "    \"\"\"带记忆的对话链示例\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"6. 带记忆的对话链\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    chat_llm = create_chat_llm()\n",
    "\n",
    "    # 创建聊天提示模板\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"你是一个友好的AI助手，能够记住之前的对话内容。\"),\n",
    "        MessagesPlaceholder(variable_name=\"history\"),\n",
    "        (\"human\", \"{input}\")\n",
    "    ])\n",
    "\n",
    "    # 创建基础链\n",
    "    chain = prompt | chat_llm | StrOutputParser()\n",
    "\n",
    "    # 创建记忆存储\n",
    "    memory = InMemoryChatMessageHistory()\n",
    "\n",
    "    # 创建带记忆的链\n",
    "    chain_with_memory = RunnableWithMessageHistory(\n",
    "        chain,\n",
    "        lambda session_id: memory,\n",
    "        input_messages_key=\"input\",\n",
    "        history_messages_key=\"history\"\n",
    "    )\n",
    "\n",
    "    # 模拟对话\n",
    "    conversations = [\n",
    "        \"我叫张三，今年25岁\",\n",
    "        \"我的爱好是什么？\",\n",
    "        \"我多大了？\",\n",
    "        \"能总结一下我们的对话吗？\"\n",
    "    ]\n",
    "\n",
    "    config = {\"configurable\": {\"session_id\": \"user_123\"}}\n",
    "\n",
    "    for i, message in enumerate(conversations, 1):\n",
    "        response = chain_with_memory.invoke(\n",
    "            {\"input\": message},\n",
    "            config=config\n",
    "        )\n",
    "        print(f\"用户{i}：{message}\")\n",
    "        print(f\"AI{i}：{response}\\n\")\n",
    "\n",
    "    return chain_with_memory"
   ],
   "id": "3c5d996866fe88bb",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 7. 复杂数据处理链",
   "id": "dfb3237e9961b2c8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def complex_data_processing():\n",
    "    \"\"\"复杂数据处理链示例\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"7. 复杂数据处理链\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    llm = create_llm()\n",
    "\n",
    "    # 模拟数据\n",
    "    data = {\n",
    "        \"users\": [\n",
    "            {\"name\": \"张三\", \"age\": 25, \"city\": \"北京\", \"interests\": [\"编程\", \"阅读\"]},\n",
    "            {\"name\": \"李四\", \"age\": 30, \"city\": \"上海\", \"interests\": [\"音乐\", \"旅行\"]},\n",
    "            {\"name\": \"王五\", \"age\": 28, \"city\": \"深圳\", \"interests\": [\"运动\", \"摄影\"]}\n",
    "        ],\n",
    "        \"query\": \"分析用户群体特征\"\n",
    "    }\n",
    "\n",
    "    # 数据处理函数\n",
    "    def analyze_users(x):\n",
    "        users = x[\"users\"]\n",
    "        total_users = len(users)\n",
    "        avg_age = sum(user[\"age\"] for user in users) / total_users\n",
    "        cities = list(set(user[\"city\"] for user in users))\n",
    "        all_interests = []\n",
    "        for user in users:\n",
    "            all_interests.extend(user[\"interests\"])\n",
    "\n",
    "        return {\n",
    "            \"total_users\": total_users,\n",
    "            \"average_age\": round(avg_age, 1),\n",
    "            \"cities\": cities,\n",
    "            \"common_interests\": list(set(all_interests)),\n",
    "            \"original_query\": x[\"query\"]\n",
    "        }\n",
    "\n",
    "    def format_analysis(x):\n",
    "        return f\"\"\"\n",
    "用户群体分析报告：\n",
    "- 总用户数：{x['total_users']}人\n",
    "- 平均年龄：{x['average_age']}岁\n",
    "- 分布城市：{', '.join(x['cities'])}\n",
    "- 兴趣爱好：{', '.join(x['common_interests'])}\n",
    "\n",
    "请基于以上数据回答：{x['original_query']}\n",
    "\"\"\"\n",
    "\n",
    "    # 创建处理链\n",
    "    processing_chain = (\n",
    "        RunnableLambda(analyze_users)\n",
    "        | RunnableLambda(format_analysis)\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "\n",
    "    result = processing_chain.invoke(data)\n",
    "    print(\"数据分析结果：\")\n",
    "    print(result)\n",
    "\n",
    "    return processing_chain"
   ],
   "id": "46f677ed3fd24199",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 8. 异步处理示例",
   "id": "d9d863aba76175f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "async def async_example():\n",
    "    \"\"\"异步处理示例\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"8. 异步处理示例\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    llm = create_llm()\n",
    "\n",
    "    # 创建异步处理链\n",
    "    prompt = PromptTemplate.from_template(\"请用中文简要回答：{question}\")\n",
    "    chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "    # 准备多个问题\n",
    "    questions = [\n",
    "        {\"question\": \"什么是机器学习？\"},\n",
    "        {\"question\": \"什么是深度学习？\"},\n",
    "        {\"question\": \"什么是自然语言处理？\"},\n",
    "        {\"question\": \"什么是计算机视觉？\"}\n",
    "    ]\n",
    "\n",
    "    print(\"开始异步处理多个问题...\")\n",
    "    start_time = datetime.now()\n",
    "\n",
    "    # 异步批量处理\n",
    "    results = await chain.abatch(questions)\n",
    "\n",
    "    end_time = datetime.now()\n",
    "    processing_time = (end_time - start_time).total_seconds()\n",
    "\n",
    "    print(f\"处理完成，耗时：{processing_time:.2f}秒\\n\")\n",
    "\n",
    "    for i, (question, result) in enumerate(zip(questions, results), 1):\n",
    "        print(f\"问题{i}：{question['question']}\")\n",
    "        print(f\"回答{i}：{result}\\n\")\n",
    "\n",
    "    return chain"
   ],
   "id": "403941e573edb342",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 9. 流式处理示例",
   "id": "63226c997facf053"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def streaming_example():\n",
    "    \"\"\"流式处理示例\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"9. 流式处理示例\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    llm = create_llm()\n",
    "\n",
    "    prompt = PromptTemplate.from_template(\"请详细解释：{topic}\")\n",
    "    chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "    print(\"开始流式生成回答...\")\n",
    "    print(\"问题：什么是人工智能？\")\n",
    "    print(\"回答：\", end=\"\", flush=True)\n",
    "\n",
    "    # 流式处理\n",
    "    for chunk in chain.stream({\"topic\": \"什么是人工智能\"}):\n",
    "        print(chunk, end=\"\", flush=True)\n",
    "\n",
    "    print(\"\\n\\n流式处理完成！\")\n",
    "\n",
    "    return chain"
   ],
   "id": "ca9ee5dd22fb97cb",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 10. JSON 输出解析示例",
   "id": "f28e6b115bbe78aa"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def json_output_example():\n",
    "    \"\"\"JSON输出解析示例\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"10. JSON输出解析示例\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    llm = create_llm()\n",
    "\n",
    "    # 创建JSON输出解析器\n",
    "    json_parser = JsonOutputParser()\n",
    "\n",
    "    # 创建提示模板，要求JSON格式输出\n",
    "    prompt = PromptTemplate.from_template(\"\"\"\n",
    "请分析以下文本并以JSON格式返回结果：\n",
    "文本：{text}\n",
    "\n",
    "请返回包含以下字段的JSON：\n",
    "- sentiment: 情感（positive/negative/neutral）\n",
    "- topics: 主要话题列表\n",
    "- summary: 一句话总结\n",
    "- word_count: 词数\n",
    "\n",
    "JSON格式：\n",
    "\"\"\")\n",
    "\n",
    "    # 创建链\n",
    "    chain = prompt | llm | json_parser\n",
    "\n",
    "    # 测试文本\n",
    "    test_text = \"今天的会议非常成功，我们讨论了新产品的开发计划和市场策略，团队成员都很积极参与。\"\n",
    "\n",
    "    try:\n",
    "        result = chain.invoke({\"text\": test_text})\n",
    "        print(f\"输入文本：{test_text}\")\n",
    "        print(\"JSON分析结果：\")\n",
    "        print(json.dumps(result, ensure_ascii=False, indent=2))\n",
    "    except Exception as e:\n",
    "        print(f\"JSON解析失败：{e}\")\n",
    "        # 降级处理\n",
    "        simple_chain = prompt | llm | StrOutputParser()\n",
    "        result = simple_chain.invoke({\"text\": test_text})\n",
    "        print(f\"文本结果：{result}\")\n",
    "\n",
    "    return chain"
   ],
   "id": "f8aa710fd4597b6",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "# ============================================================================\n",
    "# 主函数\n",
    "# ============================================================================\n",
    "\n",
    "def main():\n",
    "    \"\"\"运行所有示例\"\"\"\n",
    "    print(\"LangChain 0.3 LCEL 完整示例集合\")\n",
    "    print(\"基于 LangChain 0.3.26 版本\")\n",
    "    print(\"确保 Ollama 服务正在运行：http://localhost:11434\")\n",
    "\n",
    "    try:\n",
    "        # 运行所有同步示例\n",
    "        basic_chain_example()\n",
    "        passthrough_example()\n",
    "        parallel_example()\n",
    "        branch_example()\n",
    "        lambda_example()\n",
    "        memory_chat_example()\n",
    "        complex_data_processing()\n",
    "        streaming_example()\n",
    "        json_output_example()\n",
    "\n",
    "        # 运行异步示例\n",
    "        print(\"\\n开始运行异步示例...\")\n",
    "        asyncio.run(async_example())\n",
    "\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"所有示例运行完成！\")\n",
    "        print(\"=\" * 60)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"运行出错：{e}\")\n",
    "        print(\"请确保：\")\n",
    "        print(\"1. Ollama 服务正在运行\")\n",
    "        print(\"2. qwen2.5:3b 模型已下载\")\n",
    "        print(\"3. 网络连接正常\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ],
   "id": "c1539e699a47b62b",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 总结\n",
    "\n",
    "LCEL 是 LangChain 0.3 中的核心特性，提供了：\n",
    "\n",
    "1. **声明式编程**：简洁的链式语法\n",
    "2. **强大组合能力**：灵活的组件组合\n",
    "3. **性能优化**：自动并行化和批处理\n",
    "4. **流式支持**：实时响应能力\n",
    "5. **调试友好**：清晰的执行流程\n",
    "\n",
    "**选择建议**：\n",
    "- 简单应用：使用基础链组合\n",
    "- 复杂逻辑：使用 `RunnableBranch` 和 `RunnableParallel`\n",
    "- 高性能需求：利用批处理和并行特性\n",
    "- 复杂状态管理：考虑升级到 LangGraph\n",
    "\n",
    "所有示例代码都基于 LangChain 0.3.26 版本，确保与您的环境兼容。\n"
   ],
   "id": "19a72de9d062c3a4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "---",
   "id": "eb067d87cf910678"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-22T16:14:45.317709Z",
     "start_time": "2025-07-22T16:14:44.362991Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "LangChain 0.3 LCEL 高级示例集合\n",
    "基于 LangChain 0.3.26 版本\n",
    "\"\"\"\n",
    "\n",
    "import asyncio\n",
    "import json\n",
    "import time\n",
    "from typing import Dict, List, Any, Optional, Union\n",
    "from datetime import datetime\n",
    "from collections import deque\n",
    "\n",
    "# LangChain 核心组件\n",
    "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, PromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.output_parsers import StrOutputParser, JsonOutputParser, PydanticOutputParser\n",
    "from langchain_core.runnables import (\n",
    "    RunnablePassthrough,\n",
    "    RunnableLambda,\n",
    "    RunnableParallel,\n",
    "    RunnableBranch,\n",
    "    RunnableMap,\n",
    "    RunnableSequence,\n",
    "    RunnableConfig,\n",
    "    Runnable\n",
    ")\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_core.chat_history import InMemoryChatMessageHistory\n",
    "from langchain_ollama import OllamaLLM, ChatOllama\n",
    "from pydantic import BaseModel, Field"
   ],
   "id": "9234fe29699c31ac",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "# 配置\n",
    "OLLAMA_BASE_URL = \"http://localhost:11434\"\n",
    "OLLAMA_MODEL = \"qwen2.5:3b\"\n",
    "\n",
    "def create_llm():\n",
    "    \"\"\"创建LLM实例\"\"\"\n",
    "    return OllamaLLM(\n",
    "        base_url=OLLAMA_BASE_URL,\n",
    "        model=OLLAMA_MODEL,\n",
    "        temperature=0.7\n",
    "    )\n",
    "\n",
    "def create_chat_llm():\n",
    "    \"\"\"创建Chat LLM实例\"\"\"\n",
    "    return ChatOllama(\n",
    "        base_url=OLLAMA_BASE_URL,\n",
    "        model=OLLAMA_MODEL,\n",
    "        temperature=0.7\n",
    "    )\n",
    "\n",
    "# ============================================================================\n",
    "# 1. 基础 LCEL 操作符示例\n",
    "# ============================================================================\n",
    "\n",
    "def basic_operators_example():\n",
    "    \"\"\"基础 LCEL 操作符示例\"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"1. 基础 LCEL 操作符\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    llm = create_llm()\n",
    "\n",
    "    # 管道操作符 |\n",
    "    prompt = PromptTemplate.from_template(\"翻译成英文：{text}\")\n",
    "    chain1 = prompt | llm | StrOutputParser()\n",
    "\n",
    "    # 等价于 RunnableSequence\n",
    "    chain2 = RunnableSequence(first=prompt, middle=[llm], last=StrOutputParser())\n",
    "\n",
    "    result1 = chain1.invoke({\"text\": \"你好世界\"})\n",
    "    result2 = chain2.invoke({\"text\": \"你好世界\"})\n",
    "\n",
    "    print(f\"管道操作符结果：{result1}\")\n",
    "    print(f\"RunnableSequence结果：{result2}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 2. RunnablePassthrough 高级用法\n",
    "# ============================================================================\n",
    "\n",
    "def advanced_passthrough_example():\n",
    "    \"\"\"RunnablePassthrough 高级用法\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"2. RunnablePassthrough 高级用法\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    llm = create_llm()\n",
    "\n",
    "    # 使用 assign 添加新字段\n",
    "    def calculate_stats(x):\n",
    "        text = x[\"text\"]\n",
    "        return {\n",
    "            \"word_count\": len(text.split()),\n",
    "            \"char_count\": len(text),\n",
    "            \"has_question\": \"?\" in text or \"？\" in text\n",
    "        }\n",
    "\n",
    "    # 复杂的数据流处理\n",
    "    chain = (\n",
    "        RunnablePassthrough.assign(stats=RunnableLambda(calculate_stats))\n",
    "        | RunnablePassthrough.assign(\n",
    "            analysis_prompt=lambda x: f\"\"\"\n",
    "分析以下文本（{x['stats']['word_count']}词，{x['stats']['char_count']}字符）：\n",
    "文本：{x['text']}\n",
    "是否包含问题：{x['stats']['has_question']}\n",
    "\n",
    "请提供详细分析：\n",
    "\"\"\"\n",
    "        )\n",
    "        | RunnablePassthrough.assign(\n",
    "            analysis=lambda x: (PromptTemplate.from_template(\"{analysis_prompt}\") | llm | StrOutputParser()).invoke(x)\n",
    "        )\n",
    "        | RunnableLambda(lambda x: {\n",
    "            \"original\": x[\"text\"],\n",
    "            \"stats\": x[\"stats\"],\n",
    "            \"analysis\": x[\"analysis\"]\n",
    "        })\n",
    "    )\n",
    "\n",
    "    result = chain.invoke({\"text\": \"人工智能的发展前景如何？它会改变我们的生活吗？\"})\n",
    "\n",
    "    print(f\"原文：{result['original']}\")\n",
    "    print(f\"统计：{result['stats']}\")\n",
    "    print(f\"分析：{result['analysis']}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 3. RunnableParallel 复杂并行处理\n",
    "# ============================================================================\n",
    "\n",
    "def complex_parallel_example():\n",
    "    \"\"\"复杂并行处理示例\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"3. 复杂并行处理\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    llm = create_llm()\n",
    "\n",
    "    # 定义多个分析任务\n",
    "    sentiment_chain = (\n",
    "        PromptTemplate.from_template(\"分析情感（积极/消极/中性）：{text}\")\n",
    "        | llm | StrOutputParser()\n",
    "    )\n",
    "\n",
    "    topic_chain = (\n",
    "        PromptTemplate.from_template(\"提取3个主要关键词：{text}\")\n",
    "        | llm | StrOutputParser()\n",
    "    )\n",
    "\n",
    "    summary_chain = (\n",
    "        PromptTemplate.from_template(\"一句话总结：{text}\")\n",
    "        | llm | StrOutputParser()\n",
    "    )\n",
    "\n",
    "    length_analysis = RunnableLambda(lambda x: {\n",
    "        \"word_count\": len(x[\"text\"].split()),\n",
    "        \"char_count\": len(x[\"text\"]),\n",
    "        \"sentence_count\": len([s for s in x[\"text\"].split(\"。\") if s.strip()])\n",
    "    })\n",
    "\n",
    "    # 创建复杂并行链\n",
    "    parallel_chain = RunnableParallel({\n",
    "        \"sentiment\": sentiment_chain,\n",
    "        \"topics\": topic_chain,\n",
    "        \"summary\": summary_chain,\n",
    "        \"length_stats\": length_analysis,\n",
    "        \"original\": RunnablePassthrough(),\n",
    "        \"timestamp\": RunnableLambda(lambda x: datetime.now().isoformat())\n",
    "    })\n",
    "\n",
    "    # 后处理：合并结果\n",
    "    def format_results(results):\n",
    "        return f\"\"\"\n",
    "文本分析报告\n",
    "================\n",
    "原文：{results['original']['text']}\n",
    "时间：{results['timestamp']}\n",
    "\n",
    "情感分析：{results['sentiment']}\n",
    "关键词：{results['topics']}\n",
    "摘要：{results['summary']}\n",
    "\n",
    "统计信息：\n",
    "- 字数：{results['length_stats']['word_count']}\n",
    "- 字符数：{results['length_stats']['char_count']}\n",
    "- 句子数：{results['length_stats']['sentence_count']}\n",
    "\"\"\"\n",
    "\n",
    "    final_chain = parallel_chain | RunnableLambda(format_results)\n",
    "\n",
    "    text = \"今天参加了一个关于人工智能的会议，讨论了机器学习、深度学习和自然语言处理的最新进展。专家们分享了很多有趣的观点，让我对AI的未来发展有了更深的理解。\"\n",
    "\n",
    "    result = final_chain.invoke({\"text\": text})\n",
    "    print(result)\n",
    "\n",
    "# ============================================================================\n",
    "# 4. RunnableBranch 复杂条件分支\n",
    "# ============================================================================\n",
    "\n",
    "def complex_branch_example():\n",
    "    \"\"\"复杂条件分支示例\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"4. 复杂条件分支\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    llm = create_llm()\n",
    "\n",
    "    # 复杂条件判断函数\n",
    "    def is_technical_question(x):\n",
    "        technical_keywords = [\"算法\", \"编程\", \"代码\", \"技术\", \"开发\", \"API\", \"数据库\"]\n",
    "        return any(keyword in x[\"text\"] for keyword in technical_keywords)\n",
    "\n",
    "    def is_business_question(x):\n",
    "        business_keywords = [\"市场\", \"销售\", \"客户\", \"收入\", \"成本\", \"利润\", \"商业\"]\n",
    "        return any(keyword in x[\"text\"] for keyword in business_keywords)\n",
    "\n",
    "    def is_personal_question(x):\n",
    "        personal_keywords = [\"我\", \"个人\", \"建议\", \"帮助\", \"怎么办\"]\n",
    "        return any(keyword in x[\"text\"] for keyword in personal_keywords)\n",
    "\n",
    "    def get_text_complexity(x):\n",
    "        text = x[\"text\"]\n",
    "        return len(text.split()) > 20  # 超过20词认为是复杂问题\n",
    "\n",
    "    # 创建不同类型的处理链\n",
    "    technical_chain = (\n",
    "        PromptTemplate.from_template(\"\"\"\n",
    "作为技术专家，请详细回答这个技术问题：{text}\n",
    "\n",
    "请包括：\n",
    "1. 技术原理\n",
    "2. 实现方法\n",
    "3. 最佳实践\n",
    "\"\"\")\n",
    "        | llm | StrOutputParser()\n",
    "    )\n",
    "\n",
    "    business_chain = (\n",
    "        PromptTemplate.from_template(\"\"\"\n",
    "作为商业顾问，请分析这个商业问题：{text}\n",
    "\n",
    "请包括：\n",
    "1. 市场分析\n",
    "2. 风险评估\n",
    "3. 建议方案\n",
    "\"\"\")\n",
    "        | llm | StrOutputParser()\n",
    "    )\n",
    "\n",
    "    personal_chain = (\n",
    "        PromptTemplate.from_template(\"\"\"\n",
    "作为生活顾问，请给出贴心的建议：{text}\n",
    "\n",
    "请提供：\n",
    "1. 理解和共情\n",
    "2. 具体建议\n",
    "3. 鼓励话语\n",
    "\"\"\")\n",
    "        | llm | StrOutputParser()\n",
    "    )\n",
    "\n",
    "    # 复杂问题需要更详细的分析\n",
    "    complex_analysis_chain = (\n",
    "        RunnablePassthrough.assign(\n",
    "            analysis=lambda x: (\n",
    "                PromptTemplate.from_template(\"首先分析问题的核心要点：{text}\")\n",
    "                | llm | StrOutputParser()\n",
    "            ).invoke(x)\n",
    "        )\n",
    "        | RunnablePassthrough.assign(\n",
    "            detailed_response=lambda x: (\n",
    "                PromptTemplate.from_template(\"\"\"\n",
    "基于分析：{analysis}\n",
    "\n",
    "请详细回答原问题：{text}\n",
    "\"\"\")\n",
    "                | llm | StrOutputParser()\n",
    "            ).invoke(x)\n",
    "        )\n",
    "        | RunnableLambda(lambda x: f\"分析：{x['analysis']}\\n\\n详细回答：{x['detailed_response']}\")\n",
    "    )\n",
    "\n",
    "    simple_chain = (\n",
    "        PromptTemplate.from_template(\"简洁回答：{text}\")\n",
    "        | llm | StrOutputParser()\n",
    "    )\n",
    "\n",
    "    # 创建嵌套分支\n",
    "    complexity_branch = RunnableBranch(\n",
    "        (get_text_complexity, complex_analysis_chain),\n",
    "        simple_chain\n",
    "    )\n",
    "\n",
    "    main_branch = RunnableBranch(\n",
    "        (is_technical_question, technical_chain),\n",
    "        (is_business_question, business_chain),\n",
    "        (is_personal_question, personal_chain),\n",
    "        complexity_branch  # 默认根据复杂度处理\n",
    "    )\n",
    "\n",
    "    # 测试不同类型的问题\n",
    "    test_questions = [\n",
    "        {\"text\": \"如何实现一个高效的排序算法？\"},\n",
    "        {\"text\": \"我们公司的市场策略应该如何调整？\"},\n",
    "        {\"text\": \"我最近工作压力很大，该怎么办？\"},\n",
    "        {\"text\": \"什么是人工智能？\"},\n",
    "        {\"text\": \"请详细解释深度学习的工作原理，包括反向传播算法、梯度下降优化、正则化技术，以及在计算机视觉和自然语言处理中的具体应用案例。\"}\n",
    "    ]\n",
    "\n",
    "    for i, question in enumerate(test_questions, 1):\n",
    "        print(f\"\\n问题{i}：{question['text']}\")\n",
    "        result = main_branch.invoke(question)\n",
    "        print(f\"回答{i}：{result}\")\n",
    "        print(\"-\" * 40)\n",
    "\n",
    "# ============================================================================\n",
    "# 5. 自定义 Runnable 类\n",
    "# ============================================================================\n",
    "\n",
    "class CustomTextProcessor(Runnable):\n",
    "    \"\"\"自定义文本处理器\"\"\"\n",
    "\n",
    "    def __init__(self, processing_type: str = \"default\"):\n",
    "        self.processing_type = processing_type\n",
    "\n",
    "    def invoke(self, input: Dict[str, Any], config: Optional[RunnableConfig] = None) -> Dict[str, Any]:\n",
    "        text = input.get(\"text\", \"\")\n",
    "\n",
    "        if self.processing_type == \"uppercase\":\n",
    "            processed = text.upper()\n",
    "        elif self.processing_type == \"reverse\":\n",
    "            processed = text[::-1]\n",
    "        elif self.processing_type == \"word_count\":\n",
    "            processed = f\"字数统计：{len(text.split())}词\"\n",
    "        else:\n",
    "            processed = text.strip()\n",
    "\n",
    "        return {\n",
    "            \"original\": text,\n",
    "            \"processed\": processed,\n",
    "            \"type\": self.processing_type,\n",
    "            \"timestamp\": datetime.now().isoformat()\n",
    "        }\n",
    "\n",
    "def custom_runnable_example():\n",
    "    \"\"\"自定义 Runnable 示例\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"5. 自定义 Runnable 类\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # 创建不同类型的处理器\n",
    "    processors = {\n",
    "        \"uppercase\": CustomTextProcessor(\"uppercase\"),\n",
    "        \"reverse\": CustomTextProcessor(\"reverse\"),\n",
    "        \"word_count\": CustomTextProcessor(\"word_count\")\n",
    "    }\n",
    "\n",
    "    # 创建并行处理链\n",
    "    parallel_processing = RunnableParallel({\n",
    "        \"uppercase_result\": processors[\"uppercase\"],\n",
    "        \"reverse_result\": processors[\"reverse\"],\n",
    "        \"count_result\": processors[\"word_count\"],\n",
    "        \"original\": RunnablePassthrough()\n",
    "    })\n",
    "\n",
    "    test_text = {\"text\": \"Hello LangChain LCEL\"}\n",
    "    result = parallel_processing.invoke(test_text)\n",
    "\n",
    "    print(f\"原文：{result['original']['text']}\")\n",
    "    print(f\"大写处理：{result['uppercase_result']['processed']}\")\n",
    "    print(f\"反转处理：{result['reverse_result']['processed']}\")\n",
    "    print(f\"计数处理：{result['count_result']['processed']}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 6. 流式处理高级示例\n",
    "# ============================================================================\n",
    "\n",
    "def advanced_streaming_example():\n",
    "    \"\"\"高级流式处理示例\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"6. 高级流式处理\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    llm = create_llm()\n",
    "\n",
    "    # 创建流式处理链\n",
    "    prompt = PromptTemplate.from_template(\"\"\"\n",
    "请详细解释以下概念，并给出实际应用例子：{topic}\n",
    "\n",
    "请按以下结构回答：\n",
    "1. 定义和基本概念\n",
    "2. 核心原理\n",
    "3. 实际应用\n",
    "4. 发展趋势\n",
    "\"\"\")\n",
    "\n",
    "    # 添加流式处理的元数据\n",
    "    def add_streaming_metadata(chunk):\n",
    "        return {\n",
    "            \"content\": chunk,\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"chunk_length\": len(chunk)\n",
    "        }\n",
    "\n",
    "    streaming_chain = (\n",
    "        prompt\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "        | RunnableLambda(add_streaming_metadata)\n",
    "    )\n",
    "\n",
    "    print(\"开始流式生成（带元数据）...\")\n",
    "    print(\"主题：机器学习\")\n",
    "    print(\"\\n回答：\")\n",
    "\n",
    "    total_chunks = 0\n",
    "    total_length = 0\n",
    "\n",
    "    for chunk_data in streaming_chain.stream({\"topic\": \"机器学习\"}):\n",
    "        content = chunk_data[\"content\"]\n",
    "        print(content, end=\"\", flush=True)\n",
    "        total_chunks += 1\n",
    "        total_length += chunk_data[\"chunk_length\"]\n",
    "\n",
    "    print(f\"\\n\\n流式处理完成！\")\n",
    "    print(f\"总块数：{total_chunks}\")\n",
    "    print(f\"总长度：{total_length}字符\")\n",
    "\n",
    "# ============================================================================\n",
    "# 7. 批量处理示例\n",
    "# ============================================================================\n",
    "\n",
    "async def batch_processing_example():\n",
    "    \"\"\"批量处理示例\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"7. 批量处理示例\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    llm = create_llm()\n",
    "\n",
    "    # 创建批量处理链\n",
    "    prompt = PromptTemplate.from_template(\"用一句话解释：{concept}\")\n",
    "    chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "    # 准备批量数据\n",
    "    concepts = [\n",
    "        {\"concept\": \"人工智能\"},\n",
    "        {\"concept\": \"机器学习\"},\n",
    "        {\"concept\": \"深度学习\"},\n",
    "        {\"concept\": \"自然语言处理\"},\n",
    "        {\"concept\": \"计算机视觉\"},\n",
    "        {\"concept\": \"强化学习\"}\n",
    "    ]\n",
    "\n",
    "    print(\"开始批量处理...\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    # 同步批量处理\n",
    "    sync_results = chain.batch(concepts)\n",
    "    sync_time = time.time() - start_time\n",
    "\n",
    "    print(f\"同步批量处理完成，耗时：{sync_time:.2f}秒\")\n",
    "\n",
    "    # 异步批量处理\n",
    "    start_time = time.time()\n",
    "    async_results = await chain.abatch(concepts)\n",
    "    async_time = time.time() - start_time\n",
    "\n",
    "    print(f\"异步批量处理完成，耗时：{async_time:.2f}秒\")\n",
    "\n",
    "    # 显示结果\n",
    "    for i, (concept, sync_result, async_result) in enumerate(zip(concepts, sync_results, async_results), 1):\n",
    "        print(f\"\\n概念{i}：{concept['concept']}\")\n",
    "        print(f\"同步结果：{sync_result}\")\n",
    "        print(f\"异步结果：{async_result}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 8. 错误处理和重试机制\n",
    "# ============================================================================\n",
    "\n",
    "def error_handling_example():\n",
    "    \"\"\"错误处理和重试机制示例\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"8. 错误处理和重试机制\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    llm = create_llm()\n",
    "\n",
    "    # 模拟可能失败的处理函数\n",
    "    def risky_processing(x):\n",
    "        import random\n",
    "        if random.random() < 0.3:  # 30% 失败率\n",
    "            raise Exception(\"模拟处理失败\")\n",
    "        return {\"processed\": f\"成功处理：{x['text']}\", \"status\": \"success\"}\n",
    "\n",
    "    # 重试装饰器\n",
    "    def with_retry(func, max_retries=3):\n",
    "        def wrapper(x):\n",
    "            for attempt in range(max_retries):\n",
    "                try:\n",
    "                    return func(x)\n",
    "                except Exception as e:\n",
    "                    if attempt == max_retries - 1:\n",
    "                        return {\"error\": str(e), \"status\": \"failed\", \"attempts\": attempt + 1}\n",
    "                    print(f\"尝试 {attempt + 1} 失败：{e}\")\n",
    "            return {\"error\": \"最大重试次数已达到\", \"status\": \"failed\"}\n",
    "        return wrapper\n",
    "\n",
    "    # 创建带错误处理的链\n",
    "    safe_processing = RunnableLambda(with_retry(risky_processing))\n",
    "\n",
    "    # 备用处理链\n",
    "    fallback_chain = RunnableLambda(lambda x: {\n",
    "        \"processed\": f\"备用处理：{x['text']}\",\n",
    "        \"status\": \"fallback\"\n",
    "    })\n",
    "\n",
    "    # 主处理链\n",
    "    main_chain = (\n",
    "        RunnablePassthrough.assign(result=safe_processing)\n",
    "        | RunnableLambda(lambda x:\n",
    "            x[\"result\"] if x[\"result\"][\"status\"] != \"failed\"\n",
    "            else {**x, \"result\": fallback_chain.invoke(x)}\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # 测试错误处理\n",
    "    test_inputs = [\n",
    "        {\"text\": \"测试文本1\"},\n",
    "        {\"text\": \"测试文本2\"},\n",
    "        {\"text\": \"测试文本3\"},\n",
    "        {\"text\": \"测试文本4\"},\n",
    "        {\"text\": \"测试文本5\"}\n",
    "    ]\n",
    "\n",
    "    for i, input_data in enumerate(test_inputs, 1):\n",
    "        result = main_chain.invoke(input_data)\n",
    "        print(f\"输入{i}：{input_data['text']}\")\n",
    "        print(f\"结果{i}：{result['result']}\")\n",
    "        print(f\"状态{i}：{result['result']['status']}\")\n",
    "        print(\"-\" * 30)\n",
    "\n",
    "# ============================================================================\n",
    "# 9. 动态链构建\n",
    "# ============================================================================\n",
    "\n",
    "def dynamic_chain_example():\n",
    "    \"\"\"动态链构建示例\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"9. 动态链构建\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    llm = create_llm()\n",
    "\n",
    "    def build_dynamic_chain(processing_steps: List[str]):\n",
    "        \"\"\"根据配置动态构建处理链\"\"\"\n",
    "\n",
    "        # 定义可用的处理步骤\n",
    "        available_steps = {\n",
    "            \"sentiment\": PromptTemplate.from_template(\"分析情感：{text}\") | llm | StrOutputParser(),\n",
    "            \"summary\": PromptTemplate.from_template(\"总结：{text}\") | llm | StrOutputParser(),\n",
    "            \"translate\": PromptTemplate.from_template(\"翻译成英文：{text}\") | llm | StrOutputParser(),\n",
    "            \"keywords\": PromptTemplate.from_template(\"提取关键词：{text}\") | llm | StrOutputParser(),\n",
    "            \"length\": RunnableLambda(lambda x: f\"长度：{len(x['text'])}字符\")\n",
    "        }\n",
    "\n",
    "        # 构建并行处理\n",
    "        parallel_steps = {}\n",
    "        for step in processing_steps:\n",
    "            if step in available_steps:\n",
    "                parallel_steps[step] = available_steps[step]\n",
    "\n",
    "        if not parallel_steps:\n",
    "            return RunnableLambda(lambda x: {\"error\": \"没有有效的处理步骤\"})\n",
    "\n",
    "        # 添加原文\n",
    "        parallel_steps[\"original\"] = RunnablePassthrough()\n",
    "\n",
    "        return RunnableParallel(parallel_steps)\n",
    "\n",
    "    # 测试不同的配置\n",
    "    configurations = [\n",
    "        [\"sentiment\", \"summary\"],\n",
    "        [\"translate\", \"keywords\", \"length\"],\n",
    "        [\"sentiment\", \"summary\", \"translate\", \"keywords\"],\n",
    "        [\"invalid_step\"]  # 测试错误情况\n",
    "    ]\n",
    "\n",
    "    test_text = {\"text\": \"今天天气很好，我和朋友们去公园散步，感觉很放松。\"}\n",
    "\n",
    "    for i, config in enumerate(configurations, 1):\n",
    "        print(f\"\\n配置{i}：{config}\")\n",
    "        chain = build_dynamic_chain(config)\n",
    "        result = chain.invoke(test_text)\n",
    "\n",
    "        for key, value in result.items():\n",
    "            if key != \"original\":\n",
    "                print(f\"  {key}: {value}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 10. 复杂数据流处理\n",
    "# ============================================================================\n",
    "\n",
    "def complex_data_flow_example():\n",
    "    \"\"\"复杂数据流处理示例\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"10. 复杂数据流处理\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    llm = create_llm()\n",
    "\n",
    "    # 模拟复杂的业务数据\n",
    "    business_data = {\n",
    "        \"company\": \"TechCorp\",\n",
    "        \"quarter\": \"Q3 2024\",\n",
    "        \"revenue\": 1500000,\n",
    "        \"expenses\": 1200000,\n",
    "        \"employees\": 150,\n",
    "        \"products\": [\n",
    "            {\"name\": \"AI助手\", \"sales\": 800000, \"growth\": 0.25},\n",
    "            {\"name\": \"数据分析平台\", \"sales\": 500000, \"growth\": 0.15},\n",
    "            {\"name\": \"云服务\", \"sales\": 200000, \"growth\": 0.35}\n",
    "        ],\n",
    "        \"regions\": {\n",
    "            \"北美\": {\"revenue\": 600000, \"growth\": 0.20},\n",
    "            \"欧洲\": {\"revenue\": 500000, \"growth\": 0.18},\n",
    "            \"亚洲\": {\"revenue\": 400000, \"growth\": 0.30}\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # 数据预处理\n",
    "    def calculate_metrics(data):\n",
    "        profit = data[\"revenue\"] - data[\"expenses\"]\n",
    "        profit_margin = profit / data[\"revenue\"] * 100\n",
    "        revenue_per_employee = data[\"revenue\"] / data[\"employees\"]\n",
    "\n",
    "        # 产品分析\n",
    "        best_product = max(data[\"products\"], key=lambda x: x[\"sales\"])\n",
    "        fastest_growing = max(data[\"products\"], key=lambda x: x[\"growth\"])\n",
    "\n",
    "        # 地区分析\n",
    "        best_region = max(data[\"regions\"].items(), key=lambda x: x[1][\"revenue\"])\n",
    "\n",
    "        return {\n",
    "            \"financial_metrics\": {\n",
    "                \"profit\": profit,\n",
    "                \"profit_margin\": round(profit_margin, 2),\n",
    "                \"revenue_per_employee\": round(revenue_per_employee, 2)\n",
    "            },\n",
    "            \"product_insights\": {\n",
    "                \"best_selling\": best_product[\"name\"],\n",
    "                \"fastest_growing\": fastest_growing[\"name\"]\n",
    "            },\n",
    "            \"regional_insights\": {\n",
    "                \"top_region\": best_region[0],\n",
    "                \"top_region_revenue\": best_region[1][\"revenue\"]\n",
    "            },\n",
    "            \"original_data\": data\n",
    "        }\n",
    "\n",
    "    # 创建分析报告\n",
    "    def generate_analysis_prompt(metrics):\n",
    "        return f\"\"\"\n",
    "请分析以下业务数据并生成专业报告：\n",
    "\n",
    "公司：{metrics['original_data']['company']}\n",
    "季度：{metrics['original_data']['quarter']}\n",
    "\n",
    "财务指标：\n",
    "- 收入：${metrics['original_data']['revenue']:,}\n",
    "- 支出：${metrics['original_data']['expenses']:,}\n",
    "- 利润：${metrics['financial_metrics']['profit']:,}\n",
    "- 利润率：{metrics['financial_metrics']['profit_margin']}%\n",
    "- 人均收入：${metrics['financial_metrics']['revenue_per_employee']:,}\n",
    "\n",
    "产品表现：\n",
    "- 最佳销售产品：{metrics['product_insights']['best_selling']}\n",
    "- 增长最快产品：{metrics['product_insights']['fastest_growing']}\n",
    "\n",
    "地区表现：\n",
    "- 最佳地区：{metrics['regional_insights']['top_region']}\n",
    "- 该地区收入：${metrics['regional_insights']['top_region_revenue']:,}\n",
    "\n",
    "请提供：\n",
    "1. 整体业务健康度评估\n",
    "2. 关键优势和风险点\n",
    "3. 改进建议\n",
    "\"\"\"\n",
    "\n",
    "    # 构建复杂处理链\n",
    "    analysis_chain = (\n",
    "        RunnableLambda(calculate_metrics)\n",
    "        | RunnablePassthrough.assign(\n",
    "            analysis_prompt=RunnableLambda(generate_analysis_prompt)\n",
    "        )\n",
    "        | RunnablePassthrough.assign(\n",
    "            business_analysis=lambda x: (\n",
    "                PromptTemplate.from_template(\"{analysis_prompt}\")\n",
    "                | llm | StrOutputParser()\n",
    "            ).invoke(x)\n",
    "        )\n",
    "        | RunnablePassthrough.assign(\n",
    "            executive_summary=lambda x: (\n",
    "                PromptTemplate.from_template(\"\"\"\n",
    "基于以下分析，写一份执行摘要（不超过100字）：\n",
    "{business_analysis}\n",
    "\"\"\")\n",
    "                | llm | StrOutputParser()\n",
    "            ).invoke(x)\n",
    "        )\n",
    "        | RunnableLambda(lambda x: {\n",
    "            \"company\": x[\"original_data\"][\"company\"],\n",
    "            \"quarter\": x[\"original_data\"][\"quarter\"],\n",
    "            \"key_metrics\": x[\"financial_metrics\"],\n",
    "            \"insights\": {\n",
    "                \"products\": x[\"product_insights\"],\n",
    "                \"regions\": x[\"regional_insights\"]\n",
    "            },\n",
    "            \"detailed_analysis\": x[\"business_analysis\"],\n",
    "            \"executive_summary\": x[\"executive_summary\"],\n",
    "            \"generated_at\": datetime.now().isoformat()\n",
    "        })\n",
    "    )\n",
    "\n",
    "    # 执行分析\n",
    "    result = analysis_chain.invoke(business_data)\n",
    "\n",
    "    print(f\"公司：{result['company']}\")\n",
    "    print(f\"季度：{result['quarter']}\")\n",
    "    print(f\"生成时间：{result['generated_at']}\")\n",
    "    print(\"\\n关键指标：\")\n",
    "    for key, value in result['key_metrics'].items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "\n",
    "    print(\"\\n业务洞察：\")\n",
    "    print(f\"  最佳产品：{result['insights']['products']['best_selling']}\")\n",
    "    print(f\"  增长最快：{result['insights']['products']['fastest_growing']}\")\n",
    "    print(f\"  最佳地区：{result['insights']['regions']['top_region']}\")\n",
    "\n",
    "    print(f\"\\n执行摘要：\\n{result['executive_summary']}\")\n",
    "    print(f\"\\n详细分析：\\n{result['detailed_analysis']}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 主函数\n",
    "# ============================================================================\n",
    "\n",
    "async def main():\n",
    "    \"\"\"运行所有示例\"\"\"\n",
    "    print(\"LangChain 0.3 LCEL 高级示例集合\")\n",
    "    print(\"基于 LangChain 0.3.26 版本\")\n",
    "    print(\"确保 Ollama 服务正在运行：http://localhost:11434\")\n",
    "\n",
    "    try:\n",
    "        # 运行所有同步示例\n",
    "        basic_operators_example()\n",
    "        advanced_passthrough_example()\n",
    "        complex_parallel_example()\n",
    "        complex_branch_example()\n",
    "        custom_runnable_example()\n",
    "        advanced_streaming_example()\n",
    "        error_handling_example()\n",
    "        dynamic_chain_example()\n",
    "        complex_data_flow_example()\n",
    "\n",
    "        # 运行异步示例\n",
    "        print(\"\\n开始运行异步示例...\")\n",
    "        await batch_processing_example()\n",
    "\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"所有高级示例运行完成！\")\n",
    "        print(\"=\" * 60)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"运行出错：{e}\")\n",
    "        print(\"请确保：\")\n",
    "        print(\"1. Ollama 服务正在运行\")\n",
    "        print(\"2. qwen2.5:3b 模型已下载\")\n",
    "        print(\"3. 网络连接正常\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(main())"
   ],
   "id": "2f31f870b485c915"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import uuid\n",
    "\n",
    "\"\"\"\n",
    "LCEL 专业应用场景示例\n",
    "\"\"\"\n",
    "\n",
    "# ============================================================================\n",
    "# 11. RAG（检索增强生成）系统\n",
    "# ============================================================================\n",
    "\n",
    "def rag_system_example():\n",
    "    \"\"\"RAG 系统示例\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"11. RAG 检索增强生成系统\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    llm = create_llm()\n",
    "\n",
    "    # 模拟文档数据库\n",
    "    documents = {\n",
    "        \"doc1\": \"人工智能是计算机科学的一个分支，致力于创建能够执行通常需要人类智能的任务的系统。\",\n",
    "        \"doc2\": \"机器学习是人工智能的一个子集，它使计算机能够在没有明确编程的情况下学习和改进。\",\n",
    "        \"doc3\": \"深度学习是机器学习的一个分支，使用神经网络来模拟人脑的工作方式。\",\n",
    "        \"doc4\": \"自然语言处理是人工智能的一个领域，专注于计算机与人类语言之间的交互。\",\n",
    "        \"doc5\": \"计算机视觉是人工智能的一个分支，致力于让计算机能够理解和解释视觉信息。\"\n",
    "    }\n",
    "\n",
    "    # 简单的检索函数\n",
    "    def retrieve_documents(query: str, top_k: int = 3):\n",
    "        \"\"\"基于关键词匹配检索文档\"\"\"\n",
    "        scores = {}\n",
    "        query_words = query.lower().split()\n",
    "\n",
    "        for doc_id, content in documents.items():\n",
    "            score = sum(1 for word in query_words if word in content.lower())\n",
    "            if score > 0:\n",
    "                scores[doc_id] = score\n",
    "\n",
    "        # 返回得分最高的文档\n",
    "        sorted_docs = sorted(scores.items(), key=lambda x: x[1], reverse=True)\n",
    "        return [documents[doc_id] for doc_id, _ in sorted_docs[:top_k]]\n",
    "\n",
    "    # 构建 RAG 链\n",
    "    retrieval_chain = RunnableLambda(lambda x: {\n",
    "        \"query\": x[\"query\"],\n",
    "        \"retrieved_docs\": retrieve_documents(x[\"query\"])\n",
    "    })\n",
    "\n",
    "    context_chain = RunnableLambda(lambda x: {\n",
    "        \"query\": x[\"query\"],\n",
    "        \"context\": \"\\n\\n\".join([f\"文档{i+1}: {doc}\" for i, doc in enumerate(x[\"retrieved_docs\"])])\n",
    "    })\n",
    "\n",
    "    rag_prompt = PromptTemplate.from_template(\"\"\"\n",
    "基于以下文档回答问题：\n",
    "\n",
    "{context}\n",
    "\n",
    "问题：{query}\n",
    "\n",
    "请基于提供的文档内容回答，如果文档中没有相关信息，请说明。\n",
    "\"\"\")\n",
    "\n",
    "    rag_chain = (\n",
    "        retrieval_chain\n",
    "        | context_chain\n",
    "        | rag_prompt\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "\n",
    "    # 测试 RAG 系统\n",
    "    questions = [\n",
    "        {\"query\": \"什么是机器学习？\"},\n",
    "        {\"query\": \"深度学习和机器学习有什么关系？\"},\n",
    "        {\"query\": \"人工智能有哪些应用领域？\"}\n",
    "    ]\n",
    "\n",
    "    for i, question in enumerate(questions, 1):\n",
    "        print(f\"\\n问题{i}：{question['query']}\")\n",
    "        answer = rag_chain.invoke(question)\n",
    "        print(f\"回答{i}：{answer}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 12. 多模态处理链\n",
    "# ============================================================================\n",
    "\n",
    "def multimodal_processing_example():\n",
    "    \"\"\"多模态处理示例\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"12. 多模态处理链\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    llm = create_llm()\n",
    "\n",
    "    # 模拟不同类型的输入处理\n",
    "    def process_text_input(x):\n",
    "        return {\n",
    "            \"type\": \"text\",\n",
    "            \"content\": x[\"content\"],\n",
    "            \"word_count\": len(x[\"content\"].split()),\n",
    "            \"language\": \"中文\" if any('\\u4e00' <= char <= '\\u9fff' for char in x[\"content\"]) else \"英文\"\n",
    "        }\n",
    "\n",
    "    def process_image_input(x):\n",
    "        # 模拟图像处理\n",
    "        return {\n",
    "            \"type\": \"image\",\n",
    "            \"filename\": x[\"content\"],\n",
    "            \"format\": x[\"content\"].split(\".\")[-1] if \".\" in x[\"content\"] else \"unknown\",\n",
    "            \"description\": f\"这是一个{x['content']}文件\"\n",
    "        }\n",
    "\n",
    "    def process_audio_input(x):\n",
    "        # 模拟音频处理\n",
    "        return {\n",
    "            \"type\": \"audio\",\n",
    "            \"filename\": x[\"content\"],\n",
    "            \"duration\": \"未知\",\n",
    "            \"description\": f\"这是一个音频文件：{x['content']}\"\n",
    "        }\n",
    "\n",
    "    # 输入类型检测\n",
    "    def detect_input_type(x):\n",
    "        content = x[\"content\"].lower()\n",
    "        if content.endswith(('.jpg', '.png', '.gif', '.bmp')):\n",
    "            return \"image\"\n",
    "        elif content.endswith(('.mp3', '.wav', '.flac')):\n",
    "            return \"audio\"\n",
    "        else:\n",
    "            return \"text\"\n",
    "\n",
    "    # 创建多模态处理分支\n",
    "    multimodal_branch = RunnableBranch(\n",
    "        (lambda x: detect_input_type(x) == \"image\", RunnableLambda(process_image_input)),\n",
    "        (lambda x: detect_input_type(x) == \"audio\", RunnableLambda(process_audio_input)),\n",
    "        RunnableLambda(process_text_input)  # 默认文本处理\n",
    "    )\n",
    "\n",
    "    # 生成统一的分析报告\n",
    "    analysis_prompt = PromptTemplate.from_template(\"\"\"\n",
    "请分析以下输入内容：\n",
    "\n",
    "类型：{type}\n",
    "内容：{content}\n",
    "详细信息：{details}\n",
    "\n",
    "请提供适合该类型内容的分析和建议。\n",
    "\"\"\")\n",
    "\n",
    "    def prepare_analysis_input(processed_data):\n",
    "        details = {k: v for k, v in processed_data.items() if k not in [\"type\", \"content\"]}\n",
    "        return {\n",
    "            \"type\": processed_data[\"type\"],\n",
    "            \"content\": processed_data.get(\"content\", processed_data.get(\"filename\", \"未知\")),\n",
    "            \"details\": str(details)\n",
    "        }\n",
    "\n",
    "    multimodal_chain = (\n",
    "        multimodal_branch\n",
    "        | RunnableLambda(prepare_analysis_input)\n",
    "        | analysis_prompt\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "\n",
    "    # 测试不同类型的输入\n",
    "    test_inputs = [\n",
    "        {\"content\": \"这是一段中文文本，用于测试多模态处理系统。\"},\n",
    "        {\"content\": \"photo.jpg\"},\n",
    "        {\"content\": \"music.mp3\"},\n",
    "        {\"content\": \"This is an English text for testing.\"}\n",
    "    ]\n",
    "\n",
    "    for i, input_data in enumerate(test_inputs, 1):\n",
    "        print(f\"\\n输入{i}：{input_data['content']}\")\n",
    "        result = multimodal_chain.invoke(input_data)\n",
    "        print(f\"分析{i}：{result}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 13. 工作流编排系统\n",
    "# ============================================================================\n",
    "\n",
    "def workflow_orchestration_example():\n",
    "    \"\"\"工作流编排示例\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"13. 工作流编排系统\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    llm = create_llm()\n",
    "\n",
    "    # 定义工作流步骤\n",
    "    class WorkflowStep:\n",
    "        def __init__(self, name: str, processor: Runnable):\n",
    "            self.name = name\n",
    "            self.processor = processor\n",
    "\n",
    "    # 数据验证步骤\n",
    "    def validate_data(x):\n",
    "        data = x.get(\"data\", {})\n",
    "        errors = []\n",
    "\n",
    "        if not data.get(\"title\"):\n",
    "            errors.append(\"标题不能为空\")\n",
    "        if not data.get(\"content\"):\n",
    "            errors.append(\"内容不能为空\")\n",
    "        if len(data.get(\"content\", \"\")) < 10:\n",
    "            errors.append(\"内容长度不能少于10字符\")\n",
    "\n",
    "        return {\n",
    "            \"data\": data,\n",
    "            \"validation_errors\": errors,\n",
    "            \"is_valid\": len(errors) == 0,\n",
    "            \"step\": \"validation\"\n",
    "        }\n",
    "\n",
    "    # 内容处理步骤\n",
    "    def process_content(x):\n",
    "        if not x[\"is_valid\"]:\n",
    "            return {**x, \"step\": \"content_processing\", \"processed_content\": None}\n",
    "\n",
    "        content = x[\"data\"][\"content\"]\n",
    "        processed = {\n",
    "            \"original_length\": len(content),\n",
    "            \"word_count\": len(content.split()),\n",
    "            \"has_keywords\": any(keyword in content.lower() for keyword in [\"重要\", \"紧急\", \"优先\"]),\n",
    "            \"processed_at\": datetime.now().isoformat()\n",
    "        }\n",
    "\n",
    "        return {\n",
    "            **x,\n",
    "            \"step\": \"content_processing\",\n",
    "            \"processed_content\": processed\n",
    "        }\n",
    "\n",
    "    # AI 分析步骤\n",
    "    def ai_analysis(x):\n",
    "        if not x[\"is_valid\"] or not x[\"processed_content\"]:\n",
    "            return {**x, \"step\": \"ai_analysis\", \"ai_result\": None}\n",
    "\n",
    "        analysis_prompt = f\"\"\"\n",
    "分析以下内容：\n",
    "标题：{x['data']['title']}\n",
    "内容：{x['data']['content']}\n",
    "\n",
    "请提供：\n",
    "1. 内容摘要\n",
    "2. 情感分析\n",
    "3. 重要程度评级（1-5）\n",
    "\"\"\"\n",
    "\n",
    "        ai_result = (\n",
    "            PromptTemplate.from_template(analysis_prompt)\n",
    "            | llm\n",
    "            | StrOutputParser()\n",
    "        ).invoke({})\n",
    "\n",
    "        return {\n",
    "            **x,\n",
    "            \"step\": \"ai_analysis\",\n",
    "            \"ai_result\": ai_result\n",
    "        }\n",
    "\n",
    "    # 结果汇总步骤\n",
    "    def summarize_results(x):\n",
    "        summary = {\n",
    "            \"workflow_id\": str(uuid.uuid4())[:8],\n",
    "            \"completed_at\": datetime.now().isoformat(),\n",
    "            \"status\": \"success\" if x[\"is_valid\"] else \"failed\",\n",
    "            \"validation_errors\": x.get(\"validation_errors\", []),\n",
    "            \"content_stats\": x.get(\"processed_content\"),\n",
    "            \"ai_analysis\": x.get(\"ai_result\"),\n",
    "            \"original_data\": x[\"data\"]\n",
    "        }\n",
    "\n",
    "        return {**x, \"step\": \"summary\", \"final_result\": summary}\n",
    "\n",
    "    # 构建工作流链\n",
    "    workflow_chain = (\n",
    "        RunnableLambda(validate_data)\n",
    "        | RunnableLambda(process_content)\n",
    "        | RunnableLambda(ai_analysis)\n",
    "        | RunnableLambda(summarize_results)\n",
    "    )\n",
    "\n",
    "    # 测试工作流\n",
    "    test_cases = [\n",
    "        {\n",
    "            \"data\": {\n",
    "                \"title\": \"重要会议通知\",\n",
    "                \"content\": \"明天上午10点在会议室A召开重要的项目讨论会议，请所有相关人员准时参加。\"\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"data\": {\n",
    "                \"title\": \"\",  # 无效数据\n",
    "                \"content\": \"短内容\"\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"data\": {\n",
    "                \"title\": \"日常工作报告\",\n",
    "                \"content\": \"今天完成了数据分析任务，发现了一些有趣的趋势。团队协作良好，项目进展顺利。明天计划继续优化算法性能。\"\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    for i, test_case in enumerate(test_cases, 1):\n",
    "        print(f\"\\n工作流{i}：\")\n",
    "        print(f\"输入：{test_case['data']}\")\n",
    "\n",
    "        result = workflow_chain.invoke(test_case)\n",
    "        final_result = result[\"final_result\"]\n",
    "\n",
    "        print(f\"工作流ID：{final_result['workflow_id']}\")\n",
    "        print(f\"状态：{final_result['status']}\")\n",
    "\n",
    "        if final_result[\"validation_errors\"]:\n",
    "            print(f\"验证错误：{final_result['validation_errors']}\")\n",
    "\n",
    "        if final_result[\"content_stats\"]:\n",
    "            print(f\"内容统计：{final_result['content_stats']}\")\n",
    "\n",
    "        if final_result[\"ai_analysis\"]:\n",
    "            print(f\"AI分析：{final_result['ai_analysis']}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 14. 实时数据处理管道\n",
    "# ============================================================================\n",
    "\n",
    "def realtime_pipeline_example():\n",
    "    \"\"\"实时数据处理管道示例\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"14. 实时数据处理管道\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    llm = create_llm()\n",
    "\n",
    "    # 模拟实时数据源\n",
    "    def generate_realtime_data():\n",
    "        import random\n",
    "        data_types = [\"user_action\", \"system_event\", \"error_log\", \"performance_metric\"]\n",
    "\n",
    "        return {\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"type\": random.choice(data_types),\n",
    "            \"user_id\": f\"user_{random.randint(1000, 9999)}\",\n",
    "            \"value\": random.randint(1, 100),\n",
    "            \"message\": f\"随机事件消息 {random.randint(1, 1000)}\"\n",
    "        }\n",
    "\n",
    "    # 数据过滤器\n",
    "    def filter_data(x):\n",
    "        # 过滤掉低价值数据\n",
    "        if x[\"value\"] < 20:\n",
    "            return None\n",
    "        return x\n",
    "\n",
    "    # 数据增强\n",
    "    def enrich_data(x):\n",
    "        if x is None:\n",
    "            return None\n",
    "\n",
    "        # 添加计算字段\n",
    "        x[\"priority\"] = \"high\" if x[\"value\"] > 80 else \"medium\" if x[\"value\"] > 50 else \"low\"\n",
    "        x[\"category\"] = x[\"type\"].replace(\"_\", \" \").title()\n",
    "        x[\"processed_at\"] = datetime.now().isoformat()\n",
    "\n",
    "        return x\n",
    "\n",
    "    # 异常检测\n",
    "    def detect_anomalies(x):\n",
    "        if x is None:\n",
    "            return None\n",
    "\n",
    "        # 简单的异常检测逻辑\n",
    "        is_anomaly = (\n",
    "            x[\"type\"] == \"error_log\" or\n",
    "            x[\"value\"] > 95 or\n",
    "            \"error\" in x[\"message\"].lower()\n",
    "        )\n",
    "\n",
    "        x[\"is_anomaly\"] = is_anomaly\n",
    "        x[\"alert_level\"] = \"critical\" if is_anomaly else \"normal\"\n",
    "\n",
    "        return x\n",
    "\n",
    "    # AI 分析（仅对异常数据）\n",
    "    def ai_analyze_if_needed(x):\n",
    "        if x is None or not x.get(\"is_anomaly\"):\n",
    "            return x\n",
    "\n",
    "        analysis_prompt = f\"\"\"\n",
    "分析以下异常事件：\n",
    "类型：{x['type']}\n",
    "用户：{x['user_id']}\n",
    "数值：{x['value']}\n",
    "消息：{x['message']}\n",
    "优先级：{x['priority']}\n",
    "\n",
    "请提供：\n",
    "1. 可能的原因\n",
    "2. 建议的处理方案\n",
    "3. 风险评估\n",
    "\"\"\"\n",
    "\n",
    "        try:\n",
    "            ai_analysis = (\n",
    "                PromptTemplate.from_template(analysis_prompt)\n",
    "                | llm\n",
    "                | StrOutputParser()\n",
    "            ).invoke({})\n",
    "            x[\"ai_analysis\"] = ai_analysis\n",
    "        except Exception as e:\n",
    "            x[\"ai_analysis\"] = f\"AI分析失败：{e}\"\n",
    "\n",
    "        return x\n",
    "\n",
    "    # 构建实时处理管道\n",
    "    pipeline = (\n",
    "        RunnableLambda(filter_data)\n",
    "        | RunnableLambda(enrich_data)\n",
    "        | RunnableLambda(detect_anomalies)\n",
    "        | RunnableLambda(ai_analyze_if_needed)\n",
    "    )\n",
    "\n",
    "    # 模拟实时数据流处理\n",
    "    print(\"开始实时数据处理...\")\n",
    "    processed_count = 0\n",
    "    anomaly_count = 0\n",
    "\n",
    "    for i in range(10):  # 处理10条数据\n",
    "        raw_data = generate_realtime_data()\n",
    "        print(f\"\\n原始数据{i+1}：{raw_data}\")\n",
    "\n",
    "        result = pipeline.invoke(raw_data)\n",
    "\n",
    "        if result is not None:\n",
    "            processed_count += 1\n",
    "            print(f\"处理结果：优先级={result['priority']}, 异常={result['is_anomaly']}\")\n",
    "\n",
    "            if result.get(\"is_anomaly\"):\n",
    "                anomaly_count += 1\n",
    "                print(f\"异常分析：{result.get('ai_analysis', '无')}\")\n",
    "        else:\n",
    "            print(\"数据被过滤\")\n",
    "\n",
    "    print(f\"\\n处理统计：\")\n",
    "    print(f\"总处理数据：{processed_count}\")\n",
    "    print(f\"检测到异常：{anomaly_count}\")\n",
    "\n",
    "# 运行专业示例\n",
    "def run_professional_examples():\n",
    "    \"\"\"运行专业应用示例\"\"\"\n",
    "    print(\"LCEL 专业应用场景示例\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    try:\n",
    "        rag_system_example()\n",
    "        multimodal_processing_example()\n",
    "        workflow_orchestration_example()\n",
    "        realtime_pipeline_example()\n",
    "\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"所有专业示例运行完成！\")\n",
    "        print(\"=\" * 60)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"运行出错：{e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_professional_examples()"
   ],
   "id": "e2aae5168173baa4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "LCEL 最佳实践总结\n",
    "1. 选择合适的组合方式\n",
    "- 简单链：使用 | 操作符\n",
    "- 并行处理：使用 RunnableParallel\n",
    "- 条件分支：使用 RunnableBranch\n",
    "- 数据传递：使用 RunnablePassthrough\n",
    "2. 性能优化\n",
    "- 利用批量处理：batch() 和 abatch()\n",
    "- 使用流式处理：stream() 和 astream()\n",
    "- 合理使用并行：避免不必要的串行处理\n",
    "3. 错误处理\n",
    "- 实现重试机制\n",
    "- 提供备用处理链\n",
    "- 优雅的错误降级\n",
    "4. 可维护性\n",
    "- 模块化设计：将复杂逻辑拆分为小的 Runnable\n",
    "- 清晰的数据流：使用 RunnablePassthrough.assign 管理状态\n",
    "- 充分的日志和监控\n",
    "5. 扩展性\n",
    "- 动态链构建：根据配置构建不同的处理链\n",
    "- 插件化架构：易于添加新的处理步骤\n",
    "- 标准化接口：所有组件都实现 Runnable 接口\n",
    "LCEL 为构建复杂的 AI 应用提供了强大而灵活的框架，通过合理使用这些模式，可以构建出高性能、可维护的应用系统。"
   ],
   "id": "5e194b96c1fc54f1"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
