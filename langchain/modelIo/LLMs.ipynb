{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# LLMs - 大语言模型接口详解\n",
    "\n",
    "## 概述\n",
    "\n",
    "LLMs（Large Language Models）是 LangChain 中用于与传统文本生成模型交互的接口。与 Chat Models 不同，LLMs 接受字符串作为输入并返回字符串，更适合文本补全任务。\n",
    "\n",
    "### LLMs vs Chat Models 对比\n",
    "\n",
    "| 特性 | LLMs | Chat Models |\n",
    "|------|------|-------------|\n",
    "| 输入格式 | 字符串 | 消息列表 |\n",
    "| 输出格式 | 字符串 | 消息对象 |\n",
    "| 适用场景 | 文本补全、生成 | 对话、聊天 |\n",
    "| 接口方法 | `invoke()`, `generate()` | `invoke()`, `stream()` |\n",
    "\n",
    "### 核心特性\n",
    "\n",
    "- **文本补全**: 基于提示词生成文本\n",
    "- **批量生成**: 支持批量文本生成\n",
    "- **流式输出**: 支持实时流式生成\n",
    "- **参数控制**: 温度、最大长度等参数\n",
    "- **缓存支持**: 内置缓存机制\n",
    "- **回调系统**: 支持生成过程监控"
   ],
   "id": "a7f4d5171c04550"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 完整代码示例",
   "id": "5511703dee5eb371"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 引入依赖",
   "id": "d63679f0557679d0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-22T07:03:34.955609Z",
     "start_time": "2025-07-22T07:03:32.970336Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "LangChain 0.3 LLMs 完整示例\n",
    "\"\"\"\n",
    "# 引入依赖\n",
    "# from langchain_community.llms import Ollama\n",
    "from langchain_ollama import OllamaLLM\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableParallel\n",
    "from langchain_core.callbacks import BaseCallbackHandler\n",
    "from langchain.cache import InMemoryCache\n",
    "from langchain.globals import set_llm_cache\n",
    "import asyncio\n",
    "import time\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "# 设置缓存\n",
    "set_llm_cache(InMemoryCache())"
   ],
   "id": "bd3e8282de31bc7a",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 1. 基础 LLM 使用",
   "id": "79da8d95326e1b07"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "#### `invoke()` vs `generate()`\n",
    "在 `LangChain` 中使用 `OllamaLLM`（或更广义地，所有 LLM 接口如 `ChatOpenAI`、`ChatOllama`）时，常会看到两个方法：\n",
    "\n",
    "---\n",
    "\n",
    "✅ 1. `invoke()`\n",
    "\n",
    "**用途：执行一个完整的调用链，通常配合 LCEL 使用。**\n",
    "\n",
    "* `invoke()` 是 LangChain `Runnable` 接口的方法，用于**单次输入、单次输出**。\n",
    "* 输入是一个**完整结构**（如包含 prompt 字段或 messages），LangChain 会负责组织成模型能理解的格式。\n",
    "\n",
    "🔧 示例：\n",
    "\n",
    "```python\n",
    "from langchain.chat_models import ChatOllama\n",
    "\n",
    "llm = ChatOllama(model=\"llama3\")\n",
    "\n",
    "response = llm.invoke(\"你好，介绍一下你自己\")\n",
    "print(response.content)\n",
    "```\n",
    "\n",
    "📌 等价于说：“我给你一句话，你回复一句话。” —— 通常**返回的是 Message 对象**。\n",
    "\n",
    "---\n",
    "\n",
    " ✅ 2. `generate()`\n",
    "\n",
    "**用途：批量生成，适用于多个输入生成多个输出。**\n",
    "\n",
    "* `generate()` 更底层，接受多个输入（列表），返回结构化的结果（含 token 使用量、messages 等）。\n",
    "* 返回的是 `LLMResult` 对象，适合做**批量推理、统计分析、缓存优化**等。\n",
    "\n",
    "🔧 示例：\n",
    "\n",
    "```python\n",
    "from langchain.chat_models import ChatOllama\n",
    "\n",
    "llm = ChatOllama(model=\"llama3\")\n",
    "\n",
    "results = llm.generate([\n",
    "    [\"你好，介绍一下你自己\"],\n",
    "    [\"讲一个笑话\"]\n",
    "])\n",
    "\n",
    "for generation in results.generations:\n",
    "    print(generation[0].text)\n",
    "```\n",
    "\n",
    "📌 一次生成多个回复，并返回详细信息（如 `generation_info`、token数等）。\n",
    "\n",
    "---\n",
    "\n",
    "✨ 总结对比\n",
    "\n",
    "| 对比项  | `invoke()`              | `generate()`                  |\n",
    "| ---- | ----------------------- | ----------------------------- |\n",
    "| 输入格式 | 单条文本或结构化输入（字典、message）  | 列表（多个输入）                      |\n",
    "| 返回类型 | 通常是 `AIMessage` / `str` | `LLMResult`（包含多个 generations） |\n",
    "| 使用场景 | 单次调用（配合 LCEL、Runnable）  | 批量生成（如评估任务、缓存）                |\n",
    "| 易用性  | ✅ 更适合简单使用               | 🛠️ 更适合底层控制、多样化需求             |\n",
    "\n",
    "---\n",
    "\n"
   ],
   "id": "8d1ce0ed72f4ab7c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### basic_llm_usage",
   "id": "ed0ed64ed3114869"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-22T06:50:29.552739Z",
     "start_time": "2025-07-22T06:50:29.482364Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# 1. 基础 LLM 使用\n",
    "def basic_llm_usage():\n",
    "    \"\"\"基础 LLM 使用示例\"\"\"\n",
    "    print(\"=\" * 50)\n",
    "    print(\"1. 基础 LLM 使用\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    # 初始化 Ollama LLM\n",
    "    llm = OllamaLLM(\n",
    "        base_url=\"http://localhost:11434\",\n",
    "        model=\"qwen2.5:3b\",\n",
    "        temperature=0.7\n",
    "    )\n",
    "\n",
    "    # 基础文本生成\n",
    "    prompt = \"请写一个关于人工智能的简短介绍：\"\n",
    "    response = llm.invoke(prompt)\n",
    "    print(f\"生成文本: {response}\")\n",
    "\n",
    "    # 使用 generate 方法\n",
    "    prompts = [\n",
    "        \"Python是什么？\",\n",
    "        \"机器学习的定义是什么？\",\n",
    "        \"深度学习有哪些应用？\"\n",
    "    ]\n",
    "\n",
    "    generations = llm.generate(prompts)\n",
    "    print(f\"\\n批量生成结果:\")\n",
    "    for i, gen in enumerate(generations.generations):\n",
    "        print(f\"问题{i+1}: {gen[0].text[:100]}...\")\n",
    "basic_llm_usage()"
   ],
   "id": "4c0018842c29b904",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "1. 基础 LLM 使用\n",
      "==================================================\n",
      "生成文本: 人工智能（Artificial Intelligence，简称AI）是指通过模拟、扩展和增强人类智能的技术。它利用计算技术实现智能机器系统，使机器能够在无需人工干预的情况下执行复杂任务，并具备学习能力。\n",
      "\n",
      "在人工智能领域中，机器学习是一种核心方法，允许计算机从数据中自动学习并提取规律或特征，而不需要显式编程。深度学习是机器学习的一个分支，它使用多层神经网络来模仿人脑如何处理信息，从而实现对复杂数据的高效理解和预测。\n",
      "\n",
      "除此之外，自然语言处理使得计算机能够理解、解释和生成人类语言；计算机视觉技术让机器能够“看”并从中提取有用的信息；强化学习则使机器能够在环境中通过与环境交互以获得奖励或惩罚的方式进行学习。\n",
      "\n",
      "随着算法和技术的进步，人工智能已经在医疗诊断、自动驾驶汽车、语音识别以及复杂的决策支持系统等多个领域取得了显著的应用成果。此外，人工智能还在不断地挑战和拓展人类的认知边界，并且为社会带来了前所未有的变革与发展机遇。\n",
      "\n",
      "批量生成结果:\n",
      "问题1: Python是一种高级编程语言，由Guido van Rossum于1980年代末开始设计和开发。它最初被称为“Active”但很快改名为“Python”，取自英国喜剧小品演员Eric Idle的名字...\n",
      "问题2: 机器学习是一种人工智能（AI）的技术，使计算机能够在不进行明确编程的情况下通过数据学习和改进。简单来说，就是让计算机从历史数据中自动识别模式、提取特征，并利用这些信息来做出预测或决策。\n",
      "\n",
      "在实际应用中...\n",
      "问题3: 深度学习是一种机器学习的方法，它已经在多个领域都有了广泛的应用。以下是一些主要的应用场景：\n",
      "\n",
      "1. 语音识别：深度学习在这一领域的应用已经非常成熟和成功，无论是基于手机的数字助理、智能音箱还是专业的语...\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 2. 流式生成示例",
   "id": "8ca64718ca2eae16"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-22T06:55:05.207306Z",
     "start_time": "2025-07-22T06:54:30.417724Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# 2. 流式生成示例\n",
    "def streaming_generation():\n",
    "    \"\"\"流式生成示例\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"2. 流式生成示例\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    llm = OllamaLLM(\n",
    "        base_url=\"http://localhost:11434\",\n",
    "        model=\"qwen2.5:3b\"\n",
    "    )\n",
    "\n",
    "    prompt = \"请详细解释什么是区块链技术：\"\n",
    "\n",
    "    print(\"流式输出:\")\n",
    "    for chunk in llm.stream(prompt):\n",
    "        print(chunk, end=\"\", flush=True)\n",
    "    print(\"\\n\")\n",
    "streaming_generation()"
   ],
   "id": "3f1a56bb01a27fdf",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "2. 流式生成示例\n",
      "==================================================\n",
      "流式输出:\n",
      "区块链是一种去中心化的数据记录方式，它通过密码学方法确保信息的安全性和完整性，并且所有的交易信息都会被分布式地保存在一个公开的数据库中。以下是对区块链技术的详细解释：\n",
      "\n",
      "1. 去中心化架构：\n",
      "传统的数据库系统大多由单个组织或机构控制，因此存在风险：如单一节点失败、人为错误和数据篡改等。而区块链则采用去中心化的形式，每个参与者都是网络的一部分，无需依赖任何中央权威机构。\n",
      "\n",
      "2. 分布式记账技术：\n",
      "在区块链中，所有的交易记录都被分布在多个节点上进行保存，并通过密码学手段保证记录的安全性和完整性。这意味着即使有一个或几个节点出现故障或者被破坏，也不会对整个系统的正常运行造成影响。\n",
      "\n",
      "3. 加密算法和共识机制：\n",
      "每笔交易都会被打包成区块并在整个网络中进行广播以供其他节点验证。一旦多个节点都确认了交易的有效性，就会创建一个新的区块并将其添加到区块链的末端。这种技术确保了数据的安全性和防篡改特性。\n",
      "\n",
      "4. 匿名性和透明度：\n",
      "虽然每个用户都有一个独特的公钥身份用于与其他用户的交互和管理资金，但用户的真实姓名和个人信息通常不会直接公开在公共链上。同时，由于每笔交易都会被记录下来并可供所有参与者查看，因此可以保证整个系统的透明性。\n",
      "\n",
      "5. 智能合约：\n",
      "智能合约是指自动执行的代码，在满足特定条件时会触发预设的操作或行动。它们通常部署在区块链网络之上，并能够与现有的金融服务集成使用。\n",
      "\n",
      "6. 跨链通信：\n",
      "目前大多数区块链系统是独立运行并互不兼容的，这意味着不同网络之间的数据交换存在挑战。为了解决这个问题，跨链技术允许在不同的区块链之间传输信息和价值。\n",
      "\n",
      "通过上述特点，区块链技术已被广泛应用于数字货币、供应链管理以及智能合约等领域，并展现出巨大的潜力和发展前景。\n",
      "\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 3. 使用提示模板",
   "id": "e9912694a84a7acd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# 3. 使用提示模板\n",
    "def with_prompt_templates():\n",
    "    \"\"\"使用提示模板\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"3. 使用提示模板\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    llm = OllamaLLM(\n",
    "        base_url=\"http://localhost:11434\",\n",
    "        model=\"qwen2.5:3b\"\n",
    "    )\n",
    "\n",
    "    # 创建提示模板\n",
    "    template = \"\"\"\n",
    "    作为一个{role}，请用{style}的风格回答以下问题：\n",
    "\n",
    "    问题：{question}\n",
    "\n",
    "    回答：\n",
    "    \"\"\"\n",
    "\n",
    "    prompt = PromptTemplate(\n",
    "        input_variables=[\"role\", \"style\", \"question\"],\n",
    "        template=template\n",
    "    )\n",
    "\n",
    "    # 创建链\n",
    "    chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "    # 测试不同角色和风格\n",
    "    examples = [\n",
    "        {\"role\": \"历史学家\", \"style\": \"学术\", \"question\": \"秦始皇统一中国的意义\"},\n",
    "        {\"role\": \"诗人\", \"style\": \"浪漫\", \"question\": \"春天的特点\"},\n",
    "        {\"role\": \"程序员\", \"style\": \"技术\", \"question\": \"什么是算法复杂度\"}\n",
    "    ]\n",
    "\n",
    "    for example in examples:\n",
    "        response = chain.invoke(example)\n",
    "        print(f\"\\n{example['role']}({example['style']})回答:\")\n",
    "        print(response[:200] + \"...\")"
   ],
   "id": "77c325c464045e18",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 4. 批量处理优化",
   "id": "a13852bfd359eda4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "# 4. 批量处理优化\n",
    "def batch_processing():\n",
    "    \"\"\"批量处理示例\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"4. 批量处理示例\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    llm = OllamaLLM(\n",
    "        base_url=\"http://localhost:11434\",\n",
    "        model=\"qwen2.5:3b\"\n",
    "    )\n",
    "\n",
    "    # 批量提示\n",
    "    prompts = [\n",
    "        \"解释什么是机器学习\",\n",
    "        \"Python的优势是什么\",\n",
    "        \"什么是云计算\",\n",
    "        \"区块链的应用场景\",\n",
    "        \"人工智能的发展历程\"\n",
    "    ]\n",
    "\n",
    "    # 计时批量处理\n",
    "    start_time = time.time()\n",
    "    responses = llm.batch(prompts)\n",
    "    batch_time = time.time() - start_time\n",
    "\n",
    "    print(f\"批量处理{len(prompts)}个请求耗时: {batch_time:.2f}秒\")\n",
    "\n",
    "    for i, response in enumerate(responses):\n",
    "        print(f\"\\n问题{i+1}: {prompts[i]}\")\n",
    "        print(f\"回答: {response[:100]}...\")"
   ],
   "id": "a7b506e56533e663",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 5. 参数调优示例",
   "id": "f978c6860150b7ce"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "# 5. 参数调优示例\n",
    "def parameter_tuning():\n",
    "    \"\"\"参数调优示例\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"5. 参数调优示例\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    prompt = \"创作一个科幻故事的开头：\"\n",
    "\n",
    "    # 不同温度设置\n",
    "    temperatures = [0.1, 0.5, 0.9, 1.2]\n",
    "\n",
    "    for temp in temperatures:\n",
    "        llm = Ollama(\n",
    "            base_url=\"http://localhost:11434\",\n",
    "            model=\"qwen2.5:3b\",\n",
    "            temperature=temp\n",
    "        )\n",
    "\n",
    "        response = llm.invoke(prompt)\n",
    "        print(f\"\\n温度 {temp}:\")\n",
    "        print(response[:150] + \"...\")"
   ],
   "id": "78d04c6cd28edd48",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 6. 自定义回调处理",
   "id": "29bb15f31d8d2db7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-22T06:57:01.688375Z",
     "start_time": "2025-07-22T06:56:47.985626Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 6. 自定义回调处理\n",
    "class CustomCallbackHandler(BaseCallbackHandler):\n",
    "    \"\"\"自定义回调处理器\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.tokens = []\n",
    "        self.start_time = None\n",
    "\n",
    "    def on_llm_start(self, serialized, prompts, **kwargs):\n",
    "        self.start_time = time.time()\n",
    "        print(f\"开始生成，提示数量: {len(prompts)}\")\n",
    "\n",
    "    def on_llm_new_token(self, token: str, **kwargs):\n",
    "        self.tokens.append(token)\n",
    "        print(f\"新token: '{token}'\", end=\"\")\n",
    "\n",
    "    def on_llm_end(self, response, **kwargs):\n",
    "        duration = time.time() - self.start_time\n",
    "        total_tokens = len(self.tokens)\n",
    "        print(f\"\\n生成完成，耗时: {duration:.2f}秒，token数: {total_tokens}\")\n",
    "        self.tokens = []\n",
    "\n",
    "def callback_example():\n",
    "    \"\"\"回调示例\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"6. 回调示例\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    llm = OllamaLLM(\n",
    "        base_url=\"http://localhost:11434\",\n",
    "        model=\"qwen2.5:3b\"\n",
    "    )\n",
    "\n",
    "    callback_handler = CustomCallbackHandler()\n",
    "\n",
    "    response = llm.invoke(\n",
    "        \"请简单介绍一下量子计算\",\n",
    "        config={\"callbacks\": [callback_handler]}\n",
    "    )\n",
    "callback_example()"
   ],
   "id": "7921569a8c0d2b48",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "6. 回调示例\n",
      "==================================================\n",
      "开始生成，提示数量: 1\n",
      "新token: '量子'新token: '计算'新token: '是一种'新token: '基于'新token: '量子'新token: '力学'新token: '原理'新token: '的'新token: '计算'新token: '方式'新token: '，'新token: '它'新token: '利用'新token: '量子'新token: '位'新token: '（'新token: '量子'新token: '比特'新token: '或'新token: 'q'新token: 'ubit'新token: '）'新token: '来'新token: '存储'新token: '和'新token: '处理'新token: '信息'新token: '。'新token: '与'新token: '经典'新token: '计算机'新token: '使用的'新token: '二'新token: '进'新token: '制'新token: '位'新token: '（'新token: '0'新token: ' 或'新token: ' '新token: '1'新token: '）'新token: '不同'新token: '的是'新token: '，'新token: '量子'新token: '位'新token: '可以'新token: '同时'新token: '表示'新token: '0'新token: ' 和'新token: ' '新token: '1'新token: ' 的'新token: '叠加'新token: '态'新token: '，'新token: '这'新token: '被称为'新token: '“'新token: '量子'新token: '叠加'新token: '”。\n",
      "\n",
      "'新token: '此外'新token: '，'新token: '量子'新token: '比特'新token: '还可以'新token: '实现'新token: '一种'新token: '称为'新token: '“'新token: '量子'新token: '纠缠'新token: '”的'新token: '现象'新token: '，在'新token: '这种'新token: '状态下'新token: '，'新token: '一个'新token: '量子'新token: '位'新token: '的状态'新token: '会'新token: '依赖'新token: '于'新token: '另一个'新token: '或'新token: '多个'新token: '量子'新token: '位'新token: '的状态'新token: '。'新token: '即使'新token: '在'新token: '遥远'新token: '的距离'新token: '上'新token: '，'新token: '量子'新token: '位'新token: '之间的'新token: '状态'新token: '也会'新token: '保持'新token: '相关'新token: '性'新token: '。\n",
      "\n",
      "'新token: '这些'新token: '特性'新token: '使得'新token: '量子'新token: '计算机'新token: '在'新token: '处理'新token: '某些'新token: '特定'新token: '类型'新token: '的问题'新token: '时'新token: '可能'新token: '表现出'新token: '远远'新token: '超过'新token: '经典'新token: '计算机'新token: '的速度'新token: '优势'新token: '。'新token: '例如'新token: '，'新token: '对于'新token: '那些'新token: '具有'新token: '指数'新token: '级'新token: '复杂'新token: '度'新token: '的经典'新token: '问题'新token: '，'新token: '如果'新token: '找到'新token: '合适的'新token: '算法'新token: '，'新token: '量子'新token: '计算机'新token: '可能'新token: '能够在'新token: '多项'新token: '式'新token: '时间内'新token: '解决'新token: '这些问题'新token: '。\n",
      "\n",
      "'新token: '尽管'新token: '如此'新token: '，'新token: '目前'新token: '实现'新token: '高效的'新token: '、'新token: '大规模'新token: '的'新token: '量子'新token: '计算'新token: '仍'新token: '面临'新token: '诸多'新token: '挑战'新token: '，'新token: '包括'新token: '如何'新token: '创建'新token: '和'新token: '保持'新token: '量子'新token: '态'新token: '稳定'新token: '等'新token: '技术'新token: '难题'新token: '。'新token: '量子'新token: '计算'新token: '的研究'新token: '还在'新token: '持续'新token: '发展中'新token: '，'新token: '未来'新token: '可能会'新token: '为'新token: '信息'新token: '处理'新token: '带来'新token: '革命'新token: '性的'新token: '变化'新token: '。'新token: ''\n",
      "生成完成，耗时: 13.63秒，token数: 204\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 7. 缓存机制示例",
   "id": "579b8d9465f7af09"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-22T06:59:43.708908Z",
     "start_time": "2025-07-22T06:59:34.661157Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# 7. 缓存机制示例\n",
    "def caching_example():\n",
    "    \"\"\"缓存机制示例\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"7. 缓存机制示例\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    llm = OllamaLLM(\n",
    "        base_url=\"http://localhost:11434\",\n",
    "        model=\"qwen2.5:3b\"\n",
    "    )\n",
    "\n",
    "    prompt = \"什么是深度学习？\"\n",
    "\n",
    "    # 第一次调用（无缓存）\n",
    "    start_time = time.time()\n",
    "    response1 = llm.invoke(prompt)\n",
    "    first_call_time = time.time() - start_time\n",
    "\n",
    "    # 第二次调用（有缓存）\n",
    "    start_time = time.time()\n",
    "    response2 = llm.invoke(prompt)\n",
    "    second_call_time = time.time() - start_time\n",
    "\n",
    "    print(f\"第一次调用耗时: {first_call_time:.2f}秒\")\n",
    "    print(f\"第二次调用耗时: {second_call_time:.2f}秒\")\n",
    "    print(f\"缓存加速比: {first_call_time/second_call_time:.2f}x\")\n",
    "    print(f\"响应一致性: {response1 == response2}\")\n",
    "caching_example()"
   ],
   "id": "122fc8163db7d2ae",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "7. 缓存机制示例\n",
      "==================================================\n",
      "第一次调用耗时: 8.98秒\n",
      "第二次调用耗时: 0.00秒\n",
      "缓存加速比: 36483.76x\n",
      "响应一致性: True\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 8. 异步处理示例",
   "id": "60a2cdfb6e99cbb1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-22T07:07:48.853620Z",
     "start_time": "2025-07-22T07:06:53.559302Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 8. 异步处理示例\n",
    "async def async_llm_example():\n",
    "    \"\"\"异步处理示例\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"8. 异步处理示例\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    llm = OllamaLLM(\n",
    "        base_url=\"http://localhost:11434\",\n",
    "        model=\"qwen2.5:3b\"\n",
    "    )\n",
    "\n",
    "    # 异步单次调用\n",
    "    response = await llm.ainvoke(\"异步调用测试\")\n",
    "    print(f\"异步响应: {response[:100]}...\")\n",
    "\n",
    "    # 异步批量调用\n",
    "    prompts = [\n",
    "        \"什么是API？\",\n",
    "        \"什么是数据库？\",\n",
    "        \"什么是网络协议？\"\n",
    "    ]\n",
    "\n",
    "    start_time = time.time()\n",
    "    responses = await llm.abatch(prompts)\n",
    "    async_time = time.time() - start_time\n",
    "\n",
    "    print(f\"\\n异步批量处理耗时: {async_time:.2f}秒\")\n",
    "    for i, response in enumerate(responses):\n",
    "        print(f\"问题{i+1}回答: {response[:80]}...\")\n",
    "\n",
    "    # 异步流式处理\n",
    "    print(\"\\n异步流式输出:\")\n",
    "    async for chunk in llm.astream(\"请介绍一下云原生技术\"):\n",
    "        print(chunk, end=\"\", flush=True)\n",
    "    print(\"\\n\")\n",
    "\n",
    "# Jupyter Notebook 中的正确运行方式\n",
    "print(\"\\n运行异步示例...\")\n",
    "await async_llm_example()"
   ],
   "id": "110c1d84fd062be8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "运行异步示例...\n",
      "\n",
      "==================================================\n",
      "8. 异步处理示例\n",
      "==================================================\n",
      "异步响应: 异步调用是一种在不阻塞主线程的情况下进行请求或执行操作的技术。它允许一个操作开始后，即使客户端还在等待其他任务完成时，另一个任务已经开始处理。这通常用于提高用户体验和系统的响应性。下面我将通过几个示例...\n"
     ]
    },
    {
     "ename": "CancelledError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mCancelledError\u001B[39m                            Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[7]\u001B[39m\u001B[32m, line 40\u001B[39m\n\u001B[32m     38\u001B[39m \u001B[38;5;66;03m# Jupyter Notebook 中的正确运行方式\u001B[39;00m\n\u001B[32m     39\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m运行异步示例...\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m---> \u001B[39m\u001B[32m40\u001B[39m \u001B[38;5;28;01mawait\u001B[39;00m async_llm_example()\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[7]\u001B[39m\u001B[32m, line 25\u001B[39m, in \u001B[36masync_llm_example\u001B[39m\u001B[34m()\u001B[39m\n\u001B[32m     18\u001B[39m prompts = [\n\u001B[32m     19\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m什么是API？\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m     20\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m什么是数据库？\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m     21\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33m什么是网络协议？\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m     22\u001B[39m ]\n\u001B[32m     24\u001B[39m start_time = time.time()\n\u001B[32m---> \u001B[39m\u001B[32m25\u001B[39m responses = \u001B[38;5;28;01mawait\u001B[39;00m llm.abatch(prompts)\n\u001B[32m     26\u001B[39m async_time = time.time() - start_time\n\u001B[32m     28\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m异步批量处理耗时: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00masync_time\u001B[38;5;132;01m:\u001B[39;00m\u001B[33m.2f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m秒\u001B[39m\u001B[33m\"\u001B[39m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/FastAPIProject/.venv/lib/python3.11/site-packages/langchain_core/language_models/llms.py:488\u001B[39m, in \u001B[36mBaseLLM.abatch\u001B[39m\u001B[34m(self, inputs, config, return_exceptions, **kwargs)\u001B[39m\n\u001B[32m    486\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m max_concurrency \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m    487\u001B[39m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m488\u001B[39m         llm_result = \u001B[38;5;28;01mawait\u001B[39;00m \u001B[38;5;28mself\u001B[39m.agenerate_prompt(\n\u001B[32m    489\u001B[39m             [\u001B[38;5;28mself\u001B[39m._convert_input(input_) \u001B[38;5;28;01mfor\u001B[39;00m input_ \u001B[38;5;129;01min\u001B[39;00m inputs],\n\u001B[32m    490\u001B[39m             callbacks=[c.get(\u001B[33m\"\u001B[39m\u001B[33mcallbacks\u001B[39m\u001B[33m\"\u001B[39m) \u001B[38;5;28;01mfor\u001B[39;00m c \u001B[38;5;129;01min\u001B[39;00m config],\n\u001B[32m    491\u001B[39m             tags=[c.get(\u001B[33m\"\u001B[39m\u001B[33mtags\u001B[39m\u001B[33m\"\u001B[39m) \u001B[38;5;28;01mfor\u001B[39;00m c \u001B[38;5;129;01min\u001B[39;00m config],\n\u001B[32m    492\u001B[39m             metadata=[c.get(\u001B[33m\"\u001B[39m\u001B[33mmetadata\u001B[39m\u001B[33m\"\u001B[39m) \u001B[38;5;28;01mfor\u001B[39;00m c \u001B[38;5;129;01min\u001B[39;00m config],\n\u001B[32m    493\u001B[39m             run_name=[c.get(\u001B[33m\"\u001B[39m\u001B[33mrun_name\u001B[39m\u001B[33m\"\u001B[39m) \u001B[38;5;28;01mfor\u001B[39;00m c \u001B[38;5;129;01min\u001B[39;00m config],\n\u001B[32m    494\u001B[39m             **kwargs,\n\u001B[32m    495\u001B[39m         )\n\u001B[32m    496\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m [g[\u001B[32m0\u001B[39m].text \u001B[38;5;28;01mfor\u001B[39;00m g \u001B[38;5;129;01min\u001B[39;00m llm_result.generations]\n\u001B[32m    497\u001B[39m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/FastAPIProject/.venv/lib/python3.11/site-packages/langchain_core/language_models/llms.py:777\u001B[39m, in \u001B[36mBaseLLM.agenerate_prompt\u001B[39m\u001B[34m(self, prompts, stop, callbacks, **kwargs)\u001B[39m\n\u001B[32m    768\u001B[39m \u001B[38;5;129m@override\u001B[39m\n\u001B[32m    769\u001B[39m \u001B[38;5;28;01masync\u001B[39;00m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34magenerate_prompt\u001B[39m(\n\u001B[32m    770\u001B[39m     \u001B[38;5;28mself\u001B[39m,\n\u001B[32m   (...)\u001B[39m\u001B[32m    774\u001B[39m     **kwargs: Any,\n\u001B[32m    775\u001B[39m ) -> LLMResult:\n\u001B[32m    776\u001B[39m     prompt_strings = [p.to_string() \u001B[38;5;28;01mfor\u001B[39;00m p \u001B[38;5;129;01min\u001B[39;00m prompts]\n\u001B[32m--> \u001B[39m\u001B[32m777\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mawait\u001B[39;00m \u001B[38;5;28mself\u001B[39m.agenerate(\n\u001B[32m    778\u001B[39m         prompt_strings, stop=stop, callbacks=callbacks, **kwargs\n\u001B[32m    779\u001B[39m     )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/FastAPIProject/.venv/lib/python3.11/site-packages/langchain_core/language_models/llms.py:1249\u001B[39m, in \u001B[36mBaseLLM.agenerate\u001B[39m\u001B[34m(self, prompts, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001B[39m\n\u001B[32m   1235\u001B[39m run_managers = \u001B[38;5;28;01mawait\u001B[39;00m asyncio.gather(\n\u001B[32m   1236\u001B[39m     *[\n\u001B[32m   1237\u001B[39m         callback_managers[idx].on_llm_start(\n\u001B[32m   (...)\u001B[39m\u001B[32m   1246\u001B[39m     ]\n\u001B[32m   1247\u001B[39m )\n\u001B[32m   1248\u001B[39m run_managers = [r[\u001B[32m0\u001B[39m] \u001B[38;5;28;01mfor\u001B[39;00m r \u001B[38;5;129;01min\u001B[39;00m run_managers]  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m1249\u001B[39m new_results = \u001B[38;5;28;01mawait\u001B[39;00m \u001B[38;5;28mself\u001B[39m._agenerate_helper(\n\u001B[32m   1250\u001B[39m     missing_prompts,\n\u001B[32m   1251\u001B[39m     stop,\n\u001B[32m   1252\u001B[39m     run_managers,  \u001B[38;5;66;03m# type: ignore[arg-type]\u001B[39;00m\n\u001B[32m   1253\u001B[39m     new_arg_supported=\u001B[38;5;28mbool\u001B[39m(new_arg_supported),\n\u001B[32m   1254\u001B[39m     **kwargs,\n\u001B[32m   1255\u001B[39m )\n\u001B[32m   1256\u001B[39m llm_output = \u001B[38;5;28;01mawait\u001B[39;00m aupdate_cache(\n\u001B[32m   1257\u001B[39m     \u001B[38;5;28mself\u001B[39m.cache,\n\u001B[32m   1258\u001B[39m     existing_prompts,\n\u001B[32m   (...)\u001B[39m\u001B[32m   1262\u001B[39m     prompts,\n\u001B[32m   1263\u001B[39m )\n\u001B[32m   1264\u001B[39m run_info = (\n\u001B[32m   1265\u001B[39m     [RunInfo(run_id=run_manager.run_id) \u001B[38;5;28;01mfor\u001B[39;00m run_manager \u001B[38;5;129;01min\u001B[39;00m run_managers]  \u001B[38;5;66;03m# type: ignore[attr-defined]\u001B[39;00m\n\u001B[32m   1266\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m run_managers\n\u001B[32m   1267\u001B[39m     \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1268\u001B[39m )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/FastAPIProject/.venv/lib/python3.11/site-packages/langchain_core/language_models/llms.py:1043\u001B[39m, in \u001B[36mBaseLLM._agenerate_helper\u001B[39m\u001B[34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001B[39m\n\u001B[32m   1032\u001B[39m \u001B[38;5;28;01masync\u001B[39;00m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m_agenerate_helper\u001B[39m(\n\u001B[32m   1033\u001B[39m     \u001B[38;5;28mself\u001B[39m,\n\u001B[32m   1034\u001B[39m     prompts: \u001B[38;5;28mlist\u001B[39m[\u001B[38;5;28mstr\u001B[39m],\n\u001B[32m   (...)\u001B[39m\u001B[32m   1039\u001B[39m     **kwargs: Any,\n\u001B[32m   1040\u001B[39m ) -> LLMResult:\n\u001B[32m   1041\u001B[39m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m   1042\u001B[39m         output = (\n\u001B[32m-> \u001B[39m\u001B[32m1043\u001B[39m             \u001B[38;5;28;01mawait\u001B[39;00m \u001B[38;5;28mself\u001B[39m._agenerate(\n\u001B[32m   1044\u001B[39m                 prompts,\n\u001B[32m   1045\u001B[39m                 stop=stop,\n\u001B[32m   1046\u001B[39m                 run_manager=run_managers[\u001B[32m0\u001B[39m] \u001B[38;5;28;01mif\u001B[39;00m run_managers \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[32m   1047\u001B[39m                 **kwargs,\n\u001B[32m   1048\u001B[39m             )\n\u001B[32m   1049\u001B[39m             \u001B[38;5;28;01mif\u001B[39;00m new_arg_supported\n\u001B[32m   1050\u001B[39m             \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mawait\u001B[39;00m \u001B[38;5;28mself\u001B[39m._agenerate(prompts, stop=stop)\n\u001B[32m   1051\u001B[39m         )\n\u001B[32m   1052\u001B[39m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[32m   1053\u001B[39m         \u001B[38;5;28;01mawait\u001B[39;00m asyncio.gather(\n\u001B[32m   1054\u001B[39m             *[\n\u001B[32m   1055\u001B[39m                 run_manager.on_llm_error(e, response=LLMResult(generations=[]))\n\u001B[32m   1056\u001B[39m                 \u001B[38;5;28;01mfor\u001B[39;00m run_manager \u001B[38;5;129;01min\u001B[39;00m run_managers\n\u001B[32m   1057\u001B[39m             ]\n\u001B[32m   1058\u001B[39m         )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/FastAPIProject/.venv/lib/python3.11/site-packages/langchain_ollama/llms.py:378\u001B[39m, in \u001B[36mOllamaLLM._agenerate\u001B[39m\u001B[34m(self, prompts, stop, run_manager, **kwargs)\u001B[39m\n\u001B[32m    376\u001B[39m generations = []\n\u001B[32m    377\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m prompt \u001B[38;5;129;01min\u001B[39;00m prompts:\n\u001B[32m--> \u001B[39m\u001B[32m378\u001B[39m     final_chunk = \u001B[38;5;28;01mawait\u001B[39;00m \u001B[38;5;28mself\u001B[39m._astream_with_aggregation(\n\u001B[32m    379\u001B[39m         prompt,\n\u001B[32m    380\u001B[39m         stop=stop,\n\u001B[32m    381\u001B[39m         run_manager=run_manager,\n\u001B[32m    382\u001B[39m         verbose=\u001B[38;5;28mself\u001B[39m.verbose,\n\u001B[32m    383\u001B[39m         **kwargs,\n\u001B[32m    384\u001B[39m     )\n\u001B[32m    385\u001B[39m     generations.append([final_chunk])\n\u001B[32m    386\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m LLMResult(generations=generations)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/FastAPIProject/.venv/lib/python3.11/site-packages/langchain_ollama/llms.py:276\u001B[39m, in \u001B[36mOllamaLLM._astream_with_aggregation\u001B[39m\u001B[34m(self, prompt, stop, run_manager, verbose, **kwargs)\u001B[39m\n\u001B[32m    274\u001B[39m final_chunk = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m    275\u001B[39m thinking_content = \u001B[33m\"\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m--> \u001B[39m\u001B[32m276\u001B[39m \u001B[38;5;28;01masync\u001B[39;00m \u001B[38;5;28;01mfor\u001B[39;00m stream_resp \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m._acreate_generate_stream(prompt, stop, **kwargs):\n\u001B[32m    277\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(stream_resp, \u001B[38;5;28mstr\u001B[39m):\n\u001B[32m    278\u001B[39m         \u001B[38;5;28;01mif\u001B[39;00m stream_resp.get(\u001B[33m\"\u001B[39m\u001B[33mthinking\u001B[39m\u001B[33m\"\u001B[39m):\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/FastAPIProject/.venv/lib/python3.11/site-packages/langchain_ollama/llms.py:250\u001B[39m, in \u001B[36mOllamaLLM._acreate_generate_stream\u001B[39m\u001B[34m(self, prompt, stop, **kwargs)\u001B[39m\n\u001B[32m    243\u001B[39m \u001B[38;5;28;01masync\u001B[39;00m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m_acreate_generate_stream\u001B[39m(\n\u001B[32m    244\u001B[39m     \u001B[38;5;28mself\u001B[39m,\n\u001B[32m    245\u001B[39m     prompt: \u001B[38;5;28mstr\u001B[39m,\n\u001B[32m    246\u001B[39m     stop: Optional[\u001B[38;5;28mlist\u001B[39m[\u001B[38;5;28mstr\u001B[39m]] = \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[32m    247\u001B[39m     **kwargs: Any,\n\u001B[32m    248\u001B[39m ) -> AsyncIterator[Union[Mapping[\u001B[38;5;28mstr\u001B[39m, Any], \u001B[38;5;28mstr\u001B[39m]]:\n\u001B[32m    249\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m._async_client:\n\u001B[32m--> \u001B[39m\u001B[32m250\u001B[39m         \u001B[38;5;28;01masync\u001B[39;00m \u001B[38;5;28;01mfor\u001B[39;00m part \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28;01mawait\u001B[39;00m \u001B[38;5;28mself\u001B[39m._async_client.generate(\n\u001B[32m    251\u001B[39m             **\u001B[38;5;28mself\u001B[39m._generate_params(prompt, stop=stop, **kwargs)\n\u001B[32m    252\u001B[39m         ):\n\u001B[32m    253\u001B[39m             \u001B[38;5;28;01myield\u001B[39;00m part\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/FastAPIProject/.venv/lib/python3.11/site-packages/ollama/_client.py:684\u001B[39m, in \u001B[36mAsyncClient._request.<locals>.inner\u001B[39m\u001B[34m()\u001B[39m\n\u001B[32m    681\u001B[39m   \u001B[38;5;28;01mawait\u001B[39;00m e.response.aread()\n\u001B[32m    682\u001B[39m   \u001B[38;5;28;01mraise\u001B[39;00m ResponseError(e.response.text, e.response.status_code) \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m684\u001B[39m \u001B[38;5;28;01masync\u001B[39;00m \u001B[38;5;28;01mfor\u001B[39;00m line \u001B[38;5;129;01min\u001B[39;00m r.aiter_lines():\n\u001B[32m    685\u001B[39m   part = json.loads(line)\n\u001B[32m    686\u001B[39m   \u001B[38;5;28;01mif\u001B[39;00m err := part.get(\u001B[33m'\u001B[39m\u001B[33merror\u001B[39m\u001B[33m'\u001B[39m):\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/FastAPIProject/.venv/lib/python3.11/site-packages/httpx/_models.py:1031\u001B[39m, in \u001B[36mResponse.aiter_lines\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m   1029\u001B[39m decoder = LineDecoder()\n\u001B[32m   1030\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m request_context(request=\u001B[38;5;28mself\u001B[39m._request):\n\u001B[32m-> \u001B[39m\u001B[32m1031\u001B[39m     \u001B[38;5;28;01masync\u001B[39;00m \u001B[38;5;28;01mfor\u001B[39;00m text \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m.aiter_text():\n\u001B[32m   1032\u001B[39m         \u001B[38;5;28;01mfor\u001B[39;00m line \u001B[38;5;129;01min\u001B[39;00m decoder.decode(text):\n\u001B[32m   1033\u001B[39m             \u001B[38;5;28;01myield\u001B[39;00m line\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/FastAPIProject/.venv/lib/python3.11/site-packages/httpx/_models.py:1018\u001B[39m, in \u001B[36mResponse.aiter_text\u001B[39m\u001B[34m(self, chunk_size)\u001B[39m\n\u001B[32m   1016\u001B[39m chunker = TextChunker(chunk_size=chunk_size)\n\u001B[32m   1017\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m request_context(request=\u001B[38;5;28mself\u001B[39m._request):\n\u001B[32m-> \u001B[39m\u001B[32m1018\u001B[39m     \u001B[38;5;28;01masync\u001B[39;00m \u001B[38;5;28;01mfor\u001B[39;00m byte_content \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m.aiter_bytes():\n\u001B[32m   1019\u001B[39m         text_content = decoder.decode(byte_content)\n\u001B[32m   1020\u001B[39m         \u001B[38;5;28;01mfor\u001B[39;00m chunk \u001B[38;5;129;01min\u001B[39;00m chunker.decode(text_content):\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/FastAPIProject/.venv/lib/python3.11/site-packages/httpx/_models.py:997\u001B[39m, in \u001B[36mResponse.aiter_bytes\u001B[39m\u001B[34m(self, chunk_size)\u001B[39m\n\u001B[32m    995\u001B[39m chunker = ByteChunker(chunk_size=chunk_size)\n\u001B[32m    996\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m request_context(request=\u001B[38;5;28mself\u001B[39m._request):\n\u001B[32m--> \u001B[39m\u001B[32m997\u001B[39m     \u001B[38;5;28;01masync\u001B[39;00m \u001B[38;5;28;01mfor\u001B[39;00m raw_bytes \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m.aiter_raw():\n\u001B[32m    998\u001B[39m         decoded = decoder.decode(raw_bytes)\n\u001B[32m    999\u001B[39m         \u001B[38;5;28;01mfor\u001B[39;00m chunk \u001B[38;5;129;01min\u001B[39;00m chunker.decode(decoded):\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/FastAPIProject/.venv/lib/python3.11/site-packages/httpx/_models.py:1055\u001B[39m, in \u001B[36mResponse.aiter_raw\u001B[39m\u001B[34m(self, chunk_size)\u001B[39m\n\u001B[32m   1052\u001B[39m chunker = ByteChunker(chunk_size=chunk_size)\n\u001B[32m   1054\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m request_context(request=\u001B[38;5;28mself\u001B[39m._request):\n\u001B[32m-> \u001B[39m\u001B[32m1055\u001B[39m     \u001B[38;5;28;01masync\u001B[39;00m \u001B[38;5;28;01mfor\u001B[39;00m raw_stream_bytes \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m.stream:\n\u001B[32m   1056\u001B[39m         \u001B[38;5;28mself\u001B[39m._num_bytes_downloaded += \u001B[38;5;28mlen\u001B[39m(raw_stream_bytes)\n\u001B[32m   1057\u001B[39m         \u001B[38;5;28;01mfor\u001B[39;00m chunk \u001B[38;5;129;01min\u001B[39;00m chunker.decode(raw_stream_bytes):\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/FastAPIProject/.venv/lib/python3.11/site-packages/httpx/_client.py:176\u001B[39m, in \u001B[36mBoundAsyncStream.__aiter__\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    175\u001B[39m \u001B[38;5;28;01masync\u001B[39;00m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m__aiter__\u001B[39m(\u001B[38;5;28mself\u001B[39m) -> typing.AsyncIterator[\u001B[38;5;28mbytes\u001B[39m]:\n\u001B[32m--> \u001B[39m\u001B[32m176\u001B[39m     \u001B[38;5;28;01masync\u001B[39;00m \u001B[38;5;28;01mfor\u001B[39;00m chunk \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m._stream:\n\u001B[32m    177\u001B[39m         \u001B[38;5;28;01myield\u001B[39;00m chunk\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/FastAPIProject/.venv/lib/python3.11/site-packages/httpx/_transports/default.py:271\u001B[39m, in \u001B[36mAsyncResponseStream.__aiter__\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    269\u001B[39m \u001B[38;5;28;01masync\u001B[39;00m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m__aiter__\u001B[39m(\u001B[38;5;28mself\u001B[39m) -> typing.AsyncIterator[\u001B[38;5;28mbytes\u001B[39m]:\n\u001B[32m    270\u001B[39m     \u001B[38;5;28;01mwith\u001B[39;00m map_httpcore_exceptions():\n\u001B[32m--> \u001B[39m\u001B[32m271\u001B[39m         \u001B[38;5;28;01masync\u001B[39;00m \u001B[38;5;28;01mfor\u001B[39;00m part \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m._httpcore_stream:\n\u001B[32m    272\u001B[39m             \u001B[38;5;28;01myield\u001B[39;00m part\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/FastAPIProject/.venv/lib/python3.11/site-packages/httpcore/_async/connection_pool.py:407\u001B[39m, in \u001B[36mPoolByteStream.__aiter__\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    405\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m exc:\n\u001B[32m    406\u001B[39m     \u001B[38;5;28;01mawait\u001B[39;00m \u001B[38;5;28mself\u001B[39m.aclose()\n\u001B[32m--> \u001B[39m\u001B[32m407\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m exc \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/FastAPIProject/.venv/lib/python3.11/site-packages/httpcore/_async/connection_pool.py:403\u001B[39m, in \u001B[36mPoolByteStream.__aiter__\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    401\u001B[39m \u001B[38;5;28;01masync\u001B[39;00m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m__aiter__\u001B[39m(\u001B[38;5;28mself\u001B[39m) -> typing.AsyncIterator[\u001B[38;5;28mbytes\u001B[39m]:\n\u001B[32m    402\u001B[39m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m403\u001B[39m         \u001B[38;5;28;01masync\u001B[39;00m \u001B[38;5;28;01mfor\u001B[39;00m part \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m._stream:\n\u001B[32m    404\u001B[39m             \u001B[38;5;28;01myield\u001B[39;00m part\n\u001B[32m    405\u001B[39m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m exc:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/FastAPIProject/.venv/lib/python3.11/site-packages/httpcore/_async/http11.py:342\u001B[39m, in \u001B[36mHTTP11ConnectionByteStream.__aiter__\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    340\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m AsyncShieldCancellation():\n\u001B[32m    341\u001B[39m     \u001B[38;5;28;01mawait\u001B[39;00m \u001B[38;5;28mself\u001B[39m.aclose()\n\u001B[32m--> \u001B[39m\u001B[32m342\u001B[39m \u001B[38;5;28;01mraise\u001B[39;00m exc\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/FastAPIProject/.venv/lib/python3.11/site-packages/httpcore/_async/http11.py:334\u001B[39m, in \u001B[36mHTTP11ConnectionByteStream.__aiter__\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    332\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m    333\u001B[39m     \u001B[38;5;28;01masync\u001B[39;00m \u001B[38;5;28;01mwith\u001B[39;00m Trace(\u001B[33m\"\u001B[39m\u001B[33mreceive_response_body\u001B[39m\u001B[33m\"\u001B[39m, logger, \u001B[38;5;28mself\u001B[39m._request, kwargs):\n\u001B[32m--> \u001B[39m\u001B[32m334\u001B[39m         \u001B[38;5;28;01masync\u001B[39;00m \u001B[38;5;28;01mfor\u001B[39;00m chunk \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m._connection._receive_response_body(**kwargs):\n\u001B[32m    335\u001B[39m             \u001B[38;5;28;01myield\u001B[39;00m chunk\n\u001B[32m    336\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m exc:\n\u001B[32m    337\u001B[39m     \u001B[38;5;66;03m# If we get an exception while streaming the response,\u001B[39;00m\n\u001B[32m    338\u001B[39m     \u001B[38;5;66;03m# we want to close the response (and possibly the connection)\u001B[39;00m\n\u001B[32m    339\u001B[39m     \u001B[38;5;66;03m# before raising that exception.\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/FastAPIProject/.venv/lib/python3.11/site-packages/httpcore/_async/http11.py:203\u001B[39m, in \u001B[36mAsyncHTTP11Connection._receive_response_body\u001B[39m\u001B[34m(self, request)\u001B[39m\n\u001B[32m    200\u001B[39m timeout = timeouts.get(\u001B[33m\"\u001B[39m\u001B[33mread\u001B[39m\u001B[33m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[32m    202\u001B[39m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m203\u001B[39m     event = \u001B[38;5;28;01mawait\u001B[39;00m \u001B[38;5;28mself\u001B[39m._receive_event(timeout=timeout)\n\u001B[32m    204\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(event, h11.Data):\n\u001B[32m    205\u001B[39m         \u001B[38;5;28;01myield\u001B[39;00m \u001B[38;5;28mbytes\u001B[39m(event.data)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/FastAPIProject/.venv/lib/python3.11/site-packages/httpcore/_async/http11.py:217\u001B[39m, in \u001B[36mAsyncHTTP11Connection._receive_event\u001B[39m\u001B[34m(self, timeout)\u001B[39m\n\u001B[32m    214\u001B[39m     event = \u001B[38;5;28mself\u001B[39m._h11_state.next_event()\n\u001B[32m    216\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m event \u001B[38;5;129;01mis\u001B[39;00m h11.NEED_DATA:\n\u001B[32m--> \u001B[39m\u001B[32m217\u001B[39m     data = \u001B[38;5;28;01mawait\u001B[39;00m \u001B[38;5;28mself\u001B[39m._network_stream.read(\n\u001B[32m    218\u001B[39m         \u001B[38;5;28mself\u001B[39m.READ_NUM_BYTES, timeout=timeout\n\u001B[32m    219\u001B[39m     )\n\u001B[32m    221\u001B[39m     \u001B[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001B[39;00m\n\u001B[32m    222\u001B[39m     \u001B[38;5;66;03m#\u001B[39;00m\n\u001B[32m    223\u001B[39m     \u001B[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m    227\u001B[39m     \u001B[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001B[39;00m\n\u001B[32m    228\u001B[39m     \u001B[38;5;66;03m# it as a ConnectError.\u001B[39;00m\n\u001B[32m    229\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m data == \u001B[33mb\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m\"\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m._h11_state.their_state == h11.SEND_RESPONSE:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/FastAPIProject/.venv/lib/python3.11/site-packages/httpcore/_backends/anyio.py:35\u001B[39m, in \u001B[36mAnyIOStream.read\u001B[39m\u001B[34m(self, max_bytes, timeout)\u001B[39m\n\u001B[32m     33\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m anyio.fail_after(timeout):\n\u001B[32m     34\u001B[39m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m---> \u001B[39m\u001B[32m35\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mawait\u001B[39;00m \u001B[38;5;28mself\u001B[39m._stream.receive(max_bytes=max_bytes)\n\u001B[32m     36\u001B[39m     \u001B[38;5;28;01mexcept\u001B[39;00m anyio.EndOfStream:  \u001B[38;5;66;03m# pragma: nocover\u001B[39;00m\n\u001B[32m     37\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[33mb\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m\"\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/FastAPIProject/.venv/lib/python3.11/site-packages/anyio/_backends/_asyncio.py:1254\u001B[39m, in \u001B[36mSocketStream.receive\u001B[39m\u001B[34m(self, max_bytes)\u001B[39m\n\u001B[32m   1248\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[32m   1249\u001B[39m     \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m._protocol.read_event.is_set()\n\u001B[32m   1250\u001B[39m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m._transport.is_closing()\n\u001B[32m   1251\u001B[39m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m._protocol.is_at_eof\n\u001B[32m   1252\u001B[39m ):\n\u001B[32m   1253\u001B[39m     \u001B[38;5;28mself\u001B[39m._transport.resume_reading()\n\u001B[32m-> \u001B[39m\u001B[32m1254\u001B[39m     \u001B[38;5;28;01mawait\u001B[39;00m \u001B[38;5;28mself\u001B[39m._protocol.read_event.wait()\n\u001B[32m   1255\u001B[39m     \u001B[38;5;28mself\u001B[39m._transport.pause_reading()\n\u001B[32m   1256\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/usr/local/Cellar/python@3.11/3.11.12_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/locks.py:213\u001B[39m, in \u001B[36mEvent.wait\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    211\u001B[39m \u001B[38;5;28mself\u001B[39m._waiters.append(fut)\n\u001B[32m    212\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m213\u001B[39m     \u001B[38;5;28;01mawait\u001B[39;00m fut\n\u001B[32m    214\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[32m    215\u001B[39m \u001B[38;5;28;01mfinally\u001B[39;00m:\n",
      "\u001B[31mCancelledError\u001B[39m: "
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 9. 复杂链式处理",
   "id": "15308c4ec31d49db"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-22T07:05:49.053221Z",
     "start_time": "2025-07-22T07:05:07.615328Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# 9. 复杂链式处理\n",
    "def complex_chain_example():\n",
    "    \"\"\"复杂链式处理示例\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"9. 复杂链式处理示例\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    llm = OllamaLLM(\n",
    "        base_url=\"http://localhost:11434\",\n",
    "        model=\"qwen2.5:3b\"\n",
    "    )\n",
    "\n",
    "    # 创建多步骤处理链\n",
    "    step1_template = PromptTemplate(\n",
    "        input_variables=[\"topic\"],\n",
    "        template=\"请列出关于{topic}的5个关键点：\"\n",
    "    )\n",
    "\n",
    "    step2_template = PromptTemplate(\n",
    "        input_variables=[\"key_points\"],\n",
    "        template=\"基于以下关键点，写一个简短的总结：\\n{key_points}\\n\\n总结：\"\n",
    "    )\n",
    "\n",
    "    # 构建链\n",
    "    def process_topic(topic: str) -> str:\n",
    "        # 第一步：生成关键点\n",
    "        key_points = (step1_template | llm).invoke({\"topic\": topic})\n",
    "\n",
    "        # 第二步：生成总结\n",
    "        summary = (step2_template | llm).invoke({\"key_points\": key_points})\n",
    "\n",
    "        return {\n",
    "            \"topic\": topic,\n",
    "            \"key_points\": key_points,\n",
    "            \"summary\": summary\n",
    "        }\n",
    "\n",
    "    # 测试\n",
    "    result = process_topic(\"人工智能\")\n",
    "    print(f\"主题: {result['topic']}\")\n",
    "    print(f\"关键点: {result['key_points'][:200]}...\")\n",
    "    print(f\"总结: {result['summary'][:200]}...\")\n",
    "complex_chain_example()"
   ],
   "id": "4c48d3e63f1250c6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "9. 复杂链式处理示例\n",
      "==================================================\n",
      "主题: 人工智能\n",
      "关键点: 当然，关于人工智能（AI）的五个关键点如下：\n",
      "\n",
      "1. **智能自动化**：人工智能技术让机器能够模拟人类的认知过程，通过学习、推理和自我修正来完成复杂的任务。这不仅包括语音识别、图像处理等感知功能，还涉及自然语言理解与生成。\n",
      "\n",
      "2. **数据驱动**：人工智能的发展高度依赖于大数据的支持，大量的数据用于训练模型并不断提高算法的准确性和效率。从社交媒体信息到医疗记录、电子商务交易等各种领域的数据都能...\n",
      "总结: 以下是一个基于您提供的五个关键点的简短总结：\n",
      "\n",
      "人工智能（AI）正通过智能自动化、数据驱动和机器学习等核心技术改变我们的生活和工作方式。它依赖于大数据的支持，并在伦理与隐私问题以及对社会影响方面带来了一系列挑战。AI不仅重塑了各个行业的运作模式，还推动教育体系变革以培养适应新时代的技能人才。此外，跨学科合作是推动人工智能发展的关键因素。\n",
      "\n",
      "这些核心要点有助于我们全面理解AI技术及其未来的发展趋势和...\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 10. 并行处理示例",
   "id": "ce878f26c6ca5807"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "# 10. 并行处理示例\n",
    "def parallel_processing():\n",
    "    \"\"\"并行处理示例\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"10. 并行处理示例\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    llm = Ollama(\n",
    "        base_url=\"http://localhost:11434\",\n",
    "        model=\"qwen2.5:3b\"\n",
    "    )\n",
    "\n",
    "    # 创建并行处理链\n",
    "    parallel_chain = RunnableParallel({\n",
    "        \"definition\": PromptTemplate.from_template(\"定义：{concept}是什么？\") | llm,\n",
    "        \"advantages\": PromptTemplate.from_template(\"优势：{concept}有什么优势？\") | llm,\n",
    "        \"applications\": PromptTemplate.from_template(\"应用：{concept}有哪些应用场景？\") | llm,\n",
    "        \"challenges\": PromptTemplate.from_template(\"挑战：{concept}面临哪些挑战？\") | llm\n",
    "    })\n",
    "\n",
    "    # 执行并行处理\n",
    "    concept = \"区块链技术\"\n",
    "    start_time = time.time()\n",
    "    results = parallel_chain.invoke({\"concept\": concept})\n",
    "    parallel_time = time.time() - start_time\n",
    "\n",
    "    print(f\"并行处理耗时: {parallel_time:.2f}秒\")\n",
    "    print(f\"\\n概念: {concept}\")\n",
    "\n",
    "    for aspect, content in results.items():\n",
    "        print(f\"\\n{aspect}: {content[:100]}...\")"
   ],
   "id": "8b6eaba3722f0432",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 11. 错误处理和重试",
   "id": "2fecbb9524b6e1ba"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "# 11. 错误处理和重试\n",
    "def error_handling_example():\n",
    "    \"\"\"错误处理示例\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"11. 错误处理示例\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    # 正常模型\n",
    "    llm = Ollama(\n",
    "        base_url=\"http://localhost:11434\",\n",
    "        model=\"qwen2.5:3b\"\n",
    "    )\n",
    "\n",
    "    # 错误模型（不存在）\n",
    "    error_llm = Ollama(\n",
    "        base_url=\"http://localhost:11434\",\n",
    "        model=\"nonexistent-model\"\n",
    "    )\n",
    "\n",
    "    def safe_invoke(llm, prompt: str, max_retries: int = 3):\n",
    "        \"\"\"安全调用函数\"\"\"\n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                response = llm.invoke(prompt)\n",
    "                return response\n",
    "            except Exception as e:\n",
    "                print(f\"尝试 {attempt + 1} 失败: {e}\")\n",
    "                if attempt == max_retries - 1:\n",
    "                    return f\"所有尝试都失败了，最后错误: {e}\"\n",
    "                time.sleep(1)  # 等待重试\n",
    "\n",
    "    # 测试正常调用\n",
    "    response = safe_invoke(llm, \"测试正常调用\")\n",
    "    print(f\"正常调用结果: {response[:50]}...\")\n",
    "\n",
    "    # 测试错误处理\n",
    "    error_response = safe_invoke(error_llm, \"测试错误处理\")\n",
    "    print(f\"错误处理结果: {error_response}\")"
   ],
   "id": "a549054828385b10",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 12. 自定义输出格式化",
   "id": "f486c5f8b1664db7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "# 12. 自定义输出格式化\n",
    "def custom_output_formatting():\n",
    "    \"\"\"自定义输出格式化\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"12. 自定义输出格式化\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    llm = Ollama(\n",
    "        base_url=\"http://localhost:11434\",\n",
    "        model=\"qwen2.5:3b\"\n",
    "    )\n",
    "\n",
    "    # 自定义格式化器\n",
    "    class JSONFormatter:\n",
    "        def format(self, text: str, metadata: Dict[str, Any] = None) -> Dict[str, Any]:\n",
    "            return {\n",
    "                \"content\": text.strip(),\n",
    "                \"length\": len(text),\n",
    "                \"word_count\": len(text.split()),\n",
    "                \"metadata\": metadata or {}\n",
    "            }\n",
    "\n",
    "    formatter = JSONFormatter()\n",
    "\n",
    "    # 创建格式化链\n",
    "    def formatted_invoke(prompt: str) -> Dict[str, Any]:\n",
    "        response = llm.invoke(prompt)\n",
    "        return formatter.format(response, {\"prompt\": prompt})\n",
    "\n",
    "    # 测试\n",
    "    result = formatted_invoke(\"什么是微服务架构？\")\n",
    "    print(f\"格式化结果:\")\n",
    "    for key, value in result.items():\n",
    "        if key == \"content\":\n",
    "            print(f\"{key}: {str(value)[:100]}...\")\n",
    "        else:\n",
    "            print(f\"{key}: {value}\")"
   ],
   "id": "c2737976566ca7b3",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 不同模型提供商示例",
   "id": "269c9bfb3dac107d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "## 不同模型提供商示例\n",
    "\n",
    "\"\"\"\n",
    "不同 LLM 提供商示例\n",
    "\"\"\"\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain_openai import OpenAI\n",
    "from langchain_community.llms import Bedrock\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "def ollama_example():\n",
    "    \"\"\"Ollama 本地模型示例\"\"\"\n",
    "    print(\"=\" * 30)\n",
    "    print(\"Ollama 示例\")\n",
    "    print(\"=\" * 30)\n",
    "\n",
    "    llm = Ollama(\n",
    "        base_url=\"http://localhost:11434\",\n",
    "        model=\"qwen2.5:3b\",\n",
    "        temperature=0.7\n",
    "    )\n",
    "\n",
    "    response = llm.invoke(\"解释什么是容器化技术\")\n",
    "    print(f\"Ollama 响应: {response[:100]}...\")\n",
    "\n",
    "def openai_example():\n",
    "    \"\"\"OpenAI 示例\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 30)\n",
    "    print(\"OpenAI 示例\")\n",
    "    print(\"=\" * 30)\n",
    "\n",
    "    try:\n",
    "        llm = OpenAI(\n",
    "            model=\"gpt-3.5-turbo-instruct\",\n",
    "            temperature=0.7,\n",
    "            max_tokens=100\n",
    "        )\n",
    "\n",
    "        response = llm.invoke(\"解释什么是容器化技术\")\n",
    "        print(f\"OpenAI 响应: {response}\")\n",
    "    except Exception as e:\n",
    "        print(f\"OpenAI 调用失败: {e}\")\n",
    "\n",
    "def bedrock_example():\n",
    "    \"\"\"AWS Bedrock 示例\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 30)\n",
    "    print(\"AWS Bedrock 示例\")\n",
    "    print(\"=\" * 30)\n",
    "\n",
    "    try:\n",
    "        llm = Bedrock(\n",
    "            model_id=\"anthropic.claude-v2\",\n",
    "            region_name=\"us-east-1\"\n",
    "        )\n",
    "\n",
    "        # Claude 需要特定格式\n",
    "        prompt = \"Human: 解释什么是容器化技术\\n\\nAssistant:\"\n",
    "        response = llm.invoke(prompt)\n",
    "        print(f\"Bedrock 响应: {response[:100]}...\")\n",
    "    except Exception as e:\n",
    "        print(f\"Bedrock 调用失败: {e}\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"运行所有提供商示例\"\"\"\n",
    "    ollama_example()\n",
    "    openai_example()\n",
    "    bedrock_example()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ],
   "id": "e937e9711b3a73e6",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 高级用法示例",
   "id": "229427b6e37bc956"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "\"\"\"\n",
    "LLM 高级用法示例\n",
    "\"\"\"\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain_core.prompts import PromptTemplate, FewShotPromptTemplate\n",
    "from langchain_core.example_selectors import SemanticSimilarityExampleSelector\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_core.runnables import RunnableBranch, RunnableLambda\n",
    "from typing import Dict, Any"
   ],
   "id": "a69c6abe65fc64ef",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 少样本学习示例",
   "id": "8bbe86310b282793"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "def few_shot_learning():\n",
    "    \"\"\"少样本学习示例\"\"\"\n",
    "    print(\"=\" * 50)\n",
    "    print(\"少样本学习示例\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    llm = Ollama(\n",
    "        base_url=\"http://localhost:11434\",\n",
    "        model=\"qwen2.5:3b\"\n",
    "    )\n",
    "\n",
    "    # 定义示例\n",
    "    examples = [\n",
    "        {\n",
    "            \"question\": \"什么是Python？\",\n",
    "            \"answer\": \"Python是一种高级编程语言，以其简洁的语法和强大的功能而闻名。\"\n",
    "        },\n",
    "        {\n",
    "            \"question\": \"什么是JavaScript？\",\n",
    "            \"answer\": \"JavaScript是一种主要用于网页开发的编程语言，可以创建交互式网页。\"\n",
    "        },\n",
    "        {\n",
    "            \"question\": \"什么是SQL？\",\n",
    "            \"answer\": \"SQL是结构化查询语言，用于管理和操作关系数据库中的数据。\"\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    # 创建示例模板\n",
    "    example_template = \"\"\"\n",
    "    问题: {question}\n",
    "    答案: {answer}\n",
    "    \"\"\"\n",
    "\n",
    "    example_prompt = PromptTemplate(\n",
    "        input_variables=[\"question\", \"answer\"],\n",
    "        template=example_template\n",
    "    )\n",
    "\n",
    "    # 创建少样本提示模板\n",
    "    few_shot_prompt = FewShotPromptTemplate(\n",
    "        examples=examples,\n",
    "        example_prompt=example_prompt,\n",
    "        prefix=\"以下是一些问答示例：\",\n",
    "        suffix=\"问题: {input}\\n答案:\",\n",
    "        input_variables=[\"input\"]\n",
    "    )\n",
    "\n",
    "    # 创建链\n",
    "    chain = few_shot_prompt | llm\n",
    "\n",
    "    # 测试\n",
    "    response = chain.invoke({\"input\": \"什么是Java？\"})\n",
    "    print(f\"少样本学习回答: {response}\")"
   ],
   "id": "7559271211206b3b",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 条件路由示例",
   "id": "595d910b5b8f533a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "def conditional_routing():\n",
    "    \"\"\"条件路由示例\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"条件路由示例\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    llm = Ollama(\n",
    "        base_url=\"http://localhost:11434\",\n",
    "        model=\"qwen2.5:3b\"\n",
    "    )\n",
    "\n",
    "    # 定义不同类型的提示模板\n",
    "    tech_template = PromptTemplate(\n",
    "        input_variables=[\"question\"],\n",
    "        template=\"作为技术专家，请详细回答：{question}\"\n",
    "    )\n",
    "\n",
    "    general_template = PromptTemplate(\n",
    "        input_variables=[\"question\"],\n",
    "        template=\"请简单回答：{question}\"\n",
    "    )\n",
    "\n",
    "    creative_template = PromptTemplate(\n",
    "        input_variables=[\"question\"],\n",
    "        template=\"请用创意的方式回答：{question}\"\n",
    "    )\n",
    "\n",
    "    # 路由函数\n",
    "    def route_question(input_dict: Dict[str, Any]) -> str:\n",
    "        question = input_dict[\"question\"].lower()\n",
    "        if any(word in question for word in [\"编程\", \"技术\", \"算法\", \"代码\"]):\n",
    "            return \"tech\"\n",
    "        elif any(word in question for word in [\"创意\", \"故事\", \"诗歌\", \"想象\"]):\n",
    "            return \"creative\"\n",
    "        else:\n",
    "            return \"general\"\n",
    "\n",
    "    # 创建条件分支\n",
    "    routing_chain = RunnableBranch(\n",
    "        (lambda x: route_question(x) == \"tech\", tech_template | llm),\n",
    "        (lambda x: route_question(x) == \"creative\", creative_template | llm),\n",
    "        general_template | llm  # 默认分支\n",
    "    )\n",
    "\n",
    "    # 测试不同类型的问题\n",
    "    questions = [\n",
    "        \"什么是机器学习算法？\",\n",
    "        \"写一个关于月亮的诗\",\n",
    "        \"今天天气怎么样？\"\n",
    "    ]\n",
    "\n",
    "    for question in questions:\n",
    "        route_type = route_question({\"question\": question})\n",
    "        response = routing_chain.invoke({\"question\": question})\n",
    "        print(f\"\\n问题类型: {route_type}\")\n",
    "        print(f\"问题: {question}\")\n",
    "        print(f\"回答: {response[:100]}...\")"
   ],
   "id": "943709ae17206e6a",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 记忆模拟示例",
   "id": "49f8657aea544a04"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "def memory_simulation():\n",
    "    \"\"\"记忆模拟示例\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"记忆模拟示例\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    llm = Ollama(\n",
    "        base_url=\"http://localhost:11434\",\n",
    "        model=\"qwen2.5:3b\"\n",
    "    )\n",
    "\n",
    "    # 简单的记忆存储\n",
    "    conversation_memory = []\n",
    "\n",
    "    def chat_with_memory(user_input: str) -> str:\n",
    "        # 构建包含历史的提示\n",
    "        context = \"\"\n",
    "        if conversation_memory:\n",
    "            context = \"对话历史：\\n\"\n",
    "            for i, (q, a) in enumerate(conversation_memory[-3:]):  # 只保留最近3轮\n",
    "                context += f\"用户: {q}\\nAI: {a}\\n\"\n",
    "            context += \"\\n\"\n",
    "\n",
    "        prompt = f\"{context}当前问题: {user_input}\\n请回答:\"\n",
    "\n",
    "        # 生成回答\n",
    "        response = llm.invoke(prompt)\n",
    "\n",
    "        # 保存到记忆\n",
    "        conversation_memory.append((user_input, response))\n",
    "\n",
    "        return response\n",
    "\n",
    "    # 模拟对话\n",
    "    conversation = [\n",
    "        \"我是一名软件工程师\",\n",
    "        \"我主要使用Python开发\",\n",
    "        \"你知道我的职业是什么吗？\",\n",
    "        \"我使用什么编程语言？\"\n",
    "    ]\n",
    "\n",
    "    for user_msg in conversation:\n",
    "        ai_response = chat_with_memory(user_msg)\n",
    "        print(f\"\\n用户: {user_msg}\")\n",
    "        print(f\"AI: {ai_response[:100]}...\")"
   ],
   "id": "19e80a4caab015bc",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "def main():\n",
    "    \"\"\"运行所有高级示例\"\"\"\n",
    "    few_shot_learning()\n",
    "    conditional_routing()\n",
    "    memory_simulation()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n"
   ],
   "id": "ee4816e8211319bb",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 性能优化示例",
   "id": "1fe96b5e88057fa3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "\"\"\"\n",
    "LLM 性能优化示例\n",
    "\"\"\"\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain.cache import InMemoryCache, SQLiteCache\n",
    "from langchain.globals import set_llm_cache\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "import time\n",
    "import asyncio\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from typing import List"
   ],
   "id": "fed37d4f183c96e6",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 缓存性能对比",
   "id": "df1b3aa81a8d1e4d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "def caching_comparison():\n",
    "    \"\"\"缓存性能对比\"\"\"\n",
    "    print(\"=\" * 50)\n",
    "    print(\"缓存性能对比\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    llm = Ollama(\n",
    "        base_url=\"http://localhost:11434\",\n",
    "        model=\"qwen2.5:3b\"\n",
    "    )\n",
    "\n",
    "    prompt = \"什么是人工智能？\"\n",
    "\n",
    "    # 无缓存测试\n",
    "    set_llm_cache(None)\n",
    "    start_time = time.time()\n",
    "    response1 = llm.invoke(prompt)\n",
    "    no_cache_time = time.time() - start_time\n",
    "\n",
    "    # 内存缓存测试\n",
    "    set_llm_cache(InMemoryCache())\n",
    "    start_time = time.time()\n",
    "    response2 = llm.invoke(prompt)  # 第一次调用\n",
    "    first_cache_time = time.time() - start_time\n",
    "\n",
    "    start_time = time.time()\n",
    "    response3 = llm.invoke(prompt)  # 第二次调用（命中缓存）\n",
    "    second_cache_time = time.time() - start_time\n",
    "\n",
    "    print(f\"无缓存耗时: {no_cache_time:.2f}秒\")\n",
    "    print(f\"首次缓存耗时: {first_cache_time:.2f}秒\")\n",
    "    print(f\"命中缓存耗时: {second_cache_time:.2f}秒\")\n",
    "    print(f\"缓存加速比: {no_cache_time/second_cache_time:.2f}x\")"
   ],
   "id": "644f4745215edfec"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 批量处理 vs 顺序处理性能对比",
   "id": "590d68531ffdba15"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "def batch_vs_sequential():\n",
    "    \"\"\"批量处理 vs 顺序处理性能对比\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"批量处理 vs 顺序处理性能对比\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    llm = Ollama(\n",
    "        base_url=\"http://localhost:11434\",\n",
    "        model=\"qwen2.5:3b\"\n",
    "    )\n",
    "\n",
    "    prompts = [\n",
    "        \"什么是云计算？\",\n",
    "        \"什么是大数据？\",\n",
    "        \"什么是物联网？\",\n",
    "        \"什么是5G技术？\",\n",
    "        \"什么是边缘计算？\"\n",
    "    ]\n",
    "\n",
    "    # 顺序处理\n",
    "    start_time = time.time()\n",
    "    sequential_responses = []\n",
    "    for prompt in prompts:\n",
    "        response = llm.invoke(prompt)\n",
    "        sequential_responses.append(response)\n",
    "    sequential_time = time.time() - start_time\n",
    "\n",
    "    # 批量处理\n",
    "    start_time = time.time()\n",
    "    batch_responses = llm.batch(prompts)\n",
    "    batch_time = time.time() - start_time\n",
    "\n",
    "    print(f\"顺序处理耗时: {sequential_time:.2f}秒\")\n",
    "    print(f\"批量处理耗时: {batch_time:.2f}秒\")\n",
    "    print(f\"批量处理加速比: {sequential_time/batch_time:.2f}x\")"
   ],
   "id": "39d4989bdce4ff35",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 异步 vs 同步性能对比",
   "id": "d492a91a90167aaa"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "async def async_vs_sync():\n",
    "    \"\"\"异步 vs 同步性能对比\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"异步 vs 同步性能对比\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    llm = Ollama(\n",
    "        base_url=\"http://localhost:11434\",\n",
    "        model=\"qwen2.5:3b\"\n",
    "    )\n",
    "\n",
    "    prompts = [\n",
    "        \"解释机器学习\",\n",
    "        \"解释深度学习\",\n",
    "        \"解释神经网络\"\n",
    "    ]\n",
    "\n",
    "    # 同步处理\n",
    "    start_time = time.time()\n",
    "    sync_responses = []\n",
    "    for prompt in prompts:\n",
    "        response = llm.invoke(prompt)\n",
    "        sync_responses.append(response)\n",
    "    sync_time = time.time() - start_time\n",
    "\n",
    "    # 异步处理\n",
    "    start_time = time.time()\n",
    "    async_responses = await llm.abatch(prompts)\n",
    "    async_time = time.time() - start_time\n",
    "\n",
    "    print(f\"同步处理耗时: {sync_time:.2f}秒\")\n",
    "    print(f\"异步处理耗时: {async_time:.2f}秒\")\n",
    "    print(f\"异步处理加速比: {sync_time/async_time:.2f}x\")"
   ],
   "id": "68c558efac841241",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 并发处理示例",
   "id": "411ce4231744e467"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "def concurrent_processing():\n",
    "    \"\"\"并发处理示例\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"并发处理示例\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    llm = Ollama(\n",
    "        base_url=\"http://localhost:11434\",\n",
    "        model=\"qwen2.5:3b\"\n",
    "    )\n",
    "\n",
    "    prompts = [\n",
    "        \"什么是区块链？\",\n",
    "        \"什么是量子计算？\",\n",
    "        \"什么是生物计算？\",\n",
    "        \"什么是光子计算？\"\n",
    "    ]\n",
    "\n",
    "    def process_prompt(prompt: str) -> str:\n",
    "        return llm.invoke(prompt)\n",
    "\n",
    "    # 并发处理\n",
    "    start_time = time.time()\n",
    "    with ThreadPoolExecutor(max_workers=4) as executor:\n",
    "        concurrent_responses = list(executor.map(process_prompt, prompts))\n",
    "    concurrent_time = time.time() - start_time\n",
    "\n",
    "    # 顺序处理对比\n",
    "    start_time = time.time()\n",
    "    sequential_responses = [process_prompt(prompt) for prompt in prompts]\n",
    "    sequential_time = time.time() - start_time\n",
    "\n",
    "    print(f\"并发处理耗时: {concurrent_time:.2f}秒\")\n",
    "    print(f\"顺序处理耗时: {sequential_time:.2f}秒\")\n",
    "    print(f\"并发处理加速比: {sequential_time/concurrent_time:.2f}x\")"
   ],
   "id": "facd63132763aa5f",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "def main():\n",
    "    \"\"\"运行所有性能优化示例\"\"\"\n",
    "    caching_comparison()\n",
    "    batch_vs_sequential()\n",
    "    concurrent_processing()\n",
    "\n",
    "    # 异步示例需要单独运行\n",
    "    print(\"\\n运行异步性能测试...\")\n",
    "    asyncio.run(async_vs_sync())\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ],
   "id": "c0ae255b665e6a6f",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 关键特性总结",
   "id": "33048ae24b6465d7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "### LLMs 核心优势\n",
    "1. **简单接口**: 字符串输入输出，易于使用\n",
    "2. **文本生成**: 专注于文本补全和生成任务\n",
    "3. **高性能**: 支持批量、异步、缓存优化\n",
    "4. **灵活性**: 可与各种提示模板组合\n",
    "5. **可扩展**: 支持自定义回调和处理器\n",
    "\n",
    "### 适用场景\n",
    "- **文本生成**: 文章、代码、创意内容生成\n",
    "- **文本补全**: 基于上下文的文本续写\n",
    "- **批量处理**: 大量文本的批量生成\n",
    "- **模板填充**: 基于模板的内容生成\n",
    "- **创意写作**: 故事、诗歌等创意内容\n",
    "\n",
    "### 性能优化建议\n",
    "1. **使用缓存**: 避免重复计算相同输入\n",
    "2. **批量处理**: 提高吞吐量\n",
    "3. **异步调用**: 提高并发性能\n",
    "4. **参数调优**: 根据需求调整温度等参数\n",
    "5. **错误处理**: 实现重试和降级机制\n",
    "\n",
    "这些示例展示了 LangChain 0.3 中 LLMs 的完整功能和最佳实践，可以根据具体需求选择合适的使用方式。\n",
    "\n",
    "\n"
   ],
   "id": "6be71db4dc797578"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Chat Models vs LLMs 详细对比",
   "id": "fafa6816111a1dfd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "## 核心差异对比\n"
   ],
   "id": "8555a0b28f075c36"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "\"\"\"\n",
    "Chat Models vs LLMs 详细对比示例\n",
    "\"\"\"\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_ollama import OllamaLLM\n",
    "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "import time\n",
    "\n",
    "def basic_interface_comparison():\n",
    "    \"\"\"基础接口对比\"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"1. 基础接口对比\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # 初始化模型\n",
    "    chat_model = ChatOllama(\n",
    "        base_url=\"http://localhost:11434\",\n",
    "        model=\"qwen2.5:3b\"\n",
    "    )\n",
    "\n",
    "    llm = OllamaLLM(\n",
    "        base_url=\"http://localhost:11434\",\n",
    "        model=\"qwen2.5:3b\"\n",
    "    )\n",
    "\n",
    "    # Chat Model - 消息格式\n",
    "    print(\"Chat Model 输入输出:\")\n",
    "    chat_messages = [\n",
    "        SystemMessage(content=\"你是一个友好的助手\"),\n",
    "        HumanMessage(content=\"你好，请介绍一下自己\")\n",
    "    ]\n",
    "    chat_response = chat_model.invoke(chat_messages)\n",
    "    print(f\"输入类型: {type(chat_messages)}\")\n",
    "    print(f\"输出类型: {type(chat_response)}\")\n",
    "    print(f\"输出内容: {chat_response.content[:100]}...\")\n",
    "    print(f\"输出元数据: {chat_response.response_metadata}\")\n",
    "\n",
    "    print(\"\\n\" + \"-\" * 40)\n",
    "\n",
    "    # LLM - 字符串格式\n",
    "    print(\"LLM 输入输出:\")\n",
    "    llm_prompt = \"你是一个友好的助手。用户说：你好，请介绍一下自己。请回答：\"\n",
    "    llm_response = llm.invoke(llm_prompt)\n",
    "    print(f\"输入类型: {type(llm_prompt)}\")\n",
    "    print(f\"输出类型: {type(llm_response)}\")\n",
    "    print(f\"输出内容: {llm_response[:100]}...\")\n",
    "\n",
    "def conversation_handling():\n",
    "    \"\"\"对话处理对比\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"2. 对话处理对比\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    chat_model = ChatOllama(base_url=\"http://localhost:11434\", model=\"qwen2.5:3b\")\n",
    "    llm = Ollama(base_url=\"http://localhost:11434\", model=\"qwen2.5:3b\")\n",
    "\n",
    "    # Chat Model - 自然的多轮对话\n",
    "    print(\"Chat Model 多轮对话:\")\n",
    "    conversation = [\n",
    "        SystemMessage(content=\"你是一个编程助手\"),\n",
    "        HumanMessage(content=\"我想学Python\"),\n",
    "        AIMessage(content=\"很好！Python是一门优秀的编程语言。你想从哪里开始？\"),\n",
    "        HumanMessage(content=\"从基础语法开始\")\n",
    "    ]\n",
    "\n",
    "    chat_response = chat_model.invoke(conversation)\n",
    "    print(f\"Chat Model 回复: {chat_response.content[:150]}...\")\n",
    "\n",
    "    print(\"\\n\" + \"-\" * 40)\n",
    "\n",
    "    # LLM - 需要手动构建对话上下文\n",
    "    print(\"LLM 多轮对话:\")\n",
    "    llm_context = \"\"\"你是一个编程助手。\n",
    "\n",
    "对话历史:\n",
    "用户: 我想学Python\n",
    "助手: 很好！Python是一门优秀的编程语言。你想从哪里开始？\n",
    "用户: 从基础语法开始\n",
    "\n",
    "请回答用户的最新问题:\"\"\"\n",
    "\n",
    "    llm_response = llm.invoke(llm_context)\n",
    "    print(f\"LLM 回复: {llm_response[:150]}...\")\n",
    "\n",
    "def prompt_template_comparison():\n",
    "    \"\"\"提示模板对比\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"3. 提示模板对比\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    chat_model = ChatOllama(base_url=\"http://localhost:11434\", model=\"qwen2.5:3b\")\n",
    "    llm = Ollama(base_url=\"http://localhost:11434\", model=\"qwen2.5:3b\")\n",
    "\n",
    "    # Chat Model - 结构化消息模板\n",
    "    print(\"Chat Model 提示模板:\")\n",
    "    chat_prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"你是一个{role}，专门帮助用户{task}\"),\n",
    "        (\"human\", \"我的问题是：{question}\"),\n",
    "        (\"ai\", \"我理解你的问题，让我来帮助你。\"),\n",
    "        (\"human\", \"请详细解答\")\n",
    "    ])\n",
    "\n",
    "    chat_chain = chat_prompt | chat_model | StrOutputParser()\n",
    "    chat_result = chat_chain.invoke({\n",
    "        \"role\": \"技术专家\",\n",
    "        \"task\": \"解决编程问题\",\n",
    "        \"question\": \"如何优化Python代码性能\"\n",
    "    })\n",
    "    print(f\"Chat 结果: {chat_result[:150]}...\")\n",
    "\n",
    "    print(\"\\n\" + \"-\" * 40)\n",
    "\n",
    "    # LLM - 文本模板\n",
    "    print(\"LLM 提示模板:\")\n",
    "    llm_prompt = PromptTemplate(\n",
    "        input_variables=[\"role\", \"task\", \"question\"],\n",
    "        template=\"\"\"你是一个{role}，专门帮助用户{task}。\n",
    "\n",
    "用户问题：{question}\n",
    "\n",
    "请详细解答：\"\"\"\n",
    "    )\n",
    "\n",
    "    llm_chain = llm_prompt | llm | StrOutputParser()\n",
    "    llm_result = llm_chain.invoke({\n",
    "        \"role\": \"技术专家\",\n",
    "        \"task\": \"解决编程问题\",\n",
    "        \"question\": \"如何优化Python代码性能\"\n",
    "    })\n",
    "    print(f\"LLM 结果: {llm_result[:150]}...\")\n",
    "\n",
    "def streaming_comparison():\n",
    "    \"\"\"流式处理对比\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"4. 流式处理对比\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    chat_model = ChatOllama(base_url=\"http://localhost:11434\", model=\"qwen2.5:3b\")\n",
    "    llm = Ollama(base_url=\"http://localhost:11434\", model=\"qwen2.5:3b\")\n",
    "\n",
    "    # Chat Model 流式处理\n",
    "    print(\"Chat Model 流式输出:\")\n",
    "    chat_messages = [HumanMessage(content=\"请写一首关于春天的短诗\")]\n",
    "\n",
    "    for chunk in chat_model.stream(chat_messages):\n",
    "        print(chunk.content, end=\"\", flush=True)\n",
    "    print(\"\\n\")\n",
    "\n",
    "    print(\"\\n\" + \"-\" * 40)\n",
    "\n",
    "    # LLM 流式处理\n",
    "    print(\"LLM 流式输出:\")\n",
    "    llm_prompt = \"请写一首关于春天的短诗：\"\n",
    "\n",
    "    for chunk in llm.stream(llm_prompt):\n",
    "        print(chunk, end=\"\", flush=True)\n",
    "    print(\"\\n\")\n",
    "\n",
    "def memory_management_comparison():\n",
    "    \"\"\"记忆管理对比\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"5. 记忆管理对比\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    chat_model = ChatOllama(base_url=\"http://localhost:11434\", model=\"qwen2.5:3b\")\n",
    "    llm = Ollama(base_url=\"http://localhost:11434\", model=\"qwen2.5:3b\")\n",
    "\n",
    "    # Chat Model - 天然支持对话历史\n",
    "    print(\"Chat Model 记忆管理:\")\n",
    "    chat_history = [\n",
    "        SystemMessage(content=\"你是一个记忆力很好的助手\"),\n",
    "        HumanMessage(content=\"我的名字是张三\"),\n",
    "        AIMessage(content=\"你好张三！很高兴认识你。\"),\n",
    "        HumanMessage(content=\"我是一名程序员\"),\n",
    "        AIMessage(content=\"太好了！作为程序员，你一定很有逻辑思维。\"),\n",
    "        HumanMessage(content=\"你还记得我的名字和职业吗？\")\n",
    "    ]\n",
    "\n",
    "    chat_response = chat_model.invoke(chat_history)\n",
    "    print(f\"Chat Model: {chat_response.content}\")\n",
    "\n",
    "    print(\"\\n\" + \"-\" * 40)\n",
    "\n",
    "    # LLM - 需要手动管理上下文\n",
    "    print(\"LLM 记忆管理:\")\n",
    "    llm_context = \"\"\"你是一个记忆力很好的助手。\n",
    "\n",
    "对话历史：\n",
    "用户：我的名字是张三\n",
    "助手：你好张三！很高兴认识你。\n",
    "用户：我是一名程序员\n",
    "助手：太好了！作为程序员，你一定很有逻辑思维。\n",
    "\n",
    "当前问题：你还记得我的名字和职业吗？\n",
    "请回答：\"\"\"\n",
    "\n",
    "    llm_response = llm.invoke(llm_context)\n",
    "    print(f\"LLM: {llm_response}\")\n",
    "\n",
    "def performance_comparison():\n",
    "    \"\"\"性能对比\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"6. 性能对比\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    chat_model = ChatOllama(base_url=\"http://localhost:11434\", model=\"qwen2.5:3b\")\n",
    "    llm = Ollama(base_url=\"http://localhost:11434\", model=\"qwen2.5:3b\")\n",
    "\n",
    "    # 测试相同内容的处理时间\n",
    "    test_content = \"请解释什么是机器学习\"\n",
    "\n",
    "    # Chat Model 性能测试\n",
    "    start_time = time.time()\n",
    "    chat_response = chat_model.invoke([HumanMessage(content=test_content)])\n",
    "    chat_time = time.time() - start_time\n",
    "\n",
    "    # LLM 性能测试\n",
    "    start_time = time.time()\n",
    "    llm_response = llm.invoke(test_content)\n",
    "    llm_time = time.time() - start_time\n",
    "\n",
    "    print(f\"Chat Model 耗时: {chat_time:.3f}秒\")\n",
    "    print(f\"LLM 耗时: {llm_time:.3f}秒\")\n",
    "    print(f\"性能差异: {abs(chat_time - llm_time):.3f}秒\")\n",
    "\n",
    "def use_case_examples():\n",
    "    \"\"\"使用场景示例\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"7. 使用场景示例\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    chat_model = ChatOllama(base_url=\"http://localhost:11434\", model=\"qwen2.5:3b\")\n",
    "    llm = Ollama(base_url=\"http://localhost:11434\", model=\"qwen2.5:3b\")\n",
    "\n",
    "    print(\"Chat Model 适用场景:\")\n",
    "    print(\"1. 客服聊天机器人\")\n",
    "    customer_service = [\n",
    "        SystemMessage(content=\"你是一个专业的客服代表\"),\n",
    "        HumanMessage(content=\"我的订单有问题\"),\n",
    "        AIMessage(content=\"很抱歉听到您遇到问题。请告诉我您的订单号。\"),\n",
    "        HumanMessage(content=\"订单号是12345\")\n",
    "    ]\n",
    "    cs_response = chat_model.invoke(customer_service)\n",
    "    print(f\"客服回复: {cs_response.content[:100]}...\")\n",
    "\n",
    "    print(\"\\n2. 教育对话系统\")\n",
    "    education = [\n",
    "        SystemMessage(content=\"你是一个耐心的数学老师\"),\n",
    "        HumanMessage(content=\"我不理解二次方程\"),\n",
    "        AIMessage(content=\"没关系，我们一步步来学习二次方程。\"),\n",
    "        HumanMessage(content=\"什么是判别式？\")\n",
    "    ]\n",
    "    edu_response = chat_model.invoke(education)\n",
    "    print(f\"老师回复: {edu_response.content[:100]}...\")\n",
    "\n",
    "    print(\"\\n\" + \"-\" * 40)\n",
    "    print(\"LLM 适用场景:\")\n",
    "\n",
    "    print(\"1. 文本生成\")\n",
    "    article_prompt = \"写一篇关于人工智能发展趋势的文章开头：\"\n",
    "    article_response = llm.invoke(article_prompt)\n",
    "    print(f\"文章开头: {article_response[:100]}...\")\n",
    "\n",
    "    print(\"\\n2. 代码生成\")\n",
    "    code_prompt = \"用Python写一个快速排序算法：\"\n",
    "    code_response = llm.invoke(code_prompt)\n",
    "    print(f\"代码生成: {code_response[:100]}...\")\n",
    "\n",
    "    print(\"\\n3. 文本补全\")\n",
    "    completion_prompt = \"人工智能的三大核心技术是机器学习、\"\n",
    "    completion_response = llm.invoke(completion_prompt)\n",
    "    print(f\"文本补全: {completion_response[:100]}...\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"运行所有对比示例\"\"\"\n",
    "    print(\"Chat Models vs LLMs 详细对比\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    basic_interface_comparison()\n",
    "    conversation_handling()\n",
    "    prompt_template_comparison()\n",
    "    streaming_comparison()\n",
    "    memory_management_comparison()\n",
    "    performance_comparison()\n",
    "    use_case_examples()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ],
   "id": "50dbf0acc69d7c5",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "## 详细对比表格\n",
    "\n",
    "| 特性维度 | Chat Models | LLMs | 说明 |\n",
    "|---------|-------------|------|------|\n",
    "| **输入格式** | 消息列表 (`List[BaseMessage]`) | 字符串 (`str`) | Chat Models 更结构化 |\n",
    "| **输出格式** | 消息对象 (`BaseMessage`) | 字符串 (`str`) | Chat Models 包含更多元数据 |\n",
    "| **角色支持** | ✅ 原生支持 (system/human/ai) | ❌ 需手动构建 | Chat Models 天然支持多角色 |\n",
    "| **对话历史** | ✅ 自动管理 | ❌ 需手动维护 | Chat Models 更适合对话场景 |\n",
    "| **提示模板** | `ChatPromptTemplate` | `PromptTemplate` | 不同的模板系统 |\n",
    "| **流式处理** | ✅ 支持 | ✅ 支持 | 两者都支持 |\n",
    "| **批量处理** | ✅ 支持 | ✅ 支持 | 两者都支持 |\n",
    "| **异步支持** | ✅ 支持 | ✅ 支持 | 两者都支持 |\n",
    "| **工具调用** | ✅ 原生支持 | ❌ 不支持 | Chat Models 支持 Function Calling |\n",
    "| **结构化输出** | ✅ `withStructuredOutput()` | ❌ 需额外处理 | Chat Models 更强大 |\n",
    "| **缓存支持** | ✅ 支持 | ✅ 支持 | 两者都支持 |\n",
    "| **性能开销** | 略高 (消息解析) | 较低 (直接字符串) | LLMs 略快 |\n",
    "| **使用复杂度** | 中等 | 简单 | LLMs 更简单直接 |"
   ],
   "id": "bea5eb991c820ae4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "## 使用场景指南"
   ],
   "id": "6157a81578d9df61"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "\"\"\"\n",
    "什么时候使用 Chat Models vs LLMs 指南\n",
    "\"\"\"\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, PromptTemplate\n",
    "\n",
    "def when_to_use_chat_models():\n",
    "    \"\"\"什么时候使用 Chat Models\"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"使用 Chat Models 的场景\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    chat_model = ChatOllama(base_url=\"http://localhost:11434\", model=\"qwen2.5:3b\")\n",
    "\n",
    "    print(\"1. 多轮对话应用\")\n",
    "    print(\"✅ 适合：聊天机器人、客服系统、教育对话\")\n",
    "\n",
    "    # 示例：客服对话\n",
    "    conversation = [\n",
    "        SystemMessage(content=\"你是专业的技术支持\"),\n",
    "        HumanMessage(content=\"我的软件崩溃了\"),\n",
    "        AIMessage(content=\"很抱歉听到这个问题。请告诉我软件版本和错误信息。\"),\n",
    "        HumanMessage(content=\"版本是2.1，显示内存不足\")\n",
    "    ]\n",
    "    response = chat_model.invoke(conversation)\n",
    "    print(f\"客服回复: {response.content[:100]}...\")\n",
    "\n",
    "    print(\"\\n2. 需要角色扮演的应用\")\n",
    "    print(\"✅ 适合：虚拟助手、角色扮演游戏、专业顾问\")\n",
    "\n",
    "    # 示例：专业顾问\n",
    "    consultant = [\n",
    "        SystemMessage(content=\"你是一位资深的投资顾问，有20年经验\"),\n",
    "        HumanMessage(content=\"我应该如何分配我的投资组合？\")\n",
    "    ]\n",
    "    response = chat_model.invoke(consultant)\n",
    "    print(f\"投资顾问: {response.content[:100]}...\")\n",
    "\n",
    "    print(\"\\n3. 需要上下文记忆的应用\")\n",
    "    print(\"✅ 适合：个人助理、学习伙伴、长期对话\")\n",
    "\n",
    "    print(\"\\n4. 需要工具调用的应用\")\n",
    "    print(\"✅ 适合：智能代理、API集成、复杂任务执行\")\n",
    "\n",
    "    print(\"\\n5. 需要结构化输出的应用\")\n",
    "    print(\"✅ 适合：数据提取、表单填写、API响应\")\n",
    "\n",
    "def when_to_use_llms():\n",
    "    \"\"\"什么时候使用 LLMs\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"使用 LLMs 的场景\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    llm = Ollama(base_url=\"http://localhost:11434\", model=\"qwen2.5:3b\")\n",
    "\n",
    "    print(\"1. 文本生成任务\")\n",
    "    print(\"✅ 适合：文章写作、创意内容、营销文案\")\n",
    "\n",
    "    # 示例：文章生成\n",
    "    article_prompt = \"写一篇关于可持续发展的文章，包含引言、主体和结论：\"\n",
    "    article = llm.invoke(article_prompt)\n",
    "    print(f\"文章生成: {article[:100]}...\")\n",
    "\n",
    "    print(\"\\n2. 文本补全任务\")\n",
    "    print(\"✅ 适合：代码补全、句子续写、模板填充\")\n",
    "\n",
    "    # 示例：代码补全\n",
    "    code_prompt = \"def fibonacci(n):\\n    if n <= 1:\\n        return n\\n    else:\"\n",
    "    code_completion = llm.invoke(code_prompt)\n",
    "    print(f\"代码补全: {code_completion[:100]}...\")\n",
    "\n",
    "    print(\"\\n3. 简单的问答任务\")\n",
    "    print(\"✅ 适合：FAQ、知识查询、简单咨询\")\n",
    "\n",
    "    # 示例：知识问答\n",
    "    qa_prompt = \"什么是量子计算？请简单解释。\"\n",
    "    qa_response = llm.invoke(qa_prompt)\n",
    "    print(f\"问答回复: {qa_response[:100]}...\")\n",
    "\n",
    "    print(\"\\n4. 批量文本处理\")\n",
    "    print(\"✅ 适合：文本分类、情感分析、批量翻译\")\n",
    "\n",
    "    print(\"\\n5. 性能敏感的应用\")\n",
    "    print(\"✅ 适合：实时处理、高并发、资源受限环境\")\n",
    "\n",
    "    print(\"\\n6. 简单的模板应用\")\n",
    "    print(\"✅ 适合：邮件模板、报告生成、格式化输出\")\n",
    "\n",
    "def decision_framework():\n",
    "    \"\"\"决策框架\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"选择决策框架\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    print(\" 选择 Chat Models 如果你需要:\")\n",
    "    print(\"   • 多轮对话能力\")\n",
    "    print(\"   • 角色和上下文管理\")\n",
    "    print(\"   • 工具调用功能\")\n",
    "    print(\"   • 结构化输出\")\n",
    "    print(\"   • 复杂的对话逻辑\")\n",
    "    print(\"   • 消息级别的控制\")\n",
    "\n",
    "    print(\"\\n 选择 LLMs 如果你需要:\")\n",
    "    print(\"   • 简单的文本生成\")\n",
    "    print(\"   • 最佳性能\")\n",
    "    print(\"   • 文本补全\")\n",
    "    print(\"   • 批量处理\")\n",
    "    print(\"   • 简单的接口\")\n",
    "    print(\"   • 传统的提示工程\")\n",
    "\n",
    "def practical_examples():\n",
    "    \"\"\"实际应用示例\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"实际应用示例\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    chat_model = ChatOllama(base_url=\"http://localhost:11434\", model=\"qwen2.5:3b\")\n",
    "    llm = Ollama(base_url=\"http://localhost:11434\", model=\"qwen2.5:3b\")\n",
    "\n",
    "    print(\" 应用类型对比:\")\n",
    "\n",
    "    applications = {\n",
    "        \"聊天机器人\": \"Chat Models\",\n",
    "        \"客服系统\": \"Chat Models\",\n",
    "        \"教育助手\": \"Chat Models\",\n",
    "        \"代码助手\": \"Chat Models\",\n",
    "        \"文章生成器\": \"LLMs\",\n",
    "        \"翻译工具\": \"LLMs\",\n",
    "        \"摘要工具\": \"LLMs\",\n",
    "        \"SEO文案\": \"LLMs\",\n",
    "        \"邮件模板\": \"LLMs\",\n",
    "        \"数据分析报告\": \"LLMs\",\n",
    "        \"智能代理\": \"Chat Models\",\n",
    "        \"游戏NPC\": \"Chat Models\",\n",
    "        \"内容审核\": \"LLMs\",\n",
    "        \"情感分析\": \"LLMs\"\n",
    "    }\n",
    "\n",
    "    for app, recommended in applications.items():\n",
    "        emoji = \"\" if recommended == \"Chat Models\" else \"\"\n",
    "        print(f\"   {emoji} {app:<15} → {recommended}\")\n",
    "\n",
    "def migration_guide():\n",
    "    \"\"\"迁移指南\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"迁移指南\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    print(\" 从 LLMs 迁移到 Chat Models:\")\n",
    "    print(\"   1. 将字符串提示转换为消息格式\")\n",
    "    print(\"   2. 使用 ChatPromptTemplate 替代 PromptTemplate\")\n",
    "    print(\"   3. 处理消息对象而不是字符串\")\n",
    "    print(\"   4. 利用角色系统优化提示\")\n",
    "\n",
    "    print(\"\\n 从 Chat Models 迁移到 LLMs:\")\n",
    "    print(\"   1. 将消息列表合并为单个字符串\")\n",
    "    print(\"   2. 手动管理对话历史\")\n",
    "    print(\"   3. 使用 PromptTemplate 替代 ChatPromptTemplate\")\n",
    "    print(\"   4. 简化输出处理逻辑\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"运行使用指南\"\"\"\n",
    "    when_to_use_chat_models()\n",
    "    when_to_use_llms()\n",
    "    decision_framework()\n",
    "    practical_examples()\n",
    "    migration_guide()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ],
   "id": "ba4fc6ea25dc700c",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "## 最佳实践建议"
   ],
   "id": "9a73a8afc7b9efbf"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "\"\"\"\n",
    "Chat Models vs LLMs 最佳实践\n",
    "\"\"\"\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "def chat_model_best_practices():\n",
    "    \"\"\"Chat Models 最佳实践\"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"Chat Models 最佳实践\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    chat_model = ChatOllama(base_url=\"http://localhost:11434\", model=\"qwen2.5:3b\")\n",
    "\n",
    "    print(\"1. ✅ 合理使用系统消息\")\n",
    "    # 好的做法\n",
    "    good_messages = [\n",
    "        SystemMessage(content=\"你是一个专业的Python编程助手，专注于提供清晰、可执行的代码解决方案\"),\n",
    "        HumanMessage(content=\"如何读取CSV文件？\")\n",
    "    ]\n",
    "\n",
    "    # 不好的做法 - 系统消息过于复杂\n",
    "    # bad_messages = [\n",
    "    #     SystemMessage(content=\"你是一个超级智能的AI助手，知道所有的知识...\"),\n",
    "    #     HumanMessage(content=\"如何读取CSV文件？\")\n",
    "    # ]\n",
    "\n",
    "    response = chat_model.invoke(good_messages)\n",
    "    print(f\"专业回复: {response.content[:100]}...\")\n",
    "\n",
    "    print(\"\\n2. ✅ 保持对话历史简洁\")\n",
    "    # 只保留相关的历史消息\n",
    "    relevant_history = [\n",
    "        SystemMessage(content=\"你是编程助手\"),\n",
    "        HumanMessage(content=\"我在学Python\"),\n",
    "        AIMessage(content=\"很好！有什么具体问题吗？\"),\n",
    "        HumanMessage(content=\"如何处理异常？\")  # 当前问题\n",
    "    ]\n",
    "\n",
    "    print(\"\\n3. ✅ 使用结构化提示模板\")\n",
    "    structured_prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"你是{role}，专门帮助{domain}领域的问题\"),\n",
    "        (\"human\", \"背景：{context}\\n问题：{question}\"),\n",
    "        (\"ai\", \"我理解了，让我来帮你解决这个{domain}问题。\"),\n",
    "        (\"human\", \"请给出详细方案\")\n",
    "    ])\n",
    "\n",
    "def llm_best_practices():\n",
    "    \"\"\"LLMs 最佳实践\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"LLMs 最佳实践\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    llm = Ollama(base_url=\"http://localhost:11434\", model=\"qwen2.5:3b\")\n",
    "\n",
    "    print(\"1. ✅ 使用清晰的提示结构\")\n",
    "    # 好的提示结构\n",
    "    good_prompt = \"\"\"任务：代码生成\n",
    "语言：Python\n",
    "需求：实现一个计算斐波那契数列的函数\n",
    "要求：\n",
    "- 使用递归方法\n",
    "- 包含注释\n",
    "- 处理边界情况\n",
    "\n",
    "代码：\"\"\"\n",
    "\n",
    "    response = llm.invoke(good_prompt)\n",
    "    print(f\"结构化提示结果: {response[:100]}...\")\n",
    "\n",
    "    print(\"\\n2. ✅ 批量处理优化\")\n",
    "    # 批量处理相似任务\n",
    "    batch_prompts = [\n",
    "        \"总结：人工智能的定义\",\n",
    "        \"总结：机器学习的定义\",\n",
    "        \"总结：深度学习的定义\"\n",
    "    ]\n",
    "\n",
    "    batch_responses = llm.batch(batch_prompts)\n",
    "    print(f\"批量处理完成，处理了{len(batch_responses)}个任务\")\n",
    "\n",
    "    print(\"\\n3. ✅ 使用模板提高复用性\")\n",
    "    template = PromptTemplate(\n",
    "        input_variables=[\"topic\", \"style\", \"length\"],\n",
    "        template=\"写一篇关于{topic}的{style}风格文章，长度约{length}字：\"\n",
    "    )\n",
    "\n",
    "    chain = template | llm\n",
    "    result = chain.invoke({\n",
    "        \"topic\": \"云计算\",\n",
    "        \"style\": \"技术\",\n",
    "        \"length\": \"200\"\n",
    "    })\n",
    "    print(f\"模板化结果: {result[:100]}...\")\n",
    "\n",
    "def performance_optimization():\n",
    "    \"\"\"性能优化建议\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"性能优化建议\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    print(\" Chat Models 优化:\")\n",
    "    print(\"   • 限制对话历史长度\")\n",
    "    print(\"   • 使用消息压缩技术\")\n",
    "    print(\"   • 合理设置系统消息\")\n",
    "    print(\"   • 避免过度复杂的角色设定\")\n",
    "\n",
    "    print(\"\\n LLMs 优化:\")\n",
    "    print(\"   • 使用批量处理\")\n",
    "    print(\"   • 启用缓存机制\")\n",
    "    print(\"   • 优化提示长度\")\n",
    "    print(\"   • 使用异步调用\")\n",
    "\n",
    "    print(\"\\n 通用优化:\")\n",
    "    print(\"   • 合理设置温度参数\")\n",
    "    print(\"   • 使用流式处理改善用户体验\")\n",
    "    print(\"   • 实现错误重试机制\")\n",
    "    print(\"   • 监控和日志记录\")\n",
    "\n",
    "def common_pitfalls():\n",
    "    \"\"\"常见陷阱\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"常见陷阱和解决方案\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    print(\"❌ Chat Models 常见错误:\")\n",
    "    print(\"   • 过长的对话历史导致性能下降\")\n",
    "    print(\"   • 混乱的角色设定\")\n",
    "    print(\"   • 不必要的消息类型转换\")\n",
    "    print(\"   • 忽略消息元数据\")\n",
    "\n",
    "    print(\"\\n❌ LLMs 常见错误:\")\n",
    "    print(\"   • 手动拼接复杂上下文\")\n",
    "    print(\"   • 忽略批量处理优势\")\n",
    "    print(\"   • 过度复杂的提示工程\")\n",
    "    print(\"   • 不使用模板导致重复代码\")\n",
    "\n",
    "    print(\"\\n✅ 解决方案:\")\n",
    "    print(\"   • 定期清理对话历史\")\n",
    "    print(\"   • 使用专门的提示模板\")\n",
    "    print(\"   • 实现智能上下文管理\")\n",
    "    print(\"   • 选择合适的模型类型\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"运行最佳实践指南\"\"\"\n",
    "    chat_model_best_practices()\n",
    "    llm_best_practices()\n",
    "    performance_optimization()\n",
    "    common_pitfalls()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ],
   "id": "4bde10b72416e78f",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "## 总结建议"
   ],
   "id": "144cb3a4c6268967"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "###  选择 Chat Models 当你需要:\n",
    "1. **多轮对话**: 聊天机器人、客服系统\n",
    "2. **角色扮演**: 虚拟助手、专业顾问\n",
    "3. **上下文记忆**: 个人助理、学习伙伴\n",
    "4. **工具集成**: 智能代理、API调用\n",
    "5. **结构化交互**: 复杂的对话逻辑\n",
    "\n",
    "###  选择 LLMs 当你需要:\n",
    "1. **文本生成**: 文章、创意内容、营销文案\n",
    "2. **文本补全**: 代码补全、句子续写\n",
    "3. **批量处理**: 大量文本的统一处理\n",
    "4. **简单问答**: FAQ、知识查询\n",
    "5. **性能优先**: 高并发、低延迟场景\n",
    "\n",
    "###  迁移建议:\n",
    "- **从简单开始**: 先用 LLMs 验证概念，再升级到 Chat Models\n",
    "- **渐进式迁移**: 逐步添加对话功能和角色管理\n",
    "- **性能测试**: 在实际场景中对比两种方案的性能\n",
    "- **用户体验**: 根据用户反馈选择最合适的交互方式\n",
    "\n",
    "选择合适的模型类型是构建成功 AI 应用的关键第一步！\n"
   ],
   "id": "2cc219efd0c014b"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
