{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# LLMs - å¤§è¯­è¨€æ¨¡åž‹æŽ¥å£è¯¦è§£\n",
    "\n",
    "## æ¦‚è¿°\n",
    "\n",
    "LLMsï¼ˆLarge Language Modelsï¼‰æ˜¯ LangChain ä¸­ç”¨äºŽä¸Žä¼ ç»Ÿæ–‡æœ¬ç”Ÿæˆæ¨¡åž‹äº¤äº’çš„æŽ¥å£ã€‚ä¸Ž Chat Models ä¸åŒï¼ŒLLMs æŽ¥å—å­—ç¬¦ä¸²ä½œä¸ºè¾“å…¥å¹¶è¿”å›žå­—ç¬¦ä¸²ï¼Œæ›´é€‚åˆæ–‡æœ¬è¡¥å…¨ä»»åŠ¡ã€‚\n",
    "\n",
    "### LLMs vs Chat Models å¯¹æ¯”\n",
    "\n",
    "| ç‰¹æ€§ | LLMs | Chat Models |\n",
    "|------|------|-------------|\n",
    "| è¾“å…¥æ ¼å¼ | å­—ç¬¦ä¸² | æ¶ˆæ¯åˆ—è¡¨ |\n",
    "| è¾“å‡ºæ ¼å¼ | å­—ç¬¦ä¸² | æ¶ˆæ¯å¯¹è±¡ |\n",
    "| é€‚ç”¨åœºæ™¯ | æ–‡æœ¬è¡¥å…¨ã€ç”Ÿæˆ | å¯¹è¯ã€èŠå¤© |\n",
    "| æŽ¥å£æ–¹æ³• | `invoke()`, `generate()` | `invoke()`, `stream()` |\n",
    "\n",
    "### æ ¸å¿ƒç‰¹æ€§\n",
    "\n",
    "- **æ–‡æœ¬è¡¥å…¨**: åŸºäºŽæç¤ºè¯ç”Ÿæˆæ–‡æœ¬\n",
    "- **æ‰¹é‡ç”Ÿæˆ**: æ”¯æŒæ‰¹é‡æ–‡æœ¬ç”Ÿæˆ\n",
    "- **æµå¼è¾“å‡º**: æ”¯æŒå®žæ—¶æµå¼ç”Ÿæˆ\n",
    "- **å‚æ•°æŽ§åˆ¶**: æ¸©åº¦ã€æœ€å¤§é•¿åº¦ç­‰å‚æ•°\n",
    "- **ç¼“å­˜æ”¯æŒ**: å†…ç½®ç¼“å­˜æœºåˆ¶\n",
    "- **å›žè°ƒç³»ç»Ÿ**: æ”¯æŒç”Ÿæˆè¿‡ç¨‹ç›‘æŽ§"
   ],
   "id": "a7f4d5171c04550"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## å®Œæ•´ä»£ç ç¤ºä¾‹",
   "id": "5511703dee5eb371"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### å¼•å…¥ä¾èµ–",
   "id": "d63679f0557679d0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-22T07:03:34.955609Z",
     "start_time": "2025-07-22T07:03:32.970336Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "LangChain 0.3 LLMs å®Œæ•´ç¤ºä¾‹\n",
    "\"\"\"\n",
    "# å¼•å…¥ä¾èµ–\n",
    "# from langchain_community.llms import Ollama\n",
    "from langchain_ollama import OllamaLLM\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableParallel\n",
    "from langchain_core.callbacks import BaseCallbackHandler\n",
    "from langchain.cache import InMemoryCache\n",
    "from langchain.globals import set_llm_cache\n",
    "import asyncio\n",
    "import time\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "# è®¾ç½®ç¼“å­˜\n",
    "set_llm_cache(InMemoryCache())"
   ],
   "id": "bd3e8282de31bc7a",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 1. åŸºç¡€ LLM ä½¿ç”¨",
   "id": "79da8d95326e1b07"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "#### `invoke()` vs `generate()`\n",
    "åœ¨ `LangChain` ä¸­ä½¿ç”¨ `OllamaLLM`ï¼ˆæˆ–æ›´å¹¿ä¹‰åœ°ï¼Œæ‰€æœ‰ LLM æŽ¥å£å¦‚ `ChatOpenAI`ã€`ChatOllama`ï¼‰æ—¶ï¼Œå¸¸ä¼šçœ‹åˆ°ä¸¤ä¸ªæ–¹æ³•ï¼š\n",
    "\n",
    "---\n",
    "\n",
    "âœ… 1. `invoke()`\n",
    "\n",
    "**ç”¨é€”ï¼šæ‰§è¡Œä¸€ä¸ªå®Œæ•´çš„è°ƒç”¨é“¾ï¼Œé€šå¸¸é…åˆ LCEL ä½¿ç”¨ã€‚**\n",
    "\n",
    "* `invoke()` æ˜¯ LangChain `Runnable` æŽ¥å£çš„æ–¹æ³•ï¼Œç”¨äºŽ**å•æ¬¡è¾“å…¥ã€å•æ¬¡è¾“å‡º**ã€‚\n",
    "* è¾“å…¥æ˜¯ä¸€ä¸ª**å®Œæ•´ç»“æž„**ï¼ˆå¦‚åŒ…å« prompt å­—æ®µæˆ– messagesï¼‰ï¼ŒLangChain ä¼šè´Ÿè´£ç»„ç»‡æˆæ¨¡åž‹èƒ½ç†è§£çš„æ ¼å¼ã€‚\n",
    "\n",
    "ðŸ”§ ç¤ºä¾‹ï¼š\n",
    "\n",
    "```python\n",
    "from langchain.chat_models import ChatOllama\n",
    "\n",
    "llm = ChatOllama(model=\"llama3\")\n",
    "\n",
    "response = llm.invoke(\"ä½ å¥½ï¼Œä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±\")\n",
    "print(response.content)\n",
    "```\n",
    "\n",
    "ðŸ“Œ ç­‰ä»·äºŽè¯´ï¼šâ€œæˆ‘ç»™ä½ ä¸€å¥è¯ï¼Œä½ å›žå¤ä¸€å¥è¯ã€‚â€ â€”â€” é€šå¸¸**è¿”å›žçš„æ˜¯ Message å¯¹è±¡**ã€‚\n",
    "\n",
    "---\n",
    "\n",
    " âœ… 2. `generate()`\n",
    "\n",
    "**ç”¨é€”ï¼šæ‰¹é‡ç”Ÿæˆï¼Œé€‚ç”¨äºŽå¤šä¸ªè¾“å…¥ç”Ÿæˆå¤šä¸ªè¾“å‡ºã€‚**\n",
    "\n",
    "* `generate()` æ›´åº•å±‚ï¼ŒæŽ¥å—å¤šä¸ªè¾“å…¥ï¼ˆåˆ—è¡¨ï¼‰ï¼Œè¿”å›žç»“æž„åŒ–çš„ç»“æžœï¼ˆå« token ä½¿ç”¨é‡ã€messages ç­‰ï¼‰ã€‚\n",
    "* è¿”å›žçš„æ˜¯ `LLMResult` å¯¹è±¡ï¼Œé€‚åˆåš**æ‰¹é‡æŽ¨ç†ã€ç»Ÿè®¡åˆ†æžã€ç¼“å­˜ä¼˜åŒ–**ç­‰ã€‚\n",
    "\n",
    "ðŸ”§ ç¤ºä¾‹ï¼š\n",
    "\n",
    "```python\n",
    "from langchain.chat_models import ChatOllama\n",
    "\n",
    "llm = ChatOllama(model=\"llama3\")\n",
    "\n",
    "results = llm.generate([\n",
    "    [\"ä½ å¥½ï¼Œä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±\"],\n",
    "    [\"è®²ä¸€ä¸ªç¬‘è¯\"]\n",
    "])\n",
    "\n",
    "for generation in results.generations:\n",
    "    print(generation[0].text)\n",
    "```\n",
    "\n",
    "ðŸ“Œ ä¸€æ¬¡ç”Ÿæˆå¤šä¸ªå›žå¤ï¼Œå¹¶è¿”å›žè¯¦ç»†ä¿¡æ¯ï¼ˆå¦‚ `generation_info`ã€tokenæ•°ç­‰ï¼‰ã€‚\n",
    "\n",
    "---\n",
    "\n",
    "âœ¨ æ€»ç»“å¯¹æ¯”\n",
    "\n",
    "| å¯¹æ¯”é¡¹  | `invoke()`              | `generate()`                  |\n",
    "| ---- | ----------------------- | ----------------------------- |\n",
    "| è¾“å…¥æ ¼å¼ | å•æ¡æ–‡æœ¬æˆ–ç»“æž„åŒ–è¾“å…¥ï¼ˆå­—å…¸ã€messageï¼‰  | åˆ—è¡¨ï¼ˆå¤šä¸ªè¾“å…¥ï¼‰                      |\n",
    "| è¿”å›žç±»åž‹ | é€šå¸¸æ˜¯ `AIMessage` / `str` | `LLMResult`ï¼ˆåŒ…å«å¤šä¸ª generationsï¼‰ |\n",
    "| ä½¿ç”¨åœºæ™¯ | å•æ¬¡è°ƒç”¨ï¼ˆé…åˆ LCELã€Runnableï¼‰  | æ‰¹é‡ç”Ÿæˆï¼ˆå¦‚è¯„ä¼°ä»»åŠ¡ã€ç¼“å­˜ï¼‰                |\n",
    "| æ˜“ç”¨æ€§  | âœ… æ›´é€‚åˆç®€å•ä½¿ç”¨               | ðŸ› ï¸ æ›´é€‚åˆåº•å±‚æŽ§åˆ¶ã€å¤šæ ·åŒ–éœ€æ±‚             |\n",
    "\n",
    "---\n",
    "\n"
   ],
   "id": "8d1ce0ed72f4ab7c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### basic_llm_usage",
   "id": "ed0ed64ed3114869"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-22T06:50:29.552739Z",
     "start_time": "2025-07-22T06:50:29.482364Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# 1. åŸºç¡€ LLM ä½¿ç”¨\n",
    "def basic_llm_usage():\n",
    "    \"\"\"åŸºç¡€ LLM ä½¿ç”¨ç¤ºä¾‹\"\"\"\n",
    "    print(\"=\" * 50)\n",
    "    print(\"1. åŸºç¡€ LLM ä½¿ç”¨\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    # åˆå§‹åŒ– Ollama LLM\n",
    "    llm = OllamaLLM(\n",
    "        base_url=\"http://localhost:11434\",\n",
    "        model=\"qwen2.5:3b\",\n",
    "        temperature=0.7\n",
    "    )\n",
    "\n",
    "    # åŸºç¡€æ–‡æœ¬ç”Ÿæˆ\n",
    "    prompt = \"è¯·å†™ä¸€ä¸ªå…³äºŽäººå·¥æ™ºèƒ½çš„ç®€çŸ­ä»‹ç»ï¼š\"\n",
    "    response = llm.invoke(prompt)\n",
    "    print(f\"ç”Ÿæˆæ–‡æœ¬: {response}\")\n",
    "\n",
    "    # ä½¿ç”¨ generate æ–¹æ³•\n",
    "    prompts = [\n",
    "        \"Pythonæ˜¯ä»€ä¹ˆï¼Ÿ\",\n",
    "        \"æœºå™¨å­¦ä¹ çš„å®šä¹‰æ˜¯ä»€ä¹ˆï¼Ÿ\",\n",
    "        \"æ·±åº¦å­¦ä¹ æœ‰å“ªäº›åº”ç”¨ï¼Ÿ\"\n",
    "    ]\n",
    "\n",
    "    generations = llm.generate(prompts)\n",
    "    print(f\"\\næ‰¹é‡ç”Ÿæˆç»“æžœ:\")\n",
    "    for i, gen in enumerate(generations.generations):\n",
    "        print(f\"é—®é¢˜{i+1}: {gen[0].text[:100]}...\")\n",
    "basic_llm_usage()"
   ],
   "id": "4c0018842c29b904",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "1. åŸºç¡€ LLM ä½¿ç”¨\n",
      "==================================================\n",
      "ç”Ÿæˆæ–‡æœ¬: äººå·¥æ™ºèƒ½ï¼ˆArtificial Intelligenceï¼Œç®€ç§°AIï¼‰æ˜¯æŒ‡é€šè¿‡æ¨¡æ‹Ÿã€æ‰©å±•å’Œå¢žå¼ºäººç±»æ™ºèƒ½çš„æŠ€æœ¯ã€‚å®ƒåˆ©ç”¨è®¡ç®—æŠ€æœ¯å®žçŽ°æ™ºèƒ½æœºå™¨ç³»ç»Ÿï¼Œä½¿æœºå™¨èƒ½å¤Ÿåœ¨æ— éœ€äººå·¥å¹²é¢„çš„æƒ…å†µä¸‹æ‰§è¡Œå¤æ‚ä»»åŠ¡ï¼Œå¹¶å…·å¤‡å­¦ä¹ èƒ½åŠ›ã€‚\n",
      "\n",
      "åœ¨äººå·¥æ™ºèƒ½é¢†åŸŸä¸­ï¼Œæœºå™¨å­¦ä¹ æ˜¯ä¸€ç§æ ¸å¿ƒæ–¹æ³•ï¼Œå…è®¸è®¡ç®—æœºä»Žæ•°æ®ä¸­è‡ªåŠ¨å­¦ä¹ å¹¶æå–è§„å¾‹æˆ–ç‰¹å¾ï¼Œè€Œä¸éœ€è¦æ˜¾å¼ç¼–ç¨‹ã€‚æ·±åº¦å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œå®ƒä½¿ç”¨å¤šå±‚ç¥žç»ç½‘ç»œæ¥æ¨¡ä»¿äººè„‘å¦‚ä½•å¤„ç†ä¿¡æ¯ï¼Œä»Žè€Œå®žçŽ°å¯¹å¤æ‚æ•°æ®çš„é«˜æ•ˆç†è§£å’Œé¢„æµ‹ã€‚\n",
      "\n",
      "é™¤æ­¤ä¹‹å¤–ï¼Œè‡ªç„¶è¯­è¨€å¤„ç†ä½¿å¾—è®¡ç®—æœºèƒ½å¤Ÿç†è§£ã€è§£é‡Šå’Œç”Ÿæˆäººç±»è¯­è¨€ï¼›è®¡ç®—æœºè§†è§‰æŠ€æœ¯è®©æœºå™¨èƒ½å¤Ÿâ€œçœ‹â€å¹¶ä»Žä¸­æå–æœ‰ç”¨çš„ä¿¡æ¯ï¼›å¼ºåŒ–å­¦ä¹ åˆ™ä½¿æœºå™¨èƒ½å¤Ÿåœ¨çŽ¯å¢ƒä¸­é€šè¿‡ä¸ŽçŽ¯å¢ƒäº¤äº’ä»¥èŽ·å¾—å¥–åŠ±æˆ–æƒ©ç½šçš„æ–¹å¼è¿›è¡Œå­¦ä¹ ã€‚\n",
      "\n",
      "éšç€ç®—æ³•å’ŒæŠ€æœ¯çš„è¿›æ­¥ï¼Œäººå·¥æ™ºèƒ½å·²ç»åœ¨åŒ»ç–—è¯Šæ–­ã€è‡ªåŠ¨é©¾é©¶æ±½è½¦ã€è¯­éŸ³è¯†åˆ«ä»¥åŠå¤æ‚çš„å†³ç­–æ”¯æŒç³»ç»Ÿç­‰å¤šä¸ªé¢†åŸŸå–å¾—äº†æ˜¾è‘—çš„åº”ç”¨æˆæžœã€‚æ­¤å¤–ï¼Œäººå·¥æ™ºèƒ½è¿˜åœ¨ä¸æ–­åœ°æŒ‘æˆ˜å’Œæ‹“å±•äººç±»çš„è®¤çŸ¥è¾¹ç•Œï¼Œå¹¶ä¸”ä¸ºç¤¾ä¼šå¸¦æ¥äº†å‰æ‰€æœªæœ‰çš„å˜é©ä¸Žå‘å±•æœºé‡ã€‚\n",
      "\n",
      "æ‰¹é‡ç”Ÿæˆç»“æžœ:\n",
      "é—®é¢˜1: Pythonæ˜¯ä¸€ç§é«˜çº§ç¼–ç¨‹è¯­è¨€ï¼Œç”±Guido van RossumäºŽ1980å¹´ä»£æœ«å¼€å§‹è®¾è®¡å’Œå¼€å‘ã€‚å®ƒæœ€åˆè¢«ç§°ä¸ºâ€œActiveâ€ä½†å¾ˆå¿«æ”¹åä¸ºâ€œPythonâ€ï¼Œå–è‡ªè‹±å›½å–œå‰§å°å“æ¼”å‘˜Eric Idleçš„åå­—...\n",
      "é—®é¢˜2: æœºå™¨å­¦ä¹ æ˜¯ä¸€ç§äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰çš„æŠ€æœ¯ï¼Œä½¿è®¡ç®—æœºèƒ½å¤Ÿåœ¨ä¸è¿›è¡Œæ˜Žç¡®ç¼–ç¨‹çš„æƒ…å†µä¸‹é€šè¿‡æ•°æ®å­¦ä¹ å’Œæ”¹è¿›ã€‚ç®€å•æ¥è¯´ï¼Œå°±æ˜¯è®©è®¡ç®—æœºä»ŽåŽ†å²æ•°æ®ä¸­è‡ªåŠ¨è¯†åˆ«æ¨¡å¼ã€æå–ç‰¹å¾ï¼Œå¹¶åˆ©ç”¨è¿™äº›ä¿¡æ¯æ¥åšå‡ºé¢„æµ‹æˆ–å†³ç­–ã€‚\n",
      "\n",
      "åœ¨å®žé™…åº”ç”¨ä¸­...\n",
      "é—®é¢˜3: æ·±åº¦å­¦ä¹ æ˜¯ä¸€ç§æœºå™¨å­¦ä¹ çš„æ–¹æ³•ï¼Œå®ƒå·²ç»åœ¨å¤šä¸ªé¢†åŸŸéƒ½æœ‰äº†å¹¿æ³›çš„åº”ç”¨ã€‚ä»¥ä¸‹æ˜¯ä¸€äº›ä¸»è¦çš„åº”ç”¨åœºæ™¯ï¼š\n",
      "\n",
      "1. è¯­éŸ³è¯†åˆ«ï¼šæ·±åº¦å­¦ä¹ åœ¨è¿™ä¸€é¢†åŸŸçš„åº”ç”¨å·²ç»éžå¸¸æˆç†Ÿå’ŒæˆåŠŸï¼Œæ— è®ºæ˜¯åŸºäºŽæ‰‹æœºçš„æ•°å­—åŠ©ç†ã€æ™ºèƒ½éŸ³ç®±è¿˜æ˜¯ä¸“ä¸šçš„è¯­...\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 2. æµå¼ç”Ÿæˆç¤ºä¾‹",
   "id": "8ca64718ca2eae16"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-22T06:55:05.207306Z",
     "start_time": "2025-07-22T06:54:30.417724Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# 2. æµå¼ç”Ÿæˆç¤ºä¾‹\n",
    "def streaming_generation():\n",
    "    \"\"\"æµå¼ç”Ÿæˆç¤ºä¾‹\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"2. æµå¼ç”Ÿæˆç¤ºä¾‹\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    llm = OllamaLLM(\n",
    "        base_url=\"http://localhost:11434\",\n",
    "        model=\"qwen2.5:3b\"\n",
    "    )\n",
    "\n",
    "    prompt = \"è¯·è¯¦ç»†è§£é‡Šä»€ä¹ˆæ˜¯åŒºå—é“¾æŠ€æœ¯ï¼š\"\n",
    "\n",
    "    print(\"æµå¼è¾“å‡º:\")\n",
    "    for chunk in llm.stream(prompt):\n",
    "        print(chunk, end=\"\", flush=True)\n",
    "    print(\"\\n\")\n",
    "streaming_generation()"
   ],
   "id": "3f1a56bb01a27fdf",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "2. æµå¼ç”Ÿæˆç¤ºä¾‹\n",
      "==================================================\n",
      "æµå¼è¾“å‡º:\n",
      "åŒºå—é“¾æ˜¯ä¸€ç§åŽ»ä¸­å¿ƒåŒ–çš„æ•°æ®è®°å½•æ–¹å¼ï¼Œå®ƒé€šè¿‡å¯†ç å­¦æ–¹æ³•ç¡®ä¿ä¿¡æ¯çš„å®‰å…¨æ€§å’Œå®Œæ•´æ€§ï¼Œå¹¶ä¸”æ‰€æœ‰çš„äº¤æ˜“ä¿¡æ¯éƒ½ä¼šè¢«åˆ†å¸ƒå¼åœ°ä¿å­˜åœ¨ä¸€ä¸ªå…¬å¼€çš„æ•°æ®åº“ä¸­ã€‚ä»¥ä¸‹æ˜¯å¯¹åŒºå—é“¾æŠ€æœ¯çš„è¯¦ç»†è§£é‡Šï¼š\n",
      "\n",
      "1. åŽ»ä¸­å¿ƒåŒ–æž¶æž„ï¼š\n",
      "ä¼ ç»Ÿçš„æ•°æ®åº“ç³»ç»Ÿå¤§å¤šç”±å•ä¸ªç»„ç»‡æˆ–æœºæž„æŽ§åˆ¶ï¼Œå› æ­¤å­˜åœ¨é£Žé™©ï¼šå¦‚å•ä¸€èŠ‚ç‚¹å¤±è´¥ã€äººä¸ºé”™è¯¯å’Œæ•°æ®ç¯¡æ”¹ç­‰ã€‚è€ŒåŒºå—é“¾åˆ™é‡‡ç”¨åŽ»ä¸­å¿ƒåŒ–çš„å½¢å¼ï¼Œæ¯ä¸ªå‚ä¸Žè€…éƒ½æ˜¯ç½‘ç»œçš„ä¸€éƒ¨åˆ†ï¼Œæ— éœ€ä¾èµ–ä»»ä½•ä¸­å¤®æƒå¨æœºæž„ã€‚\n",
      "\n",
      "2. åˆ†å¸ƒå¼è®°è´¦æŠ€æœ¯ï¼š\n",
      "åœ¨åŒºå—é“¾ä¸­ï¼Œæ‰€æœ‰çš„äº¤æ˜“è®°å½•éƒ½è¢«åˆ†å¸ƒåœ¨å¤šä¸ªèŠ‚ç‚¹ä¸Šè¿›è¡Œä¿å­˜ï¼Œå¹¶é€šè¿‡å¯†ç å­¦æ‰‹æ®µä¿è¯è®°å½•çš„å®‰å…¨æ€§å’Œå®Œæ•´æ€§ã€‚è¿™æ„å‘³ç€å³ä½¿æœ‰ä¸€ä¸ªæˆ–å‡ ä¸ªèŠ‚ç‚¹å‡ºçŽ°æ•…éšœæˆ–è€…è¢«ç ´åï¼Œä¹Ÿä¸ä¼šå¯¹æ•´ä¸ªç³»ç»Ÿçš„æ­£å¸¸è¿è¡Œé€ æˆå½±å“ã€‚\n",
      "\n",
      "3. åŠ å¯†ç®—æ³•å’Œå…±è¯†æœºåˆ¶ï¼š\n",
      "æ¯ç¬”äº¤æ˜“éƒ½ä¼šè¢«æ‰“åŒ…æˆåŒºå—å¹¶åœ¨æ•´ä¸ªç½‘ç»œä¸­è¿›è¡Œå¹¿æ’­ä»¥ä¾›å…¶ä»–èŠ‚ç‚¹éªŒè¯ã€‚ä¸€æ—¦å¤šä¸ªèŠ‚ç‚¹éƒ½ç¡®è®¤äº†äº¤æ˜“çš„æœ‰æ•ˆæ€§ï¼Œå°±ä¼šåˆ›å»ºä¸€ä¸ªæ–°çš„åŒºå—å¹¶å°†å…¶æ·»åŠ åˆ°åŒºå—é“¾çš„æœ«ç«¯ã€‚è¿™ç§æŠ€æœ¯ç¡®ä¿äº†æ•°æ®çš„å®‰å…¨æ€§å’Œé˜²ç¯¡æ”¹ç‰¹æ€§ã€‚\n",
      "\n",
      "4. åŒ¿åæ€§å’Œé€æ˜Žåº¦ï¼š\n",
      "è™½ç„¶æ¯ä¸ªç”¨æˆ·éƒ½æœ‰ä¸€ä¸ªç‹¬ç‰¹çš„å…¬é’¥èº«ä»½ç”¨äºŽä¸Žå…¶ä»–ç”¨æˆ·çš„äº¤äº’å’Œç®¡ç†èµ„é‡‘ï¼Œä½†ç”¨æˆ·çš„çœŸå®žå§“åå’Œä¸ªäººä¿¡æ¯é€šå¸¸ä¸ä¼šç›´æŽ¥å…¬å¼€åœ¨å…¬å…±é“¾ä¸Šã€‚åŒæ—¶ï¼Œç”±äºŽæ¯ç¬”äº¤æ˜“éƒ½ä¼šè¢«è®°å½•ä¸‹æ¥å¹¶å¯ä¾›æ‰€æœ‰å‚ä¸Žè€…æŸ¥çœ‹ï¼Œå› æ­¤å¯ä»¥ä¿è¯æ•´ä¸ªç³»ç»Ÿçš„é€æ˜Žæ€§ã€‚\n",
      "\n",
      "5. æ™ºèƒ½åˆçº¦ï¼š\n",
      "æ™ºèƒ½åˆçº¦æ˜¯æŒ‡è‡ªåŠ¨æ‰§è¡Œçš„ä»£ç ï¼Œåœ¨æ»¡è¶³ç‰¹å®šæ¡ä»¶æ—¶ä¼šè§¦å‘é¢„è®¾çš„æ“ä½œæˆ–è¡ŒåŠ¨ã€‚å®ƒä»¬é€šå¸¸éƒ¨ç½²åœ¨åŒºå—é“¾ç½‘ç»œä¹‹ä¸Šï¼Œå¹¶èƒ½å¤Ÿä¸ŽçŽ°æœ‰çš„é‡‘èžæœåŠ¡é›†æˆä½¿ç”¨ã€‚\n",
      "\n",
      "6. è·¨é“¾é€šä¿¡ï¼š\n",
      "ç›®å‰å¤§å¤šæ•°åŒºå—é“¾ç³»ç»Ÿæ˜¯ç‹¬ç«‹è¿è¡Œå¹¶äº’ä¸å…¼å®¹çš„ï¼Œè¿™æ„å‘³ç€ä¸åŒç½‘ç»œä¹‹é—´çš„æ•°æ®äº¤æ¢å­˜åœ¨æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œè·¨é“¾æŠ€æœ¯å…è®¸åœ¨ä¸åŒçš„åŒºå—é“¾ä¹‹é—´ä¼ è¾“ä¿¡æ¯å’Œä»·å€¼ã€‚\n",
      "\n",
      "é€šè¿‡ä¸Šè¿°ç‰¹ç‚¹ï¼ŒåŒºå—é“¾æŠ€æœ¯å·²è¢«å¹¿æ³›åº”ç”¨äºŽæ•°å­—è´§å¸ã€ä¾›åº”é“¾ç®¡ç†ä»¥åŠæ™ºèƒ½åˆçº¦ç­‰é¢†åŸŸï¼Œå¹¶å±•çŽ°å‡ºå·¨å¤§çš„æ½œåŠ›å’Œå‘å±•å‰æ™¯ã€‚\n",
      "\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 3. ä½¿ç”¨æç¤ºæ¨¡æ¿",
   "id": "e9912694a84a7acd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# 3. ä½¿ç”¨æç¤ºæ¨¡æ¿\n",
    "def with_prompt_templates():\n",
    "    \"\"\"ä½¿ç”¨æç¤ºæ¨¡æ¿\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"3. ä½¿ç”¨æç¤ºæ¨¡æ¿\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    llm = OllamaLLM(\n",
    "        base_url=\"http://localhost:11434\",\n",
    "        model=\"qwen2.5:3b\"\n",
    "    )\n",
    "\n",
    "    # åˆ›å»ºæç¤ºæ¨¡æ¿\n",
    "    template = \"\"\"\n",
    "    ä½œä¸ºä¸€ä¸ª{role}ï¼Œè¯·ç”¨{style}çš„é£Žæ ¼å›žç­”ä»¥ä¸‹é—®é¢˜ï¼š\n",
    "\n",
    "    é—®é¢˜ï¼š{question}\n",
    "\n",
    "    å›žç­”ï¼š\n",
    "    \"\"\"\n",
    "\n",
    "    prompt = PromptTemplate(\n",
    "        input_variables=[\"role\", \"style\", \"question\"],\n",
    "        template=template\n",
    "    )\n",
    "\n",
    "    # åˆ›å»ºé“¾\n",
    "    chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "    # æµ‹è¯•ä¸åŒè§’è‰²å’Œé£Žæ ¼\n",
    "    examples = [\n",
    "        {\"role\": \"åŽ†å²å­¦å®¶\", \"style\": \"å­¦æœ¯\", \"question\": \"ç§¦å§‹çš‡ç»Ÿä¸€ä¸­å›½çš„æ„ä¹‰\"},\n",
    "        {\"role\": \"è¯—äºº\", \"style\": \"æµªæ¼«\", \"question\": \"æ˜¥å¤©çš„ç‰¹ç‚¹\"},\n",
    "        {\"role\": \"ç¨‹åºå‘˜\", \"style\": \"æŠ€æœ¯\", \"question\": \"ä»€ä¹ˆæ˜¯ç®—æ³•å¤æ‚åº¦\"}\n",
    "    ]\n",
    "\n",
    "    for example in examples:\n",
    "        response = chain.invoke(example)\n",
    "        print(f\"\\n{example['role']}({example['style']})å›žç­”:\")\n",
    "        print(response[:200] + \"...\")"
   ],
   "id": "77c325c464045e18",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 4. æ‰¹é‡å¤„ç†ä¼˜åŒ–",
   "id": "a13852bfd359eda4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "# 4. æ‰¹é‡å¤„ç†ä¼˜åŒ–\n",
    "def batch_processing():\n",
    "    \"\"\"æ‰¹é‡å¤„ç†ç¤ºä¾‹\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"4. æ‰¹é‡å¤„ç†ç¤ºä¾‹\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    llm = OllamaLLM(\n",
    "        base_url=\"http://localhost:11434\",\n",
    "        model=\"qwen2.5:3b\"\n",
    "    )\n",
    "\n",
    "    # æ‰¹é‡æç¤º\n",
    "    prompts = [\n",
    "        \"è§£é‡Šä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ \",\n",
    "        \"Pythonçš„ä¼˜åŠ¿æ˜¯ä»€ä¹ˆ\",\n",
    "        \"ä»€ä¹ˆæ˜¯äº‘è®¡ç®—\",\n",
    "        \"åŒºå—é“¾çš„åº”ç”¨åœºæ™¯\",\n",
    "        \"äººå·¥æ™ºèƒ½çš„å‘å±•åŽ†ç¨‹\"\n",
    "    ]\n",
    "\n",
    "    # è®¡æ—¶æ‰¹é‡å¤„ç†\n",
    "    start_time = time.time()\n",
    "    responses = llm.batch(prompts)\n",
    "    batch_time = time.time() - start_time\n",
    "\n",
    "    print(f\"æ‰¹é‡å¤„ç†{len(prompts)}ä¸ªè¯·æ±‚è€—æ—¶: {batch_time:.2f}ç§’\")\n",
    "\n",
    "    for i, response in enumerate(responses):\n",
    "        print(f\"\\né—®é¢˜{i+1}: {prompts[i]}\")\n",
    "        print(f\"å›žç­”: {response[:100]}...\")"
   ],
   "id": "a7b506e56533e663",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 5. å‚æ•°è°ƒä¼˜ç¤ºä¾‹",
   "id": "f978c6860150b7ce"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "# 5. å‚æ•°è°ƒä¼˜ç¤ºä¾‹\n",
    "def parameter_tuning():\n",
    "    \"\"\"å‚æ•°è°ƒä¼˜ç¤ºä¾‹\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"5. å‚æ•°è°ƒä¼˜ç¤ºä¾‹\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    prompt = \"åˆ›ä½œä¸€ä¸ªç§‘å¹»æ•…äº‹çš„å¼€å¤´ï¼š\"\n",
    "\n",
    "    # ä¸åŒæ¸©åº¦è®¾ç½®\n",
    "    temperatures = [0.1, 0.5, 0.9, 1.2]\n",
    "\n",
    "    for temp in temperatures:\n",
    "        llm = Ollama(\n",
    "            base_url=\"http://localhost:11434\",\n",
    "            model=\"qwen2.5:3b\",\n",
    "            temperature=temp\n",
    "        )\n",
    "\n",
    "        response = llm.invoke(prompt)\n",
    "        print(f\"\\næ¸©åº¦ {temp}:\")\n",
    "        print(response[:150] + \"...\")"
   ],
   "id": "78d04c6cd28edd48",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 6. è‡ªå®šä¹‰å›žè°ƒå¤„ç†",
   "id": "29bb15f31d8d2db7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-22T06:57:01.688375Z",
     "start_time": "2025-07-22T06:56:47.985626Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 6. è‡ªå®šä¹‰å›žè°ƒå¤„ç†\n",
    "class CustomCallbackHandler(BaseCallbackHandler):\n",
    "    \"\"\"è‡ªå®šä¹‰å›žè°ƒå¤„ç†å™¨\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.tokens = []\n",
    "        self.start_time = None\n",
    "\n",
    "    def on_llm_start(self, serialized, prompts, **kwargs):\n",
    "        self.start_time = time.time()\n",
    "        print(f\"å¼€å§‹ç”Ÿæˆï¼Œæç¤ºæ•°é‡: {len(prompts)}\")\n",
    "\n",
    "    def on_llm_new_token(self, token: str, **kwargs):\n",
    "        self.tokens.append(token)\n",
    "        print(f\"æ–°token: '{token}'\", end=\"\")\n",
    "\n",
    "    def on_llm_end(self, response, **kwargs):\n",
    "        duration = time.time() - self.start_time\n",
    "        total_tokens = len(self.tokens)\n",
    "        print(f\"\\nç”Ÿæˆå®Œæˆï¼Œè€—æ—¶: {duration:.2f}ç§’ï¼Œtokenæ•°: {total_tokens}\")\n",
    "        self.tokens = []\n",
    "\n",
    "def callback_example():\n",
    "    \"\"\"å›žè°ƒç¤ºä¾‹\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"6. å›žè°ƒç¤ºä¾‹\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    llm = OllamaLLM(\n",
    "        base_url=\"http://localhost:11434\",\n",
    "        model=\"qwen2.5:3b\"\n",
    "    )\n",
    "\n",
    "    callback_handler = CustomCallbackHandler()\n",
    "\n",
    "    response = llm.invoke(\n",
    "        \"è¯·ç®€å•ä»‹ç»ä¸€ä¸‹é‡å­è®¡ç®—\",\n",
    "        config={\"callbacks\": [callback_handler]}\n",
    "    )\n",
    "callback_example()"
   ],
   "id": "7921569a8c0d2b48",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "6. å›žè°ƒç¤ºä¾‹\n",
      "==================================================\n",
      "å¼€å§‹ç”Ÿæˆï¼Œæç¤ºæ•°é‡: 1\n",
      "æ–°token: 'é‡å­'æ–°token: 'è®¡ç®—'æ–°token: 'æ˜¯ä¸€ç§'æ–°token: 'åŸºäºŽ'æ–°token: 'é‡å­'æ–°token: 'åŠ›å­¦'æ–°token: 'åŽŸç†'æ–°token: 'çš„'æ–°token: 'è®¡ç®—'æ–°token: 'æ–¹å¼'æ–°token: 'ï¼Œ'æ–°token: 'å®ƒ'æ–°token: 'åˆ©ç”¨'æ–°token: 'é‡å­'æ–°token: 'ä½'æ–°token: 'ï¼ˆ'æ–°token: 'é‡å­'æ–°token: 'æ¯”ç‰¹'æ–°token: 'æˆ–'æ–°token: 'q'æ–°token: 'ubit'æ–°token: 'ï¼‰'æ–°token: 'æ¥'æ–°token: 'å­˜å‚¨'æ–°token: 'å’Œ'æ–°token: 'å¤„ç†'æ–°token: 'ä¿¡æ¯'æ–°token: 'ã€‚'æ–°token: 'ä¸Ž'æ–°token: 'ç»å…¸'æ–°token: 'è®¡ç®—æœº'æ–°token: 'ä½¿ç”¨çš„'æ–°token: 'äºŒ'æ–°token: 'è¿›'æ–°token: 'åˆ¶'æ–°token: 'ä½'æ–°token: 'ï¼ˆ'æ–°token: '0'æ–°token: ' æˆ–'æ–°token: ' 'æ–°token: '1'æ–°token: 'ï¼‰'æ–°token: 'ä¸åŒ'æ–°token: 'çš„æ˜¯'æ–°token: 'ï¼Œ'æ–°token: 'é‡å­'æ–°token: 'ä½'æ–°token: 'å¯ä»¥'æ–°token: 'åŒæ—¶'æ–°token: 'è¡¨ç¤º'æ–°token: '0'æ–°token: ' å’Œ'æ–°token: ' 'æ–°token: '1'æ–°token: ' çš„'æ–°token: 'å åŠ 'æ–°token: 'æ€'æ–°token: 'ï¼Œ'æ–°token: 'è¿™'æ–°token: 'è¢«ç§°ä¸º'æ–°token: 'â€œ'æ–°token: 'é‡å­'æ–°token: 'å åŠ 'æ–°token: 'â€ã€‚\n",
      "\n",
      "'æ–°token: 'æ­¤å¤–'æ–°token: 'ï¼Œ'æ–°token: 'é‡å­'æ–°token: 'æ¯”ç‰¹'æ–°token: 'è¿˜å¯ä»¥'æ–°token: 'å®žçŽ°'æ–°token: 'ä¸€ç§'æ–°token: 'ç§°ä¸º'æ–°token: 'â€œ'æ–°token: 'é‡å­'æ–°token: 'çº ç¼ 'æ–°token: 'â€çš„'æ–°token: 'çŽ°è±¡'æ–°token: 'ï¼Œåœ¨'æ–°token: 'è¿™ç§'æ–°token: 'çŠ¶æ€ä¸‹'æ–°token: 'ï¼Œ'æ–°token: 'ä¸€ä¸ª'æ–°token: 'é‡å­'æ–°token: 'ä½'æ–°token: 'çš„çŠ¶æ€'æ–°token: 'ä¼š'æ–°token: 'ä¾èµ–'æ–°token: 'äºŽ'æ–°token: 'å¦ä¸€ä¸ª'æ–°token: 'æˆ–'æ–°token: 'å¤šä¸ª'æ–°token: 'é‡å­'æ–°token: 'ä½'æ–°token: 'çš„çŠ¶æ€'æ–°token: 'ã€‚'æ–°token: 'å³ä½¿'æ–°token: 'åœ¨'æ–°token: 'é¥è¿œ'æ–°token: 'çš„è·ç¦»'æ–°token: 'ä¸Š'æ–°token: 'ï¼Œ'æ–°token: 'é‡å­'æ–°token: 'ä½'æ–°token: 'ä¹‹é—´çš„'æ–°token: 'çŠ¶æ€'æ–°token: 'ä¹Ÿä¼š'æ–°token: 'ä¿æŒ'æ–°token: 'ç›¸å…³'æ–°token: 'æ€§'æ–°token: 'ã€‚\n",
      "\n",
      "'æ–°token: 'è¿™äº›'æ–°token: 'ç‰¹æ€§'æ–°token: 'ä½¿å¾—'æ–°token: 'é‡å­'æ–°token: 'è®¡ç®—æœº'æ–°token: 'åœ¨'æ–°token: 'å¤„ç†'æ–°token: 'æŸäº›'æ–°token: 'ç‰¹å®š'æ–°token: 'ç±»åž‹'æ–°token: 'çš„é—®é¢˜'æ–°token: 'æ—¶'æ–°token: 'å¯èƒ½'æ–°token: 'è¡¨çŽ°å‡º'æ–°token: 'è¿œè¿œ'æ–°token: 'è¶…è¿‡'æ–°token: 'ç»å…¸'æ–°token: 'è®¡ç®—æœº'æ–°token: 'çš„é€Ÿåº¦'æ–°token: 'ä¼˜åŠ¿'æ–°token: 'ã€‚'æ–°token: 'ä¾‹å¦‚'æ–°token: 'ï¼Œ'æ–°token: 'å¯¹äºŽ'æ–°token: 'é‚£äº›'æ–°token: 'å…·æœ‰'æ–°token: 'æŒ‡æ•°'æ–°token: 'çº§'æ–°token: 'å¤æ‚'æ–°token: 'åº¦'æ–°token: 'çš„ç»å…¸'æ–°token: 'é—®é¢˜'æ–°token: 'ï¼Œ'æ–°token: 'å¦‚æžœ'æ–°token: 'æ‰¾åˆ°'æ–°token: 'åˆé€‚çš„'æ–°token: 'ç®—æ³•'æ–°token: 'ï¼Œ'æ–°token: 'é‡å­'æ–°token: 'è®¡ç®—æœº'æ–°token: 'å¯èƒ½'æ–°token: 'èƒ½å¤Ÿåœ¨'æ–°token: 'å¤šé¡¹'æ–°token: 'å¼'æ–°token: 'æ—¶é—´å†…'æ–°token: 'è§£å†³'æ–°token: 'è¿™äº›é—®é¢˜'æ–°token: 'ã€‚\n",
      "\n",
      "'æ–°token: 'å°½ç®¡'æ–°token: 'å¦‚æ­¤'æ–°token: 'ï¼Œ'æ–°token: 'ç›®å‰'æ–°token: 'å®žçŽ°'æ–°token: 'é«˜æ•ˆçš„'æ–°token: 'ã€'æ–°token: 'å¤§è§„æ¨¡'æ–°token: 'çš„'æ–°token: 'é‡å­'æ–°token: 'è®¡ç®—'æ–°token: 'ä»'æ–°token: 'é¢ä¸´'æ–°token: 'è¯¸å¤š'æ–°token: 'æŒ‘æˆ˜'æ–°token: 'ï¼Œ'æ–°token: 'åŒ…æ‹¬'æ–°token: 'å¦‚ä½•'æ–°token: 'åˆ›å»º'æ–°token: 'å’Œ'æ–°token: 'ä¿æŒ'æ–°token: 'é‡å­'æ–°token: 'æ€'æ–°token: 'ç¨³å®š'æ–°token: 'ç­‰'æ–°token: 'æŠ€æœ¯'æ–°token: 'éš¾é¢˜'æ–°token: 'ã€‚'æ–°token: 'é‡å­'æ–°token: 'è®¡ç®—'æ–°token: 'çš„ç ”ç©¶'æ–°token: 'è¿˜åœ¨'æ–°token: 'æŒç»­'æ–°token: 'å‘å±•ä¸­'æ–°token: 'ï¼Œ'æ–°token: 'æœªæ¥'æ–°token: 'å¯èƒ½ä¼š'æ–°token: 'ä¸º'æ–°token: 'ä¿¡æ¯'æ–°token: 'å¤„ç†'æ–°token: 'å¸¦æ¥'æ–°token: 'é©å‘½'æ–°token: 'æ€§çš„'æ–°token: 'å˜åŒ–'æ–°token: 'ã€‚'æ–°token: ''\n",
      "ç”Ÿæˆå®Œæˆï¼Œè€—æ—¶: 13.63ç§’ï¼Œtokenæ•°: 204\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 7. ç¼“å­˜æœºåˆ¶ç¤ºä¾‹",
   "id": "579b8d9465f7af09"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-22T06:59:43.708908Z",
     "start_time": "2025-07-22T06:59:34.661157Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# 7. ç¼“å­˜æœºåˆ¶ç¤ºä¾‹\n",
    "def caching_example():\n",
    "    \"\"\"ç¼“å­˜æœºåˆ¶ç¤ºä¾‹\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"7. ç¼“å­˜æœºåˆ¶ç¤ºä¾‹\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    llm = OllamaLLM(\n",
    "        base_url=\"http://localhost:11434\",\n",
    "        model=\"qwen2.5:3b\"\n",
    "    )\n",
    "\n",
    "    prompt = \"ä»€ä¹ˆæ˜¯æ·±åº¦å­¦ä¹ ï¼Ÿ\"\n",
    "\n",
    "    # ç¬¬ä¸€æ¬¡è°ƒç”¨ï¼ˆæ— ç¼“å­˜ï¼‰\n",
    "    start_time = time.time()\n",
    "    response1 = llm.invoke(prompt)\n",
    "    first_call_time = time.time() - start_time\n",
    "\n",
    "    # ç¬¬äºŒæ¬¡è°ƒç”¨ï¼ˆæœ‰ç¼“å­˜ï¼‰\n",
    "    start_time = time.time()\n",
    "    response2 = llm.invoke(prompt)\n",
    "    second_call_time = time.time() - start_time\n",
    "\n",
    "    print(f\"ç¬¬ä¸€æ¬¡è°ƒç”¨è€—æ—¶: {first_call_time:.2f}ç§’\")\n",
    "    print(f\"ç¬¬äºŒæ¬¡è°ƒç”¨è€—æ—¶: {second_call_time:.2f}ç§’\")\n",
    "    print(f\"ç¼“å­˜åŠ é€Ÿæ¯”: {first_call_time/second_call_time:.2f}x\")\n",
    "    print(f\"å“åº”ä¸€è‡´æ€§: {response1 == response2}\")\n",
    "caching_example()"
   ],
   "id": "122fc8163db7d2ae",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "7. ç¼“å­˜æœºåˆ¶ç¤ºä¾‹\n",
      "==================================================\n",
      "ç¬¬ä¸€æ¬¡è°ƒç”¨è€—æ—¶: 8.98ç§’\n",
      "ç¬¬äºŒæ¬¡è°ƒç”¨è€—æ—¶: 0.00ç§’\n",
      "ç¼“å­˜åŠ é€Ÿæ¯”: 36483.76x\n",
      "å“åº”ä¸€è‡´æ€§: True\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 8. å¼‚æ­¥å¤„ç†ç¤ºä¾‹",
   "id": "60a2cdfb6e99cbb1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-22T07:07:48.853620Z",
     "start_time": "2025-07-22T07:06:53.559302Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 8. å¼‚æ­¥å¤„ç†ç¤ºä¾‹\n",
    "async def async_llm_example():\n",
    "    \"\"\"å¼‚æ­¥å¤„ç†ç¤ºä¾‹\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"8. å¼‚æ­¥å¤„ç†ç¤ºä¾‹\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    llm = OllamaLLM(\n",
    "        base_url=\"http://localhost:11434\",\n",
    "        model=\"qwen2.5:3b\"\n",
    "    )\n",
    "\n",
    "    # å¼‚æ­¥å•æ¬¡è°ƒç”¨\n",
    "    response = await llm.ainvoke(\"å¼‚æ­¥è°ƒç”¨æµ‹è¯•\")\n",
    "    print(f\"å¼‚æ­¥å“åº”: {response[:100]}...\")\n",
    "\n",
    "    # å¼‚æ­¥æ‰¹é‡è°ƒç”¨\n",
    "    prompts = [\n",
    "        \"ä»€ä¹ˆæ˜¯APIï¼Ÿ\",\n",
    "        \"ä»€ä¹ˆæ˜¯æ•°æ®åº“ï¼Ÿ\",\n",
    "        \"ä»€ä¹ˆæ˜¯ç½‘ç»œåè®®ï¼Ÿ\"\n",
    "    ]\n",
    "\n",
    "    start_time = time.time()\n",
    "    responses = await llm.abatch(prompts)\n",
    "    async_time = time.time() - start_time\n",
    "\n",
    "    print(f\"\\nå¼‚æ­¥æ‰¹é‡å¤„ç†è€—æ—¶: {async_time:.2f}ç§’\")\n",
    "    for i, response in enumerate(responses):\n",
    "        print(f\"é—®é¢˜{i+1}å›žç­”: {response[:80]}...\")\n",
    "\n",
    "    # å¼‚æ­¥æµå¼å¤„ç†\n",
    "    print(\"\\nå¼‚æ­¥æµå¼è¾“å‡º:\")\n",
    "    async for chunk in llm.astream(\"è¯·ä»‹ç»ä¸€ä¸‹äº‘åŽŸç”ŸæŠ€æœ¯\"):\n",
    "        print(chunk, end=\"\", flush=True)\n",
    "    print(\"\\n\")\n",
    "\n",
    "# Jupyter Notebook ä¸­çš„æ­£ç¡®è¿è¡Œæ–¹å¼\n",
    "print(\"\\nè¿è¡Œå¼‚æ­¥ç¤ºä¾‹...\")\n",
    "await async_llm_example()"
   ],
   "id": "110c1d84fd062be8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "è¿è¡Œå¼‚æ­¥ç¤ºä¾‹...\n",
      "\n",
      "==================================================\n",
      "8. å¼‚æ­¥å¤„ç†ç¤ºä¾‹\n",
      "==================================================\n",
      "å¼‚æ­¥å“åº”: å¼‚æ­¥è°ƒç”¨æ˜¯ä¸€ç§åœ¨ä¸é˜»å¡žä¸»çº¿ç¨‹çš„æƒ…å†µä¸‹è¿›è¡Œè¯·æ±‚æˆ–æ‰§è¡Œæ“ä½œçš„æŠ€æœ¯ã€‚å®ƒå…è®¸ä¸€ä¸ªæ“ä½œå¼€å§‹åŽï¼Œå³ä½¿å®¢æˆ·ç«¯è¿˜åœ¨ç­‰å¾…å…¶ä»–ä»»åŠ¡å®Œæˆæ—¶ï¼Œå¦ä¸€ä¸ªä»»åŠ¡å·²ç»å¼€å§‹å¤„ç†ã€‚è¿™é€šå¸¸ç”¨äºŽæé«˜ç”¨æˆ·ä½“éªŒå’Œç³»ç»Ÿçš„å“åº”æ€§ã€‚ä¸‹é¢æˆ‘å°†é€šè¿‡å‡ ä¸ªç¤ºä¾‹...\n"
     ]
    },
    {
     "ename": "CancelledError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mCancelledError\u001B[39m                            Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[7]\u001B[39m\u001B[32m, line 40\u001B[39m\n\u001B[32m     38\u001B[39m \u001B[38;5;66;03m# Jupyter Notebook ä¸­çš„æ­£ç¡®è¿è¡Œæ–¹å¼\u001B[39;00m\n\u001B[32m     39\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33mè¿è¡Œå¼‚æ­¥ç¤ºä¾‹...\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m---> \u001B[39m\u001B[32m40\u001B[39m \u001B[38;5;28;01mawait\u001B[39;00m async_llm_example()\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[7]\u001B[39m\u001B[32m, line 25\u001B[39m, in \u001B[36masync_llm_example\u001B[39m\u001B[34m()\u001B[39m\n\u001B[32m     18\u001B[39m prompts = [\n\u001B[32m     19\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mä»€ä¹ˆæ˜¯APIï¼Ÿ\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m     20\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mä»€ä¹ˆæ˜¯æ•°æ®åº“ï¼Ÿ\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m     21\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mä»€ä¹ˆæ˜¯ç½‘ç»œåè®®ï¼Ÿ\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m     22\u001B[39m ]\n\u001B[32m     24\u001B[39m start_time = time.time()\n\u001B[32m---> \u001B[39m\u001B[32m25\u001B[39m responses = \u001B[38;5;28;01mawait\u001B[39;00m llm.abatch(prompts)\n\u001B[32m     26\u001B[39m async_time = time.time() - start_time\n\u001B[32m     28\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33må¼‚æ­¥æ‰¹é‡å¤„ç†è€—æ—¶: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00masync_time\u001B[38;5;132;01m:\u001B[39;00m\u001B[33m.2f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33mç§’\u001B[39m\u001B[33m\"\u001B[39m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/FastAPIProject/.venv/lib/python3.11/site-packages/langchain_core/language_models/llms.py:488\u001B[39m, in \u001B[36mBaseLLM.abatch\u001B[39m\u001B[34m(self, inputs, config, return_exceptions, **kwargs)\u001B[39m\n\u001B[32m    486\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m max_concurrency \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m    487\u001B[39m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m488\u001B[39m         llm_result = \u001B[38;5;28;01mawait\u001B[39;00m \u001B[38;5;28mself\u001B[39m.agenerate_prompt(\n\u001B[32m    489\u001B[39m             [\u001B[38;5;28mself\u001B[39m._convert_input(input_) \u001B[38;5;28;01mfor\u001B[39;00m input_ \u001B[38;5;129;01min\u001B[39;00m inputs],\n\u001B[32m    490\u001B[39m             callbacks=[c.get(\u001B[33m\"\u001B[39m\u001B[33mcallbacks\u001B[39m\u001B[33m\"\u001B[39m) \u001B[38;5;28;01mfor\u001B[39;00m c \u001B[38;5;129;01min\u001B[39;00m config],\n\u001B[32m    491\u001B[39m             tags=[c.get(\u001B[33m\"\u001B[39m\u001B[33mtags\u001B[39m\u001B[33m\"\u001B[39m) \u001B[38;5;28;01mfor\u001B[39;00m c \u001B[38;5;129;01min\u001B[39;00m config],\n\u001B[32m    492\u001B[39m             metadata=[c.get(\u001B[33m\"\u001B[39m\u001B[33mmetadata\u001B[39m\u001B[33m\"\u001B[39m) \u001B[38;5;28;01mfor\u001B[39;00m c \u001B[38;5;129;01min\u001B[39;00m config],\n\u001B[32m    493\u001B[39m             run_name=[c.get(\u001B[33m\"\u001B[39m\u001B[33mrun_name\u001B[39m\u001B[33m\"\u001B[39m) \u001B[38;5;28;01mfor\u001B[39;00m c \u001B[38;5;129;01min\u001B[39;00m config],\n\u001B[32m    494\u001B[39m             **kwargs,\n\u001B[32m    495\u001B[39m         )\n\u001B[32m    496\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m [g[\u001B[32m0\u001B[39m].text \u001B[38;5;28;01mfor\u001B[39;00m g \u001B[38;5;129;01min\u001B[39;00m llm_result.generations]\n\u001B[32m    497\u001B[39m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/FastAPIProject/.venv/lib/python3.11/site-packages/langchain_core/language_models/llms.py:777\u001B[39m, in \u001B[36mBaseLLM.agenerate_prompt\u001B[39m\u001B[34m(self, prompts, stop, callbacks, **kwargs)\u001B[39m\n\u001B[32m    768\u001B[39m \u001B[38;5;129m@override\u001B[39m\n\u001B[32m    769\u001B[39m \u001B[38;5;28;01masync\u001B[39;00m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34magenerate_prompt\u001B[39m(\n\u001B[32m    770\u001B[39m     \u001B[38;5;28mself\u001B[39m,\n\u001B[32m   (...)\u001B[39m\u001B[32m    774\u001B[39m     **kwargs: Any,\n\u001B[32m    775\u001B[39m ) -> LLMResult:\n\u001B[32m    776\u001B[39m     prompt_strings = [p.to_string() \u001B[38;5;28;01mfor\u001B[39;00m p \u001B[38;5;129;01min\u001B[39;00m prompts]\n\u001B[32m--> \u001B[39m\u001B[32m777\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mawait\u001B[39;00m \u001B[38;5;28mself\u001B[39m.agenerate(\n\u001B[32m    778\u001B[39m         prompt_strings, stop=stop, callbacks=callbacks, **kwargs\n\u001B[32m    779\u001B[39m     )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/FastAPIProject/.venv/lib/python3.11/site-packages/langchain_core/language_models/llms.py:1249\u001B[39m, in \u001B[36mBaseLLM.agenerate\u001B[39m\u001B[34m(self, prompts, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001B[39m\n\u001B[32m   1235\u001B[39m run_managers = \u001B[38;5;28;01mawait\u001B[39;00m asyncio.gather(\n\u001B[32m   1236\u001B[39m     *[\n\u001B[32m   1237\u001B[39m         callback_managers[idx].on_llm_start(\n\u001B[32m   (...)\u001B[39m\u001B[32m   1246\u001B[39m     ]\n\u001B[32m   1247\u001B[39m )\n\u001B[32m   1248\u001B[39m run_managers = [r[\u001B[32m0\u001B[39m] \u001B[38;5;28;01mfor\u001B[39;00m r \u001B[38;5;129;01min\u001B[39;00m run_managers]  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m1249\u001B[39m new_results = \u001B[38;5;28;01mawait\u001B[39;00m \u001B[38;5;28mself\u001B[39m._agenerate_helper(\n\u001B[32m   1250\u001B[39m     missing_prompts,\n\u001B[32m   1251\u001B[39m     stop,\n\u001B[32m   1252\u001B[39m     run_managers,  \u001B[38;5;66;03m# type: ignore[arg-type]\u001B[39;00m\n\u001B[32m   1253\u001B[39m     new_arg_supported=\u001B[38;5;28mbool\u001B[39m(new_arg_supported),\n\u001B[32m   1254\u001B[39m     **kwargs,\n\u001B[32m   1255\u001B[39m )\n\u001B[32m   1256\u001B[39m llm_output = \u001B[38;5;28;01mawait\u001B[39;00m aupdate_cache(\n\u001B[32m   1257\u001B[39m     \u001B[38;5;28mself\u001B[39m.cache,\n\u001B[32m   1258\u001B[39m     existing_prompts,\n\u001B[32m   (...)\u001B[39m\u001B[32m   1262\u001B[39m     prompts,\n\u001B[32m   1263\u001B[39m )\n\u001B[32m   1264\u001B[39m run_info = (\n\u001B[32m   1265\u001B[39m     [RunInfo(run_id=run_manager.run_id) \u001B[38;5;28;01mfor\u001B[39;00m run_manager \u001B[38;5;129;01min\u001B[39;00m run_managers]  \u001B[38;5;66;03m# type: ignore[attr-defined]\u001B[39;00m\n\u001B[32m   1266\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m run_managers\n\u001B[32m   1267\u001B[39m     \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1268\u001B[39m )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/FastAPIProject/.venv/lib/python3.11/site-packages/langchain_core/language_models/llms.py:1043\u001B[39m, in \u001B[36mBaseLLM._agenerate_helper\u001B[39m\u001B[34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001B[39m\n\u001B[32m   1032\u001B[39m \u001B[38;5;28;01masync\u001B[39;00m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m_agenerate_helper\u001B[39m(\n\u001B[32m   1033\u001B[39m     \u001B[38;5;28mself\u001B[39m,\n\u001B[32m   1034\u001B[39m     prompts: \u001B[38;5;28mlist\u001B[39m[\u001B[38;5;28mstr\u001B[39m],\n\u001B[32m   (...)\u001B[39m\u001B[32m   1039\u001B[39m     **kwargs: Any,\n\u001B[32m   1040\u001B[39m ) -> LLMResult:\n\u001B[32m   1041\u001B[39m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m   1042\u001B[39m         output = (\n\u001B[32m-> \u001B[39m\u001B[32m1043\u001B[39m             \u001B[38;5;28;01mawait\u001B[39;00m \u001B[38;5;28mself\u001B[39m._agenerate(\n\u001B[32m   1044\u001B[39m                 prompts,\n\u001B[32m   1045\u001B[39m                 stop=stop,\n\u001B[32m   1046\u001B[39m                 run_manager=run_managers[\u001B[32m0\u001B[39m] \u001B[38;5;28;01mif\u001B[39;00m run_managers \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[32m   1047\u001B[39m                 **kwargs,\n\u001B[32m   1048\u001B[39m             )\n\u001B[32m   1049\u001B[39m             \u001B[38;5;28;01mif\u001B[39;00m new_arg_supported\n\u001B[32m   1050\u001B[39m             \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mawait\u001B[39;00m \u001B[38;5;28mself\u001B[39m._agenerate(prompts, stop=stop)\n\u001B[32m   1051\u001B[39m         )\n\u001B[32m   1052\u001B[39m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[32m   1053\u001B[39m         \u001B[38;5;28;01mawait\u001B[39;00m asyncio.gather(\n\u001B[32m   1054\u001B[39m             *[\n\u001B[32m   1055\u001B[39m                 run_manager.on_llm_error(e, response=LLMResult(generations=[]))\n\u001B[32m   1056\u001B[39m                 \u001B[38;5;28;01mfor\u001B[39;00m run_manager \u001B[38;5;129;01min\u001B[39;00m run_managers\n\u001B[32m   1057\u001B[39m             ]\n\u001B[32m   1058\u001B[39m         )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/FastAPIProject/.venv/lib/python3.11/site-packages/langchain_ollama/llms.py:378\u001B[39m, in \u001B[36mOllamaLLM._agenerate\u001B[39m\u001B[34m(self, prompts, stop, run_manager, **kwargs)\u001B[39m\n\u001B[32m    376\u001B[39m generations = []\n\u001B[32m    377\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m prompt \u001B[38;5;129;01min\u001B[39;00m prompts:\n\u001B[32m--> \u001B[39m\u001B[32m378\u001B[39m     final_chunk = \u001B[38;5;28;01mawait\u001B[39;00m \u001B[38;5;28mself\u001B[39m._astream_with_aggregation(\n\u001B[32m    379\u001B[39m         prompt,\n\u001B[32m    380\u001B[39m         stop=stop,\n\u001B[32m    381\u001B[39m         run_manager=run_manager,\n\u001B[32m    382\u001B[39m         verbose=\u001B[38;5;28mself\u001B[39m.verbose,\n\u001B[32m    383\u001B[39m         **kwargs,\n\u001B[32m    384\u001B[39m     )\n\u001B[32m    385\u001B[39m     generations.append([final_chunk])\n\u001B[32m    386\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m LLMResult(generations=generations)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/FastAPIProject/.venv/lib/python3.11/site-packages/langchain_ollama/llms.py:276\u001B[39m, in \u001B[36mOllamaLLM._astream_with_aggregation\u001B[39m\u001B[34m(self, prompt, stop, run_manager, verbose, **kwargs)\u001B[39m\n\u001B[32m    274\u001B[39m final_chunk = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m    275\u001B[39m thinking_content = \u001B[33m\"\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m--> \u001B[39m\u001B[32m276\u001B[39m \u001B[38;5;28;01masync\u001B[39;00m \u001B[38;5;28;01mfor\u001B[39;00m stream_resp \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m._acreate_generate_stream(prompt, stop, **kwargs):\n\u001B[32m    277\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(stream_resp, \u001B[38;5;28mstr\u001B[39m):\n\u001B[32m    278\u001B[39m         \u001B[38;5;28;01mif\u001B[39;00m stream_resp.get(\u001B[33m\"\u001B[39m\u001B[33mthinking\u001B[39m\u001B[33m\"\u001B[39m):\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/FastAPIProject/.venv/lib/python3.11/site-packages/langchain_ollama/llms.py:250\u001B[39m, in \u001B[36mOllamaLLM._acreate_generate_stream\u001B[39m\u001B[34m(self, prompt, stop, **kwargs)\u001B[39m\n\u001B[32m    243\u001B[39m \u001B[38;5;28;01masync\u001B[39;00m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m_acreate_generate_stream\u001B[39m(\n\u001B[32m    244\u001B[39m     \u001B[38;5;28mself\u001B[39m,\n\u001B[32m    245\u001B[39m     prompt: \u001B[38;5;28mstr\u001B[39m,\n\u001B[32m    246\u001B[39m     stop: Optional[\u001B[38;5;28mlist\u001B[39m[\u001B[38;5;28mstr\u001B[39m]] = \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[32m    247\u001B[39m     **kwargs: Any,\n\u001B[32m    248\u001B[39m ) -> AsyncIterator[Union[Mapping[\u001B[38;5;28mstr\u001B[39m, Any], \u001B[38;5;28mstr\u001B[39m]]:\n\u001B[32m    249\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m._async_client:\n\u001B[32m--> \u001B[39m\u001B[32m250\u001B[39m         \u001B[38;5;28;01masync\u001B[39;00m \u001B[38;5;28;01mfor\u001B[39;00m part \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28;01mawait\u001B[39;00m \u001B[38;5;28mself\u001B[39m._async_client.generate(\n\u001B[32m    251\u001B[39m             **\u001B[38;5;28mself\u001B[39m._generate_params(prompt, stop=stop, **kwargs)\n\u001B[32m    252\u001B[39m         ):\n\u001B[32m    253\u001B[39m             \u001B[38;5;28;01myield\u001B[39;00m part\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/FastAPIProject/.venv/lib/python3.11/site-packages/ollama/_client.py:684\u001B[39m, in \u001B[36mAsyncClient._request.<locals>.inner\u001B[39m\u001B[34m()\u001B[39m\n\u001B[32m    681\u001B[39m   \u001B[38;5;28;01mawait\u001B[39;00m e.response.aread()\n\u001B[32m    682\u001B[39m   \u001B[38;5;28;01mraise\u001B[39;00m ResponseError(e.response.text, e.response.status_code) \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m684\u001B[39m \u001B[38;5;28;01masync\u001B[39;00m \u001B[38;5;28;01mfor\u001B[39;00m line \u001B[38;5;129;01min\u001B[39;00m r.aiter_lines():\n\u001B[32m    685\u001B[39m   part = json.loads(line)\n\u001B[32m    686\u001B[39m   \u001B[38;5;28;01mif\u001B[39;00m err := part.get(\u001B[33m'\u001B[39m\u001B[33merror\u001B[39m\u001B[33m'\u001B[39m):\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/FastAPIProject/.venv/lib/python3.11/site-packages/httpx/_models.py:1031\u001B[39m, in \u001B[36mResponse.aiter_lines\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m   1029\u001B[39m decoder = LineDecoder()\n\u001B[32m   1030\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m request_context(request=\u001B[38;5;28mself\u001B[39m._request):\n\u001B[32m-> \u001B[39m\u001B[32m1031\u001B[39m     \u001B[38;5;28;01masync\u001B[39;00m \u001B[38;5;28;01mfor\u001B[39;00m text \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m.aiter_text():\n\u001B[32m   1032\u001B[39m         \u001B[38;5;28;01mfor\u001B[39;00m line \u001B[38;5;129;01min\u001B[39;00m decoder.decode(text):\n\u001B[32m   1033\u001B[39m             \u001B[38;5;28;01myield\u001B[39;00m line\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/FastAPIProject/.venv/lib/python3.11/site-packages/httpx/_models.py:1018\u001B[39m, in \u001B[36mResponse.aiter_text\u001B[39m\u001B[34m(self, chunk_size)\u001B[39m\n\u001B[32m   1016\u001B[39m chunker = TextChunker(chunk_size=chunk_size)\n\u001B[32m   1017\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m request_context(request=\u001B[38;5;28mself\u001B[39m._request):\n\u001B[32m-> \u001B[39m\u001B[32m1018\u001B[39m     \u001B[38;5;28;01masync\u001B[39;00m \u001B[38;5;28;01mfor\u001B[39;00m byte_content \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m.aiter_bytes():\n\u001B[32m   1019\u001B[39m         text_content = decoder.decode(byte_content)\n\u001B[32m   1020\u001B[39m         \u001B[38;5;28;01mfor\u001B[39;00m chunk \u001B[38;5;129;01min\u001B[39;00m chunker.decode(text_content):\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/FastAPIProject/.venv/lib/python3.11/site-packages/httpx/_models.py:997\u001B[39m, in \u001B[36mResponse.aiter_bytes\u001B[39m\u001B[34m(self, chunk_size)\u001B[39m\n\u001B[32m    995\u001B[39m chunker = ByteChunker(chunk_size=chunk_size)\n\u001B[32m    996\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m request_context(request=\u001B[38;5;28mself\u001B[39m._request):\n\u001B[32m--> \u001B[39m\u001B[32m997\u001B[39m     \u001B[38;5;28;01masync\u001B[39;00m \u001B[38;5;28;01mfor\u001B[39;00m raw_bytes \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m.aiter_raw():\n\u001B[32m    998\u001B[39m         decoded = decoder.decode(raw_bytes)\n\u001B[32m    999\u001B[39m         \u001B[38;5;28;01mfor\u001B[39;00m chunk \u001B[38;5;129;01min\u001B[39;00m chunker.decode(decoded):\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/FastAPIProject/.venv/lib/python3.11/site-packages/httpx/_models.py:1055\u001B[39m, in \u001B[36mResponse.aiter_raw\u001B[39m\u001B[34m(self, chunk_size)\u001B[39m\n\u001B[32m   1052\u001B[39m chunker = ByteChunker(chunk_size=chunk_size)\n\u001B[32m   1054\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m request_context(request=\u001B[38;5;28mself\u001B[39m._request):\n\u001B[32m-> \u001B[39m\u001B[32m1055\u001B[39m     \u001B[38;5;28;01masync\u001B[39;00m \u001B[38;5;28;01mfor\u001B[39;00m raw_stream_bytes \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m.stream:\n\u001B[32m   1056\u001B[39m         \u001B[38;5;28mself\u001B[39m._num_bytes_downloaded += \u001B[38;5;28mlen\u001B[39m(raw_stream_bytes)\n\u001B[32m   1057\u001B[39m         \u001B[38;5;28;01mfor\u001B[39;00m chunk \u001B[38;5;129;01min\u001B[39;00m chunker.decode(raw_stream_bytes):\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/FastAPIProject/.venv/lib/python3.11/site-packages/httpx/_client.py:176\u001B[39m, in \u001B[36mBoundAsyncStream.__aiter__\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    175\u001B[39m \u001B[38;5;28;01masync\u001B[39;00m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m__aiter__\u001B[39m(\u001B[38;5;28mself\u001B[39m) -> typing.AsyncIterator[\u001B[38;5;28mbytes\u001B[39m]:\n\u001B[32m--> \u001B[39m\u001B[32m176\u001B[39m     \u001B[38;5;28;01masync\u001B[39;00m \u001B[38;5;28;01mfor\u001B[39;00m chunk \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m._stream:\n\u001B[32m    177\u001B[39m         \u001B[38;5;28;01myield\u001B[39;00m chunk\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/FastAPIProject/.venv/lib/python3.11/site-packages/httpx/_transports/default.py:271\u001B[39m, in \u001B[36mAsyncResponseStream.__aiter__\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    269\u001B[39m \u001B[38;5;28;01masync\u001B[39;00m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m__aiter__\u001B[39m(\u001B[38;5;28mself\u001B[39m) -> typing.AsyncIterator[\u001B[38;5;28mbytes\u001B[39m]:\n\u001B[32m    270\u001B[39m     \u001B[38;5;28;01mwith\u001B[39;00m map_httpcore_exceptions():\n\u001B[32m--> \u001B[39m\u001B[32m271\u001B[39m         \u001B[38;5;28;01masync\u001B[39;00m \u001B[38;5;28;01mfor\u001B[39;00m part \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m._httpcore_stream:\n\u001B[32m    272\u001B[39m             \u001B[38;5;28;01myield\u001B[39;00m part\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/FastAPIProject/.venv/lib/python3.11/site-packages/httpcore/_async/connection_pool.py:407\u001B[39m, in \u001B[36mPoolByteStream.__aiter__\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    405\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m exc:\n\u001B[32m    406\u001B[39m     \u001B[38;5;28;01mawait\u001B[39;00m \u001B[38;5;28mself\u001B[39m.aclose()\n\u001B[32m--> \u001B[39m\u001B[32m407\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m exc \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/FastAPIProject/.venv/lib/python3.11/site-packages/httpcore/_async/connection_pool.py:403\u001B[39m, in \u001B[36mPoolByteStream.__aiter__\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    401\u001B[39m \u001B[38;5;28;01masync\u001B[39;00m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m__aiter__\u001B[39m(\u001B[38;5;28mself\u001B[39m) -> typing.AsyncIterator[\u001B[38;5;28mbytes\u001B[39m]:\n\u001B[32m    402\u001B[39m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m403\u001B[39m         \u001B[38;5;28;01masync\u001B[39;00m \u001B[38;5;28;01mfor\u001B[39;00m part \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m._stream:\n\u001B[32m    404\u001B[39m             \u001B[38;5;28;01myield\u001B[39;00m part\n\u001B[32m    405\u001B[39m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m exc:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/FastAPIProject/.venv/lib/python3.11/site-packages/httpcore/_async/http11.py:342\u001B[39m, in \u001B[36mHTTP11ConnectionByteStream.__aiter__\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    340\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m AsyncShieldCancellation():\n\u001B[32m    341\u001B[39m     \u001B[38;5;28;01mawait\u001B[39;00m \u001B[38;5;28mself\u001B[39m.aclose()\n\u001B[32m--> \u001B[39m\u001B[32m342\u001B[39m \u001B[38;5;28;01mraise\u001B[39;00m exc\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/FastAPIProject/.venv/lib/python3.11/site-packages/httpcore/_async/http11.py:334\u001B[39m, in \u001B[36mHTTP11ConnectionByteStream.__aiter__\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    332\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m    333\u001B[39m     \u001B[38;5;28;01masync\u001B[39;00m \u001B[38;5;28;01mwith\u001B[39;00m Trace(\u001B[33m\"\u001B[39m\u001B[33mreceive_response_body\u001B[39m\u001B[33m\"\u001B[39m, logger, \u001B[38;5;28mself\u001B[39m._request, kwargs):\n\u001B[32m--> \u001B[39m\u001B[32m334\u001B[39m         \u001B[38;5;28;01masync\u001B[39;00m \u001B[38;5;28;01mfor\u001B[39;00m chunk \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m._connection._receive_response_body(**kwargs):\n\u001B[32m    335\u001B[39m             \u001B[38;5;28;01myield\u001B[39;00m chunk\n\u001B[32m    336\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m exc:\n\u001B[32m    337\u001B[39m     \u001B[38;5;66;03m# If we get an exception while streaming the response,\u001B[39;00m\n\u001B[32m    338\u001B[39m     \u001B[38;5;66;03m# we want to close the response (and possibly the connection)\u001B[39;00m\n\u001B[32m    339\u001B[39m     \u001B[38;5;66;03m# before raising that exception.\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/FastAPIProject/.venv/lib/python3.11/site-packages/httpcore/_async/http11.py:203\u001B[39m, in \u001B[36mAsyncHTTP11Connection._receive_response_body\u001B[39m\u001B[34m(self, request)\u001B[39m\n\u001B[32m    200\u001B[39m timeout = timeouts.get(\u001B[33m\"\u001B[39m\u001B[33mread\u001B[39m\u001B[33m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[32m    202\u001B[39m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m203\u001B[39m     event = \u001B[38;5;28;01mawait\u001B[39;00m \u001B[38;5;28mself\u001B[39m._receive_event(timeout=timeout)\n\u001B[32m    204\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(event, h11.Data):\n\u001B[32m    205\u001B[39m         \u001B[38;5;28;01myield\u001B[39;00m \u001B[38;5;28mbytes\u001B[39m(event.data)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/FastAPIProject/.venv/lib/python3.11/site-packages/httpcore/_async/http11.py:217\u001B[39m, in \u001B[36mAsyncHTTP11Connection._receive_event\u001B[39m\u001B[34m(self, timeout)\u001B[39m\n\u001B[32m    214\u001B[39m     event = \u001B[38;5;28mself\u001B[39m._h11_state.next_event()\n\u001B[32m    216\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m event \u001B[38;5;129;01mis\u001B[39;00m h11.NEED_DATA:\n\u001B[32m--> \u001B[39m\u001B[32m217\u001B[39m     data = \u001B[38;5;28;01mawait\u001B[39;00m \u001B[38;5;28mself\u001B[39m._network_stream.read(\n\u001B[32m    218\u001B[39m         \u001B[38;5;28mself\u001B[39m.READ_NUM_BYTES, timeout=timeout\n\u001B[32m    219\u001B[39m     )\n\u001B[32m    221\u001B[39m     \u001B[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001B[39;00m\n\u001B[32m    222\u001B[39m     \u001B[38;5;66;03m#\u001B[39;00m\n\u001B[32m    223\u001B[39m     \u001B[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m    227\u001B[39m     \u001B[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001B[39;00m\n\u001B[32m    228\u001B[39m     \u001B[38;5;66;03m# it as a ConnectError.\u001B[39;00m\n\u001B[32m    229\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m data == \u001B[33mb\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m\"\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m._h11_state.their_state == h11.SEND_RESPONSE:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/FastAPIProject/.venv/lib/python3.11/site-packages/httpcore/_backends/anyio.py:35\u001B[39m, in \u001B[36mAnyIOStream.read\u001B[39m\u001B[34m(self, max_bytes, timeout)\u001B[39m\n\u001B[32m     33\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m anyio.fail_after(timeout):\n\u001B[32m     34\u001B[39m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m---> \u001B[39m\u001B[32m35\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mawait\u001B[39;00m \u001B[38;5;28mself\u001B[39m._stream.receive(max_bytes=max_bytes)\n\u001B[32m     36\u001B[39m     \u001B[38;5;28;01mexcept\u001B[39;00m anyio.EndOfStream:  \u001B[38;5;66;03m# pragma: nocover\u001B[39;00m\n\u001B[32m     37\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[33mb\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m\"\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/FastAPIProject/.venv/lib/python3.11/site-packages/anyio/_backends/_asyncio.py:1254\u001B[39m, in \u001B[36mSocketStream.receive\u001B[39m\u001B[34m(self, max_bytes)\u001B[39m\n\u001B[32m   1248\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[32m   1249\u001B[39m     \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m._protocol.read_event.is_set()\n\u001B[32m   1250\u001B[39m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m._transport.is_closing()\n\u001B[32m   1251\u001B[39m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m._protocol.is_at_eof\n\u001B[32m   1252\u001B[39m ):\n\u001B[32m   1253\u001B[39m     \u001B[38;5;28mself\u001B[39m._transport.resume_reading()\n\u001B[32m-> \u001B[39m\u001B[32m1254\u001B[39m     \u001B[38;5;28;01mawait\u001B[39;00m \u001B[38;5;28mself\u001B[39m._protocol.read_event.wait()\n\u001B[32m   1255\u001B[39m     \u001B[38;5;28mself\u001B[39m._transport.pause_reading()\n\u001B[32m   1256\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/usr/local/Cellar/python@3.11/3.11.12_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/locks.py:213\u001B[39m, in \u001B[36mEvent.wait\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    211\u001B[39m \u001B[38;5;28mself\u001B[39m._waiters.append(fut)\n\u001B[32m    212\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m213\u001B[39m     \u001B[38;5;28;01mawait\u001B[39;00m fut\n\u001B[32m    214\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[32m    215\u001B[39m \u001B[38;5;28;01mfinally\u001B[39;00m:\n",
      "\u001B[31mCancelledError\u001B[39m: "
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 9. å¤æ‚é“¾å¼å¤„ç†",
   "id": "15308c4ec31d49db"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-22T07:05:49.053221Z",
     "start_time": "2025-07-22T07:05:07.615328Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# 9. å¤æ‚é“¾å¼å¤„ç†\n",
    "def complex_chain_example():\n",
    "    \"\"\"å¤æ‚é“¾å¼å¤„ç†ç¤ºä¾‹\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"9. å¤æ‚é“¾å¼å¤„ç†ç¤ºä¾‹\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    llm = OllamaLLM(\n",
    "        base_url=\"http://localhost:11434\",\n",
    "        model=\"qwen2.5:3b\"\n",
    "    )\n",
    "\n",
    "    # åˆ›å»ºå¤šæ­¥éª¤å¤„ç†é“¾\n",
    "    step1_template = PromptTemplate(\n",
    "        input_variables=[\"topic\"],\n",
    "        template=\"è¯·åˆ—å‡ºå…³äºŽ{topic}çš„5ä¸ªå…³é”®ç‚¹ï¼š\"\n",
    "    )\n",
    "\n",
    "    step2_template = PromptTemplate(\n",
    "        input_variables=[\"key_points\"],\n",
    "        template=\"åŸºäºŽä»¥ä¸‹å…³é”®ç‚¹ï¼Œå†™ä¸€ä¸ªç®€çŸ­çš„æ€»ç»“ï¼š\\n{key_points}\\n\\næ€»ç»“ï¼š\"\n",
    "    )\n",
    "\n",
    "    # æž„å»ºé“¾\n",
    "    def process_topic(topic: str) -> str:\n",
    "        # ç¬¬ä¸€æ­¥ï¼šç”Ÿæˆå…³é”®ç‚¹\n",
    "        key_points = (step1_template | llm).invoke({\"topic\": topic})\n",
    "\n",
    "        # ç¬¬äºŒæ­¥ï¼šç”Ÿæˆæ€»ç»“\n",
    "        summary = (step2_template | llm).invoke({\"key_points\": key_points})\n",
    "\n",
    "        return {\n",
    "            \"topic\": topic,\n",
    "            \"key_points\": key_points,\n",
    "            \"summary\": summary\n",
    "        }\n",
    "\n",
    "    # æµ‹è¯•\n",
    "    result = process_topic(\"äººå·¥æ™ºèƒ½\")\n",
    "    print(f\"ä¸»é¢˜: {result['topic']}\")\n",
    "    print(f\"å…³é”®ç‚¹: {result['key_points'][:200]}...\")\n",
    "    print(f\"æ€»ç»“: {result['summary'][:200]}...\")\n",
    "complex_chain_example()"
   ],
   "id": "4c48d3e63f1250c6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "9. å¤æ‚é“¾å¼å¤„ç†ç¤ºä¾‹\n",
      "==================================================\n",
      "ä¸»é¢˜: äººå·¥æ™ºèƒ½\n",
      "å…³é”®ç‚¹: å½“ç„¶ï¼Œå…³äºŽäººå·¥æ™ºèƒ½ï¼ˆAIï¼‰çš„äº”ä¸ªå…³é”®ç‚¹å¦‚ä¸‹ï¼š\n",
      "\n",
      "1. **æ™ºèƒ½è‡ªåŠ¨åŒ–**ï¼šäººå·¥æ™ºèƒ½æŠ€æœ¯è®©æœºå™¨èƒ½å¤Ÿæ¨¡æ‹Ÿäººç±»çš„è®¤çŸ¥è¿‡ç¨‹ï¼Œé€šè¿‡å­¦ä¹ ã€æŽ¨ç†å’Œè‡ªæˆ‘ä¿®æ­£æ¥å®Œæˆå¤æ‚çš„ä»»åŠ¡ã€‚è¿™ä¸ä»…åŒ…æ‹¬è¯­éŸ³è¯†åˆ«ã€å›¾åƒå¤„ç†ç­‰æ„ŸçŸ¥åŠŸèƒ½ï¼Œè¿˜æ¶‰åŠè‡ªç„¶è¯­è¨€ç†è§£ä¸Žç”Ÿæˆã€‚\n",
      "\n",
      "2. **æ•°æ®é©±åŠ¨**ï¼šäººå·¥æ™ºèƒ½çš„å‘å±•é«˜åº¦ä¾èµ–äºŽå¤§æ•°æ®çš„æ”¯æŒï¼Œå¤§é‡çš„æ•°æ®ç”¨äºŽè®­ç»ƒæ¨¡åž‹å¹¶ä¸æ–­æé«˜ç®—æ³•çš„å‡†ç¡®æ€§å’Œæ•ˆçŽ‡ã€‚ä»Žç¤¾äº¤åª’ä½“ä¿¡æ¯åˆ°åŒ»ç–—è®°å½•ã€ç”µå­å•†åŠ¡äº¤æ˜“ç­‰å„ç§é¢†åŸŸçš„æ•°æ®éƒ½èƒ½...\n",
      "æ€»ç»“: ä»¥ä¸‹æ˜¯ä¸€ä¸ªåŸºäºŽæ‚¨æä¾›çš„äº”ä¸ªå…³é”®ç‚¹çš„ç®€çŸ­æ€»ç»“ï¼š\n",
      "\n",
      "äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰æ­£é€šè¿‡æ™ºèƒ½è‡ªåŠ¨åŒ–ã€æ•°æ®é©±åŠ¨å’Œæœºå™¨å­¦ä¹ ç­‰æ ¸å¿ƒæŠ€æœ¯æ”¹å˜æˆ‘ä»¬çš„ç”Ÿæ´»å’Œå·¥ä½œæ–¹å¼ã€‚å®ƒä¾èµ–äºŽå¤§æ•°æ®çš„æ”¯æŒï¼Œå¹¶åœ¨ä¼¦ç†ä¸Žéšç§é—®é¢˜ä»¥åŠå¯¹ç¤¾ä¼šå½±å“æ–¹é¢å¸¦æ¥äº†ä¸€ç³»åˆ—æŒ‘æˆ˜ã€‚AIä¸ä»…é‡å¡‘äº†å„ä¸ªè¡Œä¸šçš„è¿ä½œæ¨¡å¼ï¼Œè¿˜æŽ¨åŠ¨æ•™è‚²ä½“ç³»å˜é©ä»¥åŸ¹å…»é€‚åº”æ–°æ—¶ä»£çš„æŠ€èƒ½äººæ‰ã€‚æ­¤å¤–ï¼Œè·¨å­¦ç§‘åˆä½œæ˜¯æŽ¨åŠ¨äººå·¥æ™ºèƒ½å‘å±•çš„å…³é”®å› ç´ ã€‚\n",
      "\n",
      "è¿™äº›æ ¸å¿ƒè¦ç‚¹æœ‰åŠ©äºŽæˆ‘ä»¬å…¨é¢ç†è§£AIæŠ€æœ¯åŠå…¶æœªæ¥çš„å‘å±•è¶‹åŠ¿å’Œ...\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 10. å¹¶è¡Œå¤„ç†ç¤ºä¾‹",
   "id": "ce878f26c6ca5807"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "# 10. å¹¶è¡Œå¤„ç†ç¤ºä¾‹\n",
    "def parallel_processing():\n",
    "    \"\"\"å¹¶è¡Œå¤„ç†ç¤ºä¾‹\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"10. å¹¶è¡Œå¤„ç†ç¤ºä¾‹\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    llm = Ollama(\n",
    "        base_url=\"http://localhost:11434\",\n",
    "        model=\"qwen2.5:3b\"\n",
    "    )\n",
    "\n",
    "    # åˆ›å»ºå¹¶è¡Œå¤„ç†é“¾\n",
    "    parallel_chain = RunnableParallel({\n",
    "        \"definition\": PromptTemplate.from_template(\"å®šä¹‰ï¼š{concept}æ˜¯ä»€ä¹ˆï¼Ÿ\") | llm,\n",
    "        \"advantages\": PromptTemplate.from_template(\"ä¼˜åŠ¿ï¼š{concept}æœ‰ä»€ä¹ˆä¼˜åŠ¿ï¼Ÿ\") | llm,\n",
    "        \"applications\": PromptTemplate.from_template(\"åº”ç”¨ï¼š{concept}æœ‰å“ªäº›åº”ç”¨åœºæ™¯ï¼Ÿ\") | llm,\n",
    "        \"challenges\": PromptTemplate.from_template(\"æŒ‘æˆ˜ï¼š{concept}é¢ä¸´å“ªäº›æŒ‘æˆ˜ï¼Ÿ\") | llm\n",
    "    })\n",
    "\n",
    "    # æ‰§è¡Œå¹¶è¡Œå¤„ç†\n",
    "    concept = \"åŒºå—é“¾æŠ€æœ¯\"\n",
    "    start_time = time.time()\n",
    "    results = parallel_chain.invoke({\"concept\": concept})\n",
    "    parallel_time = time.time() - start_time\n",
    "\n",
    "    print(f\"å¹¶è¡Œå¤„ç†è€—æ—¶: {parallel_time:.2f}ç§’\")\n",
    "    print(f\"\\næ¦‚å¿µ: {concept}\")\n",
    "\n",
    "    for aspect, content in results.items():\n",
    "        print(f\"\\n{aspect}: {content[:100]}...\")"
   ],
   "id": "8b6eaba3722f0432",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 11. é”™è¯¯å¤„ç†å’Œé‡è¯•",
   "id": "2fecbb9524b6e1ba"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "# 11. é”™è¯¯å¤„ç†å’Œé‡è¯•\n",
    "def error_handling_example():\n",
    "    \"\"\"é”™è¯¯å¤„ç†ç¤ºä¾‹\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"11. é”™è¯¯å¤„ç†ç¤ºä¾‹\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    # æ­£å¸¸æ¨¡åž‹\n",
    "    llm = Ollama(\n",
    "        base_url=\"http://localhost:11434\",\n",
    "        model=\"qwen2.5:3b\"\n",
    "    )\n",
    "\n",
    "    # é”™è¯¯æ¨¡åž‹ï¼ˆä¸å­˜åœ¨ï¼‰\n",
    "    error_llm = Ollama(\n",
    "        base_url=\"http://localhost:11434\",\n",
    "        model=\"nonexistent-model\"\n",
    "    )\n",
    "\n",
    "    def safe_invoke(llm, prompt: str, max_retries: int = 3):\n",
    "        \"\"\"å®‰å…¨è°ƒç”¨å‡½æ•°\"\"\"\n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                response = llm.invoke(prompt)\n",
    "                return response\n",
    "            except Exception as e:\n",
    "                print(f\"å°è¯• {attempt + 1} å¤±è´¥: {e}\")\n",
    "                if attempt == max_retries - 1:\n",
    "                    return f\"æ‰€æœ‰å°è¯•éƒ½å¤±è´¥äº†ï¼Œæœ€åŽé”™è¯¯: {e}\"\n",
    "                time.sleep(1)  # ç­‰å¾…é‡è¯•\n",
    "\n",
    "    # æµ‹è¯•æ­£å¸¸è°ƒç”¨\n",
    "    response = safe_invoke(llm, \"æµ‹è¯•æ­£å¸¸è°ƒç”¨\")\n",
    "    print(f\"æ­£å¸¸è°ƒç”¨ç»“æžœ: {response[:50]}...\")\n",
    "\n",
    "    # æµ‹è¯•é”™è¯¯å¤„ç†\n",
    "    error_response = safe_invoke(error_llm, \"æµ‹è¯•é”™è¯¯å¤„ç†\")\n",
    "    print(f\"é”™è¯¯å¤„ç†ç»“æžœ: {error_response}\")"
   ],
   "id": "a549054828385b10",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 12. è‡ªå®šä¹‰è¾“å‡ºæ ¼å¼åŒ–",
   "id": "f486c5f8b1664db7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "# 12. è‡ªå®šä¹‰è¾“å‡ºæ ¼å¼åŒ–\n",
    "def custom_output_formatting():\n",
    "    \"\"\"è‡ªå®šä¹‰è¾“å‡ºæ ¼å¼åŒ–\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"12. è‡ªå®šä¹‰è¾“å‡ºæ ¼å¼åŒ–\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    llm = Ollama(\n",
    "        base_url=\"http://localhost:11434\",\n",
    "        model=\"qwen2.5:3b\"\n",
    "    )\n",
    "\n",
    "    # è‡ªå®šä¹‰æ ¼å¼åŒ–å™¨\n",
    "    class JSONFormatter:\n",
    "        def format(self, text: str, metadata: Dict[str, Any] = None) -> Dict[str, Any]:\n",
    "            return {\n",
    "                \"content\": text.strip(),\n",
    "                \"length\": len(text),\n",
    "                \"word_count\": len(text.split()),\n",
    "                \"metadata\": metadata or {}\n",
    "            }\n",
    "\n",
    "    formatter = JSONFormatter()\n",
    "\n",
    "    # åˆ›å»ºæ ¼å¼åŒ–é“¾\n",
    "    def formatted_invoke(prompt: str) -> Dict[str, Any]:\n",
    "        response = llm.invoke(prompt)\n",
    "        return formatter.format(response, {\"prompt\": prompt})\n",
    "\n",
    "    # æµ‹è¯•\n",
    "    result = formatted_invoke(\"ä»€ä¹ˆæ˜¯å¾®æœåŠ¡æž¶æž„ï¼Ÿ\")\n",
    "    print(f\"æ ¼å¼åŒ–ç»“æžœ:\")\n",
    "    for key, value in result.items():\n",
    "        if key == \"content\":\n",
    "            print(f\"{key}: {str(value)[:100]}...\")\n",
    "        else:\n",
    "            print(f\"{key}: {value}\")"
   ],
   "id": "c2737976566ca7b3",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## ä¸åŒæ¨¡åž‹æä¾›å•†ç¤ºä¾‹",
   "id": "269c9bfb3dac107d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "## ä¸åŒæ¨¡åž‹æä¾›å•†ç¤ºä¾‹\n",
    "\n",
    "\"\"\"\n",
    "ä¸åŒ LLM æä¾›å•†ç¤ºä¾‹\n",
    "\"\"\"\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain_openai import OpenAI\n",
    "from langchain_community.llms import Bedrock\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "def ollama_example():\n",
    "    \"\"\"Ollama æœ¬åœ°æ¨¡åž‹ç¤ºä¾‹\"\"\"\n",
    "    print(\"=\" * 30)\n",
    "    print(\"Ollama ç¤ºä¾‹\")\n",
    "    print(\"=\" * 30)\n",
    "\n",
    "    llm = Ollama(\n",
    "        base_url=\"http://localhost:11434\",\n",
    "        model=\"qwen2.5:3b\",\n",
    "        temperature=0.7\n",
    "    )\n",
    "\n",
    "    response = llm.invoke(\"è§£é‡Šä»€ä¹ˆæ˜¯å®¹å™¨åŒ–æŠ€æœ¯\")\n",
    "    print(f\"Ollama å“åº”: {response[:100]}...\")\n",
    "\n",
    "def openai_example():\n",
    "    \"\"\"OpenAI ç¤ºä¾‹\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 30)\n",
    "    print(\"OpenAI ç¤ºä¾‹\")\n",
    "    print(\"=\" * 30)\n",
    "\n",
    "    try:\n",
    "        llm = OpenAI(\n",
    "            model=\"gpt-3.5-turbo-instruct\",\n",
    "            temperature=0.7,\n",
    "            max_tokens=100\n",
    "        )\n",
    "\n",
    "        response = llm.invoke(\"è§£é‡Šä»€ä¹ˆæ˜¯å®¹å™¨åŒ–æŠ€æœ¯\")\n",
    "        print(f\"OpenAI å“åº”: {response}\")\n",
    "    except Exception as e:\n",
    "        print(f\"OpenAI è°ƒç”¨å¤±è´¥: {e}\")\n",
    "\n",
    "def bedrock_example():\n",
    "    \"\"\"AWS Bedrock ç¤ºä¾‹\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 30)\n",
    "    print(\"AWS Bedrock ç¤ºä¾‹\")\n",
    "    print(\"=\" * 30)\n",
    "\n",
    "    try:\n",
    "        llm = Bedrock(\n",
    "            model_id=\"anthropic.claude-v2\",\n",
    "            region_name=\"us-east-1\"\n",
    "        )\n",
    "\n",
    "        # Claude éœ€è¦ç‰¹å®šæ ¼å¼\n",
    "        prompt = \"Human: è§£é‡Šä»€ä¹ˆæ˜¯å®¹å™¨åŒ–æŠ€æœ¯\\n\\nAssistant:\"\n",
    "        response = llm.invoke(prompt)\n",
    "        print(f\"Bedrock å“åº”: {response[:100]}...\")\n",
    "    except Exception as e:\n",
    "        print(f\"Bedrock è°ƒç”¨å¤±è´¥: {e}\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"è¿è¡Œæ‰€æœ‰æä¾›å•†ç¤ºä¾‹\"\"\"\n",
    "    ollama_example()\n",
    "    openai_example()\n",
    "    bedrock_example()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ],
   "id": "e937e9711b3a73e6",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## é«˜çº§ç”¨æ³•ç¤ºä¾‹",
   "id": "229427b6e37bc956"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "\"\"\"\n",
    "LLM é«˜çº§ç”¨æ³•ç¤ºä¾‹\n",
    "\"\"\"\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain_core.prompts import PromptTemplate, FewShotPromptTemplate\n",
    "from langchain_core.example_selectors import SemanticSimilarityExampleSelector\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_core.runnables import RunnableBranch, RunnableLambda\n",
    "from typing import Dict, Any"
   ],
   "id": "a69c6abe65fc64ef",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### å°‘æ ·æœ¬å­¦ä¹ ç¤ºä¾‹",
   "id": "8bbe86310b282793"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "def few_shot_learning():\n",
    "    \"\"\"å°‘æ ·æœ¬å­¦ä¹ ç¤ºä¾‹\"\"\"\n",
    "    print(\"=\" * 50)\n",
    "    print(\"å°‘æ ·æœ¬å­¦ä¹ ç¤ºä¾‹\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    llm = Ollama(\n",
    "        base_url=\"http://localhost:11434\",\n",
    "        model=\"qwen2.5:3b\"\n",
    "    )\n",
    "\n",
    "    # å®šä¹‰ç¤ºä¾‹\n",
    "    examples = [\n",
    "        {\n",
    "            \"question\": \"ä»€ä¹ˆæ˜¯Pythonï¼Ÿ\",\n",
    "            \"answer\": \"Pythonæ˜¯ä¸€ç§é«˜çº§ç¼–ç¨‹è¯­è¨€ï¼Œä»¥å…¶ç®€æ´çš„è¯­æ³•å’Œå¼ºå¤§çš„åŠŸèƒ½è€Œé—»åã€‚\"\n",
    "        },\n",
    "        {\n",
    "            \"question\": \"ä»€ä¹ˆæ˜¯JavaScriptï¼Ÿ\",\n",
    "            \"answer\": \"JavaScriptæ˜¯ä¸€ç§ä¸»è¦ç”¨äºŽç½‘é¡µå¼€å‘çš„ç¼–ç¨‹è¯­è¨€ï¼Œå¯ä»¥åˆ›å»ºäº¤äº’å¼ç½‘é¡µã€‚\"\n",
    "        },\n",
    "        {\n",
    "            \"question\": \"ä»€ä¹ˆæ˜¯SQLï¼Ÿ\",\n",
    "            \"answer\": \"SQLæ˜¯ç»“æž„åŒ–æŸ¥è¯¢è¯­è¨€ï¼Œç”¨äºŽç®¡ç†å’Œæ“ä½œå…³ç³»æ•°æ®åº“ä¸­çš„æ•°æ®ã€‚\"\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    # åˆ›å»ºç¤ºä¾‹æ¨¡æ¿\n",
    "    example_template = \"\"\"\n",
    "    é—®é¢˜: {question}\n",
    "    ç­”æ¡ˆ: {answer}\n",
    "    \"\"\"\n",
    "\n",
    "    example_prompt = PromptTemplate(\n",
    "        input_variables=[\"question\", \"answer\"],\n",
    "        template=example_template\n",
    "    )\n",
    "\n",
    "    # åˆ›å»ºå°‘æ ·æœ¬æç¤ºæ¨¡æ¿\n",
    "    few_shot_prompt = FewShotPromptTemplate(\n",
    "        examples=examples,\n",
    "        example_prompt=example_prompt,\n",
    "        prefix=\"ä»¥ä¸‹æ˜¯ä¸€äº›é—®ç­”ç¤ºä¾‹ï¼š\",\n",
    "        suffix=\"é—®é¢˜: {input}\\nç­”æ¡ˆ:\",\n",
    "        input_variables=[\"input\"]\n",
    "    )\n",
    "\n",
    "    # åˆ›å»ºé“¾\n",
    "    chain = few_shot_prompt | llm\n",
    "\n",
    "    # æµ‹è¯•\n",
    "    response = chain.invoke({\"input\": \"ä»€ä¹ˆæ˜¯Javaï¼Ÿ\"})\n",
    "    print(f\"å°‘æ ·æœ¬å­¦ä¹ å›žç­”: {response}\")"
   ],
   "id": "7559271211206b3b",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### æ¡ä»¶è·¯ç”±ç¤ºä¾‹",
   "id": "595d910b5b8f533a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "def conditional_routing():\n",
    "    \"\"\"æ¡ä»¶è·¯ç”±ç¤ºä¾‹\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"æ¡ä»¶è·¯ç”±ç¤ºä¾‹\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    llm = Ollama(\n",
    "        base_url=\"http://localhost:11434\",\n",
    "        model=\"qwen2.5:3b\"\n",
    "    )\n",
    "\n",
    "    # å®šä¹‰ä¸åŒç±»åž‹çš„æç¤ºæ¨¡æ¿\n",
    "    tech_template = PromptTemplate(\n",
    "        input_variables=[\"question\"],\n",
    "        template=\"ä½œä¸ºæŠ€æœ¯ä¸“å®¶ï¼Œè¯·è¯¦ç»†å›žç­”ï¼š{question}\"\n",
    "    )\n",
    "\n",
    "    general_template = PromptTemplate(\n",
    "        input_variables=[\"question\"],\n",
    "        template=\"è¯·ç®€å•å›žç­”ï¼š{question}\"\n",
    "    )\n",
    "\n",
    "    creative_template = PromptTemplate(\n",
    "        input_variables=[\"question\"],\n",
    "        template=\"è¯·ç”¨åˆ›æ„çš„æ–¹å¼å›žç­”ï¼š{question}\"\n",
    "    )\n",
    "\n",
    "    # è·¯ç”±å‡½æ•°\n",
    "    def route_question(input_dict: Dict[str, Any]) -> str:\n",
    "        question = input_dict[\"question\"].lower()\n",
    "        if any(word in question for word in [\"ç¼–ç¨‹\", \"æŠ€æœ¯\", \"ç®—æ³•\", \"ä»£ç \"]):\n",
    "            return \"tech\"\n",
    "        elif any(word in question for word in [\"åˆ›æ„\", \"æ•…äº‹\", \"è¯—æ­Œ\", \"æƒ³è±¡\"]):\n",
    "            return \"creative\"\n",
    "        else:\n",
    "            return \"general\"\n",
    "\n",
    "    # åˆ›å»ºæ¡ä»¶åˆ†æ”¯\n",
    "    routing_chain = RunnableBranch(\n",
    "        (lambda x: route_question(x) == \"tech\", tech_template | llm),\n",
    "        (lambda x: route_question(x) == \"creative\", creative_template | llm),\n",
    "        general_template | llm  # é»˜è®¤åˆ†æ”¯\n",
    "    )\n",
    "\n",
    "    # æµ‹è¯•ä¸åŒç±»åž‹çš„é—®é¢˜\n",
    "    questions = [\n",
    "        \"ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ç®—æ³•ï¼Ÿ\",\n",
    "        \"å†™ä¸€ä¸ªå…³äºŽæœˆäº®çš„è¯—\",\n",
    "        \"ä»Šå¤©å¤©æ°”æ€Žä¹ˆæ ·ï¼Ÿ\"\n",
    "    ]\n",
    "\n",
    "    for question in questions:\n",
    "        route_type = route_question({\"question\": question})\n",
    "        response = routing_chain.invoke({\"question\": question})\n",
    "        print(f\"\\né—®é¢˜ç±»åž‹: {route_type}\")\n",
    "        print(f\"é—®é¢˜: {question}\")\n",
    "        print(f\"å›žç­”: {response[:100]}...\")"
   ],
   "id": "943709ae17206e6a",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### è®°å¿†æ¨¡æ‹Ÿç¤ºä¾‹",
   "id": "49f8657aea544a04"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "def memory_simulation():\n",
    "    \"\"\"è®°å¿†æ¨¡æ‹Ÿç¤ºä¾‹\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"è®°å¿†æ¨¡æ‹Ÿç¤ºä¾‹\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    llm = Ollama(\n",
    "        base_url=\"http://localhost:11434\",\n",
    "        model=\"qwen2.5:3b\"\n",
    "    )\n",
    "\n",
    "    # ç®€å•çš„è®°å¿†å­˜å‚¨\n",
    "    conversation_memory = []\n",
    "\n",
    "    def chat_with_memory(user_input: str) -> str:\n",
    "        # æž„å»ºåŒ…å«åŽ†å²çš„æç¤º\n",
    "        context = \"\"\n",
    "        if conversation_memory:\n",
    "            context = \"å¯¹è¯åŽ†å²ï¼š\\n\"\n",
    "            for i, (q, a) in enumerate(conversation_memory[-3:]):  # åªä¿ç•™æœ€è¿‘3è½®\n",
    "                context += f\"ç”¨æˆ·: {q}\\nAI: {a}\\n\"\n",
    "            context += \"\\n\"\n",
    "\n",
    "        prompt = f\"{context}å½“å‰é—®é¢˜: {user_input}\\nè¯·å›žç­”:\"\n",
    "\n",
    "        # ç”Ÿæˆå›žç­”\n",
    "        response = llm.invoke(prompt)\n",
    "\n",
    "        # ä¿å­˜åˆ°è®°å¿†\n",
    "        conversation_memory.append((user_input, response))\n",
    "\n",
    "        return response\n",
    "\n",
    "    # æ¨¡æ‹Ÿå¯¹è¯\n",
    "    conversation = [\n",
    "        \"æˆ‘æ˜¯ä¸€åè½¯ä»¶å·¥ç¨‹å¸ˆ\",\n",
    "        \"æˆ‘ä¸»è¦ä½¿ç”¨Pythonå¼€å‘\",\n",
    "        \"ä½ çŸ¥é“æˆ‘çš„èŒä¸šæ˜¯ä»€ä¹ˆå—ï¼Ÿ\",\n",
    "        \"æˆ‘ä½¿ç”¨ä»€ä¹ˆç¼–ç¨‹è¯­è¨€ï¼Ÿ\"\n",
    "    ]\n",
    "\n",
    "    for user_msg in conversation:\n",
    "        ai_response = chat_with_memory(user_msg)\n",
    "        print(f\"\\nç”¨æˆ·: {user_msg}\")\n",
    "        print(f\"AI: {ai_response[:100]}...\")"
   ],
   "id": "19e80a4caab015bc",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "def main():\n",
    "    \"\"\"è¿è¡Œæ‰€æœ‰é«˜çº§ç¤ºä¾‹\"\"\"\n",
    "    few_shot_learning()\n",
    "    conditional_routing()\n",
    "    memory_simulation()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n"
   ],
   "id": "ee4816e8211319bb",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### æ€§èƒ½ä¼˜åŒ–ç¤ºä¾‹",
   "id": "1fe96b5e88057fa3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "\"\"\"\n",
    "LLM æ€§èƒ½ä¼˜åŒ–ç¤ºä¾‹\n",
    "\"\"\"\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain.cache import InMemoryCache, SQLiteCache\n",
    "from langchain.globals import set_llm_cache\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "import time\n",
    "import asyncio\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from typing import List"
   ],
   "id": "fed37d4f183c96e6",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### ç¼“å­˜æ€§èƒ½å¯¹æ¯”",
   "id": "df1b3aa81a8d1e4d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "def caching_comparison():\n",
    "    \"\"\"ç¼“å­˜æ€§èƒ½å¯¹æ¯”\"\"\"\n",
    "    print(\"=\" * 50)\n",
    "    print(\"ç¼“å­˜æ€§èƒ½å¯¹æ¯”\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    llm = Ollama(\n",
    "        base_url=\"http://localhost:11434\",\n",
    "        model=\"qwen2.5:3b\"\n",
    "    )\n",
    "\n",
    "    prompt = \"ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿ\"\n",
    "\n",
    "    # æ— ç¼“å­˜æµ‹è¯•\n",
    "    set_llm_cache(None)\n",
    "    start_time = time.time()\n",
    "    response1 = llm.invoke(prompt)\n",
    "    no_cache_time = time.time() - start_time\n",
    "\n",
    "    # å†…å­˜ç¼“å­˜æµ‹è¯•\n",
    "    set_llm_cache(InMemoryCache())\n",
    "    start_time = time.time()\n",
    "    response2 = llm.invoke(prompt)  # ç¬¬ä¸€æ¬¡è°ƒç”¨\n",
    "    first_cache_time = time.time() - start_time\n",
    "\n",
    "    start_time = time.time()\n",
    "    response3 = llm.invoke(prompt)  # ç¬¬äºŒæ¬¡è°ƒç”¨ï¼ˆå‘½ä¸­ç¼“å­˜ï¼‰\n",
    "    second_cache_time = time.time() - start_time\n",
    "\n",
    "    print(f\"æ— ç¼“å­˜è€—æ—¶: {no_cache_time:.2f}ç§’\")\n",
    "    print(f\"é¦–æ¬¡ç¼“å­˜è€—æ—¶: {first_cache_time:.2f}ç§’\")\n",
    "    print(f\"å‘½ä¸­ç¼“å­˜è€—æ—¶: {second_cache_time:.2f}ç§’\")\n",
    "    print(f\"ç¼“å­˜åŠ é€Ÿæ¯”: {no_cache_time/second_cache_time:.2f}x\")"
   ],
   "id": "644f4745215edfec"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### æ‰¹é‡å¤„ç† vs é¡ºåºå¤„ç†æ€§èƒ½å¯¹æ¯”",
   "id": "590d68531ffdba15"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "def batch_vs_sequential():\n",
    "    \"\"\"æ‰¹é‡å¤„ç† vs é¡ºåºå¤„ç†æ€§èƒ½å¯¹æ¯”\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"æ‰¹é‡å¤„ç† vs é¡ºåºå¤„ç†æ€§èƒ½å¯¹æ¯”\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    llm = Ollama(\n",
    "        base_url=\"http://localhost:11434\",\n",
    "        model=\"qwen2.5:3b\"\n",
    "    )\n",
    "\n",
    "    prompts = [\n",
    "        \"ä»€ä¹ˆæ˜¯äº‘è®¡ç®—ï¼Ÿ\",\n",
    "        \"ä»€ä¹ˆæ˜¯å¤§æ•°æ®ï¼Ÿ\",\n",
    "        \"ä»€ä¹ˆæ˜¯ç‰©è”ç½‘ï¼Ÿ\",\n",
    "        \"ä»€ä¹ˆæ˜¯5GæŠ€æœ¯ï¼Ÿ\",\n",
    "        \"ä»€ä¹ˆæ˜¯è¾¹ç¼˜è®¡ç®—ï¼Ÿ\"\n",
    "    ]\n",
    "\n",
    "    # é¡ºåºå¤„ç†\n",
    "    start_time = time.time()\n",
    "    sequential_responses = []\n",
    "    for prompt in prompts:\n",
    "        response = llm.invoke(prompt)\n",
    "        sequential_responses.append(response)\n",
    "    sequential_time = time.time() - start_time\n",
    "\n",
    "    # æ‰¹é‡å¤„ç†\n",
    "    start_time = time.time()\n",
    "    batch_responses = llm.batch(prompts)\n",
    "    batch_time = time.time() - start_time\n",
    "\n",
    "    print(f\"é¡ºåºå¤„ç†è€—æ—¶: {sequential_time:.2f}ç§’\")\n",
    "    print(f\"æ‰¹é‡å¤„ç†è€—æ—¶: {batch_time:.2f}ç§’\")\n",
    "    print(f\"æ‰¹é‡å¤„ç†åŠ é€Ÿæ¯”: {sequential_time/batch_time:.2f}x\")"
   ],
   "id": "39d4989bdce4ff35",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### å¼‚æ­¥ vs åŒæ­¥æ€§èƒ½å¯¹æ¯”",
   "id": "d492a91a90167aaa"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "async def async_vs_sync():\n",
    "    \"\"\"å¼‚æ­¥ vs åŒæ­¥æ€§èƒ½å¯¹æ¯”\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"å¼‚æ­¥ vs åŒæ­¥æ€§èƒ½å¯¹æ¯”\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    llm = Ollama(\n",
    "        base_url=\"http://localhost:11434\",\n",
    "        model=\"qwen2.5:3b\"\n",
    "    )\n",
    "\n",
    "    prompts = [\n",
    "        \"è§£é‡Šæœºå™¨å­¦ä¹ \",\n",
    "        \"è§£é‡Šæ·±åº¦å­¦ä¹ \",\n",
    "        \"è§£é‡Šç¥žç»ç½‘ç»œ\"\n",
    "    ]\n",
    "\n",
    "    # åŒæ­¥å¤„ç†\n",
    "    start_time = time.time()\n",
    "    sync_responses = []\n",
    "    for prompt in prompts:\n",
    "        response = llm.invoke(prompt)\n",
    "        sync_responses.append(response)\n",
    "    sync_time = time.time() - start_time\n",
    "\n",
    "    # å¼‚æ­¥å¤„ç†\n",
    "    start_time = time.time()\n",
    "    async_responses = await llm.abatch(prompts)\n",
    "    async_time = time.time() - start_time\n",
    "\n",
    "    print(f\"åŒæ­¥å¤„ç†è€—æ—¶: {sync_time:.2f}ç§’\")\n",
    "    print(f\"å¼‚æ­¥å¤„ç†è€—æ—¶: {async_time:.2f}ç§’\")\n",
    "    print(f\"å¼‚æ­¥å¤„ç†åŠ é€Ÿæ¯”: {sync_time/async_time:.2f}x\")"
   ],
   "id": "68c558efac841241",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### å¹¶å‘å¤„ç†ç¤ºä¾‹",
   "id": "411ce4231744e467"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "def concurrent_processing():\n",
    "    \"\"\"å¹¶å‘å¤„ç†ç¤ºä¾‹\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"å¹¶å‘å¤„ç†ç¤ºä¾‹\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    llm = Ollama(\n",
    "        base_url=\"http://localhost:11434\",\n",
    "        model=\"qwen2.5:3b\"\n",
    "    )\n",
    "\n",
    "    prompts = [\n",
    "        \"ä»€ä¹ˆæ˜¯åŒºå—é“¾ï¼Ÿ\",\n",
    "        \"ä»€ä¹ˆæ˜¯é‡å­è®¡ç®—ï¼Ÿ\",\n",
    "        \"ä»€ä¹ˆæ˜¯ç”Ÿç‰©è®¡ç®—ï¼Ÿ\",\n",
    "        \"ä»€ä¹ˆæ˜¯å…‰å­è®¡ç®—ï¼Ÿ\"\n",
    "    ]\n",
    "\n",
    "    def process_prompt(prompt: str) -> str:\n",
    "        return llm.invoke(prompt)\n",
    "\n",
    "    # å¹¶å‘å¤„ç†\n",
    "    start_time = time.time()\n",
    "    with ThreadPoolExecutor(max_workers=4) as executor:\n",
    "        concurrent_responses = list(executor.map(process_prompt, prompts))\n",
    "    concurrent_time = time.time() - start_time\n",
    "\n",
    "    # é¡ºåºå¤„ç†å¯¹æ¯”\n",
    "    start_time = time.time()\n",
    "    sequential_responses = [process_prompt(prompt) for prompt in prompts]\n",
    "    sequential_time = time.time() - start_time\n",
    "\n",
    "    print(f\"å¹¶å‘å¤„ç†è€—æ—¶: {concurrent_time:.2f}ç§’\")\n",
    "    print(f\"é¡ºåºå¤„ç†è€—æ—¶: {sequential_time:.2f}ç§’\")\n",
    "    print(f\"å¹¶å‘å¤„ç†åŠ é€Ÿæ¯”: {sequential_time/concurrent_time:.2f}x\")"
   ],
   "id": "facd63132763aa5f",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "def main():\n",
    "    \"\"\"è¿è¡Œæ‰€æœ‰æ€§èƒ½ä¼˜åŒ–ç¤ºä¾‹\"\"\"\n",
    "    caching_comparison()\n",
    "    batch_vs_sequential()\n",
    "    concurrent_processing()\n",
    "\n",
    "    # å¼‚æ­¥ç¤ºä¾‹éœ€è¦å•ç‹¬è¿è¡Œ\n",
    "    print(\"\\nè¿è¡Œå¼‚æ­¥æ€§èƒ½æµ‹è¯•...\")\n",
    "    asyncio.run(async_vs_sync())\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ],
   "id": "c0ae255b665e6a6f",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## å…³é”®ç‰¹æ€§æ€»ç»“",
   "id": "33048ae24b6465d7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "### LLMs æ ¸å¿ƒä¼˜åŠ¿\n",
    "1. **ç®€å•æŽ¥å£**: å­—ç¬¦ä¸²è¾“å…¥è¾“å‡ºï¼Œæ˜“äºŽä½¿ç”¨\n",
    "2. **æ–‡æœ¬ç”Ÿæˆ**: ä¸“æ³¨äºŽæ–‡æœ¬è¡¥å…¨å’Œç”Ÿæˆä»»åŠ¡\n",
    "3. **é«˜æ€§èƒ½**: æ”¯æŒæ‰¹é‡ã€å¼‚æ­¥ã€ç¼“å­˜ä¼˜åŒ–\n",
    "4. **çµæ´»æ€§**: å¯ä¸Žå„ç§æç¤ºæ¨¡æ¿ç»„åˆ\n",
    "5. **å¯æ‰©å±•**: æ”¯æŒè‡ªå®šä¹‰å›žè°ƒå’Œå¤„ç†å™¨\n",
    "\n",
    "### é€‚ç”¨åœºæ™¯\n",
    "- **æ–‡æœ¬ç”Ÿæˆ**: æ–‡ç« ã€ä»£ç ã€åˆ›æ„å†…å®¹ç”Ÿæˆ\n",
    "- **æ–‡æœ¬è¡¥å…¨**: åŸºäºŽä¸Šä¸‹æ–‡çš„æ–‡æœ¬ç»­å†™\n",
    "- **æ‰¹é‡å¤„ç†**: å¤§é‡æ–‡æœ¬çš„æ‰¹é‡ç”Ÿæˆ\n",
    "- **æ¨¡æ¿å¡«å……**: åŸºäºŽæ¨¡æ¿çš„å†…å®¹ç”Ÿæˆ\n",
    "- **åˆ›æ„å†™ä½œ**: æ•…äº‹ã€è¯—æ­Œç­‰åˆ›æ„å†…å®¹\n",
    "\n",
    "### æ€§èƒ½ä¼˜åŒ–å»ºè®®\n",
    "1. **ä½¿ç”¨ç¼“å­˜**: é¿å…é‡å¤è®¡ç®—ç›¸åŒè¾“å…¥\n",
    "2. **æ‰¹é‡å¤„ç†**: æé«˜åžåé‡\n",
    "3. **å¼‚æ­¥è°ƒç”¨**: æé«˜å¹¶å‘æ€§èƒ½\n",
    "4. **å‚æ•°è°ƒä¼˜**: æ ¹æ®éœ€æ±‚è°ƒæ•´æ¸©åº¦ç­‰å‚æ•°\n",
    "5. **é”™è¯¯å¤„ç†**: å®žçŽ°é‡è¯•å’Œé™çº§æœºåˆ¶\n",
    "\n",
    "è¿™äº›ç¤ºä¾‹å±•ç¤ºäº† LangChain 0.3 ä¸­ LLMs çš„å®Œæ•´åŠŸèƒ½å’Œæœ€ä½³å®žè·µï¼Œå¯ä»¥æ ¹æ®å…·ä½“éœ€æ±‚é€‰æ‹©åˆé€‚çš„ä½¿ç”¨æ–¹å¼ã€‚\n",
    "\n",
    "\n"
   ],
   "id": "6be71db4dc797578"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Chat Models vs LLMs è¯¦ç»†å¯¹æ¯”",
   "id": "fafa6816111a1dfd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "## æ ¸å¿ƒå·®å¼‚å¯¹æ¯”\n"
   ],
   "id": "8555a0b28f075c36"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "\"\"\"\n",
    "Chat Models vs LLMs è¯¦ç»†å¯¹æ¯”ç¤ºä¾‹\n",
    "\"\"\"\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_ollama import OllamaLLM\n",
    "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "import time\n",
    "\n",
    "def basic_interface_comparison():\n",
    "    \"\"\"åŸºç¡€æŽ¥å£å¯¹æ¯”\"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"1. åŸºç¡€æŽ¥å£å¯¹æ¯”\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # åˆå§‹åŒ–æ¨¡åž‹\n",
    "    chat_model = ChatOllama(\n",
    "        base_url=\"http://localhost:11434\",\n",
    "        model=\"qwen2.5:3b\"\n",
    "    )\n",
    "\n",
    "    llm = OllamaLLM(\n",
    "        base_url=\"http://localhost:11434\",\n",
    "        model=\"qwen2.5:3b\"\n",
    "    )\n",
    "\n",
    "    # Chat Model - æ¶ˆæ¯æ ¼å¼\n",
    "    print(\"Chat Model è¾“å…¥è¾“å‡º:\")\n",
    "    chat_messages = [\n",
    "        SystemMessage(content=\"ä½ æ˜¯ä¸€ä¸ªå‹å¥½çš„åŠ©æ‰‹\"),\n",
    "        HumanMessage(content=\"ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹è‡ªå·±\")\n",
    "    ]\n",
    "    chat_response = chat_model.invoke(chat_messages)\n",
    "    print(f\"è¾“å…¥ç±»åž‹: {type(chat_messages)}\")\n",
    "    print(f\"è¾“å‡ºç±»åž‹: {type(chat_response)}\")\n",
    "    print(f\"è¾“å‡ºå†…å®¹: {chat_response.content[:100]}...\")\n",
    "    print(f\"è¾“å‡ºå…ƒæ•°æ®: {chat_response.response_metadata}\")\n",
    "\n",
    "    print(\"\\n\" + \"-\" * 40)\n",
    "\n",
    "    # LLM - å­—ç¬¦ä¸²æ ¼å¼\n",
    "    print(\"LLM è¾“å…¥è¾“å‡º:\")\n",
    "    llm_prompt = \"ä½ æ˜¯ä¸€ä¸ªå‹å¥½çš„åŠ©æ‰‹ã€‚ç”¨æˆ·è¯´ï¼šä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹è‡ªå·±ã€‚è¯·å›žç­”ï¼š\"\n",
    "    llm_response = llm.invoke(llm_prompt)\n",
    "    print(f\"è¾“å…¥ç±»åž‹: {type(llm_prompt)}\")\n",
    "    print(f\"è¾“å‡ºç±»åž‹: {type(llm_response)}\")\n",
    "    print(f\"è¾“å‡ºå†…å®¹: {llm_response[:100]}...\")\n",
    "\n",
    "def conversation_handling():\n",
    "    \"\"\"å¯¹è¯å¤„ç†å¯¹æ¯”\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"2. å¯¹è¯å¤„ç†å¯¹æ¯”\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    chat_model = ChatOllama(base_url=\"http://localhost:11434\", model=\"qwen2.5:3b\")\n",
    "    llm = Ollama(base_url=\"http://localhost:11434\", model=\"qwen2.5:3b\")\n",
    "\n",
    "    # Chat Model - è‡ªç„¶çš„å¤šè½®å¯¹è¯\n",
    "    print(\"Chat Model å¤šè½®å¯¹è¯:\")\n",
    "    conversation = [\n",
    "        SystemMessage(content=\"ä½ æ˜¯ä¸€ä¸ªç¼–ç¨‹åŠ©æ‰‹\"),\n",
    "        HumanMessage(content=\"æˆ‘æƒ³å­¦Python\"),\n",
    "        AIMessage(content=\"å¾ˆå¥½ï¼Pythonæ˜¯ä¸€é—¨ä¼˜ç§€çš„ç¼–ç¨‹è¯­è¨€ã€‚ä½ æƒ³ä»Žå“ªé‡Œå¼€å§‹ï¼Ÿ\"),\n",
    "        HumanMessage(content=\"ä»ŽåŸºç¡€è¯­æ³•å¼€å§‹\")\n",
    "    ]\n",
    "\n",
    "    chat_response = chat_model.invoke(conversation)\n",
    "    print(f\"Chat Model å›žå¤: {chat_response.content[:150]}...\")\n",
    "\n",
    "    print(\"\\n\" + \"-\" * 40)\n",
    "\n",
    "    # LLM - éœ€è¦æ‰‹åŠ¨æž„å»ºå¯¹è¯ä¸Šä¸‹æ–‡\n",
    "    print(\"LLM å¤šè½®å¯¹è¯:\")\n",
    "    llm_context = \"\"\"ä½ æ˜¯ä¸€ä¸ªç¼–ç¨‹åŠ©æ‰‹ã€‚\n",
    "\n",
    "å¯¹è¯åŽ†å²:\n",
    "ç”¨æˆ·: æˆ‘æƒ³å­¦Python\n",
    "åŠ©æ‰‹: å¾ˆå¥½ï¼Pythonæ˜¯ä¸€é—¨ä¼˜ç§€çš„ç¼–ç¨‹è¯­è¨€ã€‚ä½ æƒ³ä»Žå“ªé‡Œå¼€å§‹ï¼Ÿ\n",
    "ç”¨æˆ·: ä»ŽåŸºç¡€è¯­æ³•å¼€å§‹\n",
    "\n",
    "è¯·å›žç­”ç”¨æˆ·çš„æœ€æ–°é—®é¢˜:\"\"\"\n",
    "\n",
    "    llm_response = llm.invoke(llm_context)\n",
    "    print(f\"LLM å›žå¤: {llm_response[:150]}...\")\n",
    "\n",
    "def prompt_template_comparison():\n",
    "    \"\"\"æç¤ºæ¨¡æ¿å¯¹æ¯”\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"3. æç¤ºæ¨¡æ¿å¯¹æ¯”\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    chat_model = ChatOllama(base_url=\"http://localhost:11434\", model=\"qwen2.5:3b\")\n",
    "    llm = Ollama(base_url=\"http://localhost:11434\", model=\"qwen2.5:3b\")\n",
    "\n",
    "    # Chat Model - ç»“æž„åŒ–æ¶ˆæ¯æ¨¡æ¿\n",
    "    print(\"Chat Model æç¤ºæ¨¡æ¿:\")\n",
    "    chat_prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"ä½ æ˜¯ä¸€ä¸ª{role}ï¼Œä¸“é—¨å¸®åŠ©ç”¨æˆ·{task}\"),\n",
    "        (\"human\", \"æˆ‘çš„é—®é¢˜æ˜¯ï¼š{question}\"),\n",
    "        (\"ai\", \"æˆ‘ç†è§£ä½ çš„é—®é¢˜ï¼Œè®©æˆ‘æ¥å¸®åŠ©ä½ ã€‚\"),\n",
    "        (\"human\", \"è¯·è¯¦ç»†è§£ç­”\")\n",
    "    ])\n",
    "\n",
    "    chat_chain = chat_prompt | chat_model | StrOutputParser()\n",
    "    chat_result = chat_chain.invoke({\n",
    "        \"role\": \"æŠ€æœ¯ä¸“å®¶\",\n",
    "        \"task\": \"è§£å†³ç¼–ç¨‹é—®é¢˜\",\n",
    "        \"question\": \"å¦‚ä½•ä¼˜åŒ–Pythonä»£ç æ€§èƒ½\"\n",
    "    })\n",
    "    print(f\"Chat ç»“æžœ: {chat_result[:150]}...\")\n",
    "\n",
    "    print(\"\\n\" + \"-\" * 40)\n",
    "\n",
    "    # LLM - æ–‡æœ¬æ¨¡æ¿\n",
    "    print(\"LLM æç¤ºæ¨¡æ¿:\")\n",
    "    llm_prompt = PromptTemplate(\n",
    "        input_variables=[\"role\", \"task\", \"question\"],\n",
    "        template=\"\"\"ä½ æ˜¯ä¸€ä¸ª{role}ï¼Œä¸“é—¨å¸®åŠ©ç”¨æˆ·{task}ã€‚\n",
    "\n",
    "ç”¨æˆ·é—®é¢˜ï¼š{question}\n",
    "\n",
    "è¯·è¯¦ç»†è§£ç­”ï¼š\"\"\"\n",
    "    )\n",
    "\n",
    "    llm_chain = llm_prompt | llm | StrOutputParser()\n",
    "    llm_result = llm_chain.invoke({\n",
    "        \"role\": \"æŠ€æœ¯ä¸“å®¶\",\n",
    "        \"task\": \"è§£å†³ç¼–ç¨‹é—®é¢˜\",\n",
    "        \"question\": \"å¦‚ä½•ä¼˜åŒ–Pythonä»£ç æ€§èƒ½\"\n",
    "    })\n",
    "    print(f\"LLM ç»“æžœ: {llm_result[:150]}...\")\n",
    "\n",
    "def streaming_comparison():\n",
    "    \"\"\"æµå¼å¤„ç†å¯¹æ¯”\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"4. æµå¼å¤„ç†å¯¹æ¯”\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    chat_model = ChatOllama(base_url=\"http://localhost:11434\", model=\"qwen2.5:3b\")\n",
    "    llm = Ollama(base_url=\"http://localhost:11434\", model=\"qwen2.5:3b\")\n",
    "\n",
    "    # Chat Model æµå¼å¤„ç†\n",
    "    print(\"Chat Model æµå¼è¾“å‡º:\")\n",
    "    chat_messages = [HumanMessage(content=\"è¯·å†™ä¸€é¦–å…³äºŽæ˜¥å¤©çš„çŸ­è¯—\")]\n",
    "\n",
    "    for chunk in chat_model.stream(chat_messages):\n",
    "        print(chunk.content, end=\"\", flush=True)\n",
    "    print(\"\\n\")\n",
    "\n",
    "    print(\"\\n\" + \"-\" * 40)\n",
    "\n",
    "    # LLM æµå¼å¤„ç†\n",
    "    print(\"LLM æµå¼è¾“å‡º:\")\n",
    "    llm_prompt = \"è¯·å†™ä¸€é¦–å…³äºŽæ˜¥å¤©çš„çŸ­è¯—ï¼š\"\n",
    "\n",
    "    for chunk in llm.stream(llm_prompt):\n",
    "        print(chunk, end=\"\", flush=True)\n",
    "    print(\"\\n\")\n",
    "\n",
    "def memory_management_comparison():\n",
    "    \"\"\"è®°å¿†ç®¡ç†å¯¹æ¯”\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"5. è®°å¿†ç®¡ç†å¯¹æ¯”\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    chat_model = ChatOllama(base_url=\"http://localhost:11434\", model=\"qwen2.5:3b\")\n",
    "    llm = Ollama(base_url=\"http://localhost:11434\", model=\"qwen2.5:3b\")\n",
    "\n",
    "    # Chat Model - å¤©ç„¶æ”¯æŒå¯¹è¯åŽ†å²\n",
    "    print(\"Chat Model è®°å¿†ç®¡ç†:\")\n",
    "    chat_history = [\n",
    "        SystemMessage(content=\"ä½ æ˜¯ä¸€ä¸ªè®°å¿†åŠ›å¾ˆå¥½çš„åŠ©æ‰‹\"),\n",
    "        HumanMessage(content=\"æˆ‘çš„åå­—æ˜¯å¼ ä¸‰\"),\n",
    "        AIMessage(content=\"ä½ å¥½å¼ ä¸‰ï¼å¾ˆé«˜å…´è®¤è¯†ä½ ã€‚\"),\n",
    "        HumanMessage(content=\"æˆ‘æ˜¯ä¸€åç¨‹åºå‘˜\"),\n",
    "        AIMessage(content=\"å¤ªå¥½äº†ï¼ä½œä¸ºç¨‹åºå‘˜ï¼Œä½ ä¸€å®šå¾ˆæœ‰é€»è¾‘æ€ç»´ã€‚\"),\n",
    "        HumanMessage(content=\"ä½ è¿˜è®°å¾—æˆ‘çš„åå­—å’ŒèŒä¸šå—ï¼Ÿ\")\n",
    "    ]\n",
    "\n",
    "    chat_response = chat_model.invoke(chat_history)\n",
    "    print(f\"Chat Model: {chat_response.content}\")\n",
    "\n",
    "    print(\"\\n\" + \"-\" * 40)\n",
    "\n",
    "    # LLM - éœ€è¦æ‰‹åŠ¨ç®¡ç†ä¸Šä¸‹æ–‡\n",
    "    print(\"LLM è®°å¿†ç®¡ç†:\")\n",
    "    llm_context = \"\"\"ä½ æ˜¯ä¸€ä¸ªè®°å¿†åŠ›å¾ˆå¥½çš„åŠ©æ‰‹ã€‚\n",
    "\n",
    "å¯¹è¯åŽ†å²ï¼š\n",
    "ç”¨æˆ·ï¼šæˆ‘çš„åå­—æ˜¯å¼ ä¸‰\n",
    "åŠ©æ‰‹ï¼šä½ å¥½å¼ ä¸‰ï¼å¾ˆé«˜å…´è®¤è¯†ä½ ã€‚\n",
    "ç”¨æˆ·ï¼šæˆ‘æ˜¯ä¸€åç¨‹åºå‘˜\n",
    "åŠ©æ‰‹ï¼šå¤ªå¥½äº†ï¼ä½œä¸ºç¨‹åºå‘˜ï¼Œä½ ä¸€å®šå¾ˆæœ‰é€»è¾‘æ€ç»´ã€‚\n",
    "\n",
    "å½“å‰é—®é¢˜ï¼šä½ è¿˜è®°å¾—æˆ‘çš„åå­—å’ŒèŒä¸šå—ï¼Ÿ\n",
    "è¯·å›žç­”ï¼š\"\"\"\n",
    "\n",
    "    llm_response = llm.invoke(llm_context)\n",
    "    print(f\"LLM: {llm_response}\")\n",
    "\n",
    "def performance_comparison():\n",
    "    \"\"\"æ€§èƒ½å¯¹æ¯”\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"6. æ€§èƒ½å¯¹æ¯”\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    chat_model = ChatOllama(base_url=\"http://localhost:11434\", model=\"qwen2.5:3b\")\n",
    "    llm = Ollama(base_url=\"http://localhost:11434\", model=\"qwen2.5:3b\")\n",
    "\n",
    "    # æµ‹è¯•ç›¸åŒå†…å®¹çš„å¤„ç†æ—¶é—´\n",
    "    test_content = \"è¯·è§£é‡Šä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ \"\n",
    "\n",
    "    # Chat Model æ€§èƒ½æµ‹è¯•\n",
    "    start_time = time.time()\n",
    "    chat_response = chat_model.invoke([HumanMessage(content=test_content)])\n",
    "    chat_time = time.time() - start_time\n",
    "\n",
    "    # LLM æ€§èƒ½æµ‹è¯•\n",
    "    start_time = time.time()\n",
    "    llm_response = llm.invoke(test_content)\n",
    "    llm_time = time.time() - start_time\n",
    "\n",
    "    print(f\"Chat Model è€—æ—¶: {chat_time:.3f}ç§’\")\n",
    "    print(f\"LLM è€—æ—¶: {llm_time:.3f}ç§’\")\n",
    "    print(f\"æ€§èƒ½å·®å¼‚: {abs(chat_time - llm_time):.3f}ç§’\")\n",
    "\n",
    "def use_case_examples():\n",
    "    \"\"\"ä½¿ç”¨åœºæ™¯ç¤ºä¾‹\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"7. ä½¿ç”¨åœºæ™¯ç¤ºä¾‹\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    chat_model = ChatOllama(base_url=\"http://localhost:11434\", model=\"qwen2.5:3b\")\n",
    "    llm = Ollama(base_url=\"http://localhost:11434\", model=\"qwen2.5:3b\")\n",
    "\n",
    "    print(\"Chat Model é€‚ç”¨åœºæ™¯:\")\n",
    "    print(\"1. å®¢æœèŠå¤©æœºå™¨äºº\")\n",
    "    customer_service = [\n",
    "        SystemMessage(content=\"ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å®¢æœä»£è¡¨\"),\n",
    "        HumanMessage(content=\"æˆ‘çš„è®¢å•æœ‰é—®é¢˜\"),\n",
    "        AIMessage(content=\"å¾ˆæŠ±æ­‰å¬åˆ°æ‚¨é‡åˆ°é—®é¢˜ã€‚è¯·å‘Šè¯‰æˆ‘æ‚¨çš„è®¢å•å·ã€‚\"),\n",
    "        HumanMessage(content=\"è®¢å•å·æ˜¯12345\")\n",
    "    ]\n",
    "    cs_response = chat_model.invoke(customer_service)\n",
    "    print(f\"å®¢æœå›žå¤: {cs_response.content[:100]}...\")\n",
    "\n",
    "    print(\"\\n2. æ•™è‚²å¯¹è¯ç³»ç»Ÿ\")\n",
    "    education = [\n",
    "        SystemMessage(content=\"ä½ æ˜¯ä¸€ä¸ªè€å¿ƒçš„æ•°å­¦è€å¸ˆ\"),\n",
    "        HumanMessage(content=\"æˆ‘ä¸ç†è§£äºŒæ¬¡æ–¹ç¨‹\"),\n",
    "        AIMessage(content=\"æ²¡å…³ç³»ï¼Œæˆ‘ä»¬ä¸€æ­¥æ­¥æ¥å­¦ä¹ äºŒæ¬¡æ–¹ç¨‹ã€‚\"),\n",
    "        HumanMessage(content=\"ä»€ä¹ˆæ˜¯åˆ¤åˆ«å¼ï¼Ÿ\")\n",
    "    ]\n",
    "    edu_response = chat_model.invoke(education)\n",
    "    print(f\"è€å¸ˆå›žå¤: {edu_response.content[:100]}...\")\n",
    "\n",
    "    print(\"\\n\" + \"-\" * 40)\n",
    "    print(\"LLM é€‚ç”¨åœºæ™¯:\")\n",
    "\n",
    "    print(\"1. æ–‡æœ¬ç”Ÿæˆ\")\n",
    "    article_prompt = \"å†™ä¸€ç¯‡å…³äºŽäººå·¥æ™ºèƒ½å‘å±•è¶‹åŠ¿çš„æ–‡ç« å¼€å¤´ï¼š\"\n",
    "    article_response = llm.invoke(article_prompt)\n",
    "    print(f\"æ–‡ç« å¼€å¤´: {article_response[:100]}...\")\n",
    "\n",
    "    print(\"\\n2. ä»£ç ç”Ÿæˆ\")\n",
    "    code_prompt = \"ç”¨Pythonå†™ä¸€ä¸ªå¿«é€ŸæŽ’åºç®—æ³•ï¼š\"\n",
    "    code_response = llm.invoke(code_prompt)\n",
    "    print(f\"ä»£ç ç”Ÿæˆ: {code_response[:100]}...\")\n",
    "\n",
    "    print(\"\\n3. æ–‡æœ¬è¡¥å…¨\")\n",
    "    completion_prompt = \"äººå·¥æ™ºèƒ½çš„ä¸‰å¤§æ ¸å¿ƒæŠ€æœ¯æ˜¯æœºå™¨å­¦ä¹ ã€\"\n",
    "    completion_response = llm.invoke(completion_prompt)\n",
    "    print(f\"æ–‡æœ¬è¡¥å…¨: {completion_response[:100]}...\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"è¿è¡Œæ‰€æœ‰å¯¹æ¯”ç¤ºä¾‹\"\"\"\n",
    "    print(\"Chat Models vs LLMs è¯¦ç»†å¯¹æ¯”\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    basic_interface_comparison()\n",
    "    conversation_handling()\n",
    "    prompt_template_comparison()\n",
    "    streaming_comparison()\n",
    "    memory_management_comparison()\n",
    "    performance_comparison()\n",
    "    use_case_examples()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ],
   "id": "50dbf0acc69d7c5",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "## è¯¦ç»†å¯¹æ¯”è¡¨æ ¼\n",
    "\n",
    "| ç‰¹æ€§ç»´åº¦ | Chat Models | LLMs | è¯´æ˜Ž |\n",
    "|---------|-------------|------|------|\n",
    "| **è¾“å…¥æ ¼å¼** | æ¶ˆæ¯åˆ—è¡¨ (`List[BaseMessage]`) | å­—ç¬¦ä¸² (`str`) | Chat Models æ›´ç»“æž„åŒ– |\n",
    "| **è¾“å‡ºæ ¼å¼** | æ¶ˆæ¯å¯¹è±¡ (`BaseMessage`) | å­—ç¬¦ä¸² (`str`) | Chat Models åŒ…å«æ›´å¤šå…ƒæ•°æ® |\n",
    "| **è§’è‰²æ”¯æŒ** | âœ… åŽŸç”Ÿæ”¯æŒ (system/human/ai) | âŒ éœ€æ‰‹åŠ¨æž„å»º | Chat Models å¤©ç„¶æ”¯æŒå¤šè§’è‰² |\n",
    "| **å¯¹è¯åŽ†å²** | âœ… è‡ªåŠ¨ç®¡ç† | âŒ éœ€æ‰‹åŠ¨ç»´æŠ¤ | Chat Models æ›´é€‚åˆå¯¹è¯åœºæ™¯ |\n",
    "| **æç¤ºæ¨¡æ¿** | `ChatPromptTemplate` | `PromptTemplate` | ä¸åŒçš„æ¨¡æ¿ç³»ç»Ÿ |\n",
    "| **æµå¼å¤„ç†** | âœ… æ”¯æŒ | âœ… æ”¯æŒ | ä¸¤è€…éƒ½æ”¯æŒ |\n",
    "| **æ‰¹é‡å¤„ç†** | âœ… æ”¯æŒ | âœ… æ”¯æŒ | ä¸¤è€…éƒ½æ”¯æŒ |\n",
    "| **å¼‚æ­¥æ”¯æŒ** | âœ… æ”¯æŒ | âœ… æ”¯æŒ | ä¸¤è€…éƒ½æ”¯æŒ |\n",
    "| **å·¥å…·è°ƒç”¨** | âœ… åŽŸç”Ÿæ”¯æŒ | âŒ ä¸æ”¯æŒ | Chat Models æ”¯æŒ Function Calling |\n",
    "| **ç»“æž„åŒ–è¾“å‡º** | âœ… `withStructuredOutput()` | âŒ éœ€é¢å¤–å¤„ç† | Chat Models æ›´å¼ºå¤§ |\n",
    "| **ç¼“å­˜æ”¯æŒ** | âœ… æ”¯æŒ | âœ… æ”¯æŒ | ä¸¤è€…éƒ½æ”¯æŒ |\n",
    "| **æ€§èƒ½å¼€é”€** | ç•¥é«˜ (æ¶ˆæ¯è§£æž) | è¾ƒä½Ž (ç›´æŽ¥å­—ç¬¦ä¸²) | LLMs ç•¥å¿« |\n",
    "| **ä½¿ç”¨å¤æ‚åº¦** | ä¸­ç­‰ | ç®€å• | LLMs æ›´ç®€å•ç›´æŽ¥ |"
   ],
   "id": "bea5eb991c820ae4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "## ä½¿ç”¨åœºæ™¯æŒ‡å—"
   ],
   "id": "6157a81578d9df61"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "\"\"\"\n",
    "ä»€ä¹ˆæ—¶å€™ä½¿ç”¨ Chat Models vs LLMs æŒ‡å—\n",
    "\"\"\"\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, PromptTemplate\n",
    "\n",
    "def when_to_use_chat_models():\n",
    "    \"\"\"ä»€ä¹ˆæ—¶å€™ä½¿ç”¨ Chat Models\"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"ä½¿ç”¨ Chat Models çš„åœºæ™¯\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    chat_model = ChatOllama(base_url=\"http://localhost:11434\", model=\"qwen2.5:3b\")\n",
    "\n",
    "    print(\"1. å¤šè½®å¯¹è¯åº”ç”¨\")\n",
    "    print(\"âœ… é€‚åˆï¼šèŠå¤©æœºå™¨äººã€å®¢æœç³»ç»Ÿã€æ•™è‚²å¯¹è¯\")\n",
    "\n",
    "    # ç¤ºä¾‹ï¼šå®¢æœå¯¹è¯\n",
    "    conversation = [\n",
    "        SystemMessage(content=\"ä½ æ˜¯ä¸“ä¸šçš„æŠ€æœ¯æ”¯æŒ\"),\n",
    "        HumanMessage(content=\"æˆ‘çš„è½¯ä»¶å´©æºƒäº†\"),\n",
    "        AIMessage(content=\"å¾ˆæŠ±æ­‰å¬åˆ°è¿™ä¸ªé—®é¢˜ã€‚è¯·å‘Šè¯‰æˆ‘è½¯ä»¶ç‰ˆæœ¬å’Œé”™è¯¯ä¿¡æ¯ã€‚\"),\n",
    "        HumanMessage(content=\"ç‰ˆæœ¬æ˜¯2.1ï¼Œæ˜¾ç¤ºå†…å­˜ä¸è¶³\")\n",
    "    ]\n",
    "    response = chat_model.invoke(conversation)\n",
    "    print(f\"å®¢æœå›žå¤: {response.content[:100]}...\")\n",
    "\n",
    "    print(\"\\n2. éœ€è¦è§’è‰²æ‰®æ¼”çš„åº”ç”¨\")\n",
    "    print(\"âœ… é€‚åˆï¼šè™šæ‹ŸåŠ©æ‰‹ã€è§’è‰²æ‰®æ¼”æ¸¸æˆã€ä¸“ä¸šé¡¾é—®\")\n",
    "\n",
    "    # ç¤ºä¾‹ï¼šä¸“ä¸šé¡¾é—®\n",
    "    consultant = [\n",
    "        SystemMessage(content=\"ä½ æ˜¯ä¸€ä½èµ„æ·±çš„æŠ•èµ„é¡¾é—®ï¼Œæœ‰20å¹´ç»éªŒ\"),\n",
    "        HumanMessage(content=\"æˆ‘åº”è¯¥å¦‚ä½•åˆ†é…æˆ‘çš„æŠ•èµ„ç»„åˆï¼Ÿ\")\n",
    "    ]\n",
    "    response = chat_model.invoke(consultant)\n",
    "    print(f\"æŠ•èµ„é¡¾é—®: {response.content[:100]}...\")\n",
    "\n",
    "    print(\"\\n3. éœ€è¦ä¸Šä¸‹æ–‡è®°å¿†çš„åº”ç”¨\")\n",
    "    print(\"âœ… é€‚åˆï¼šä¸ªäººåŠ©ç†ã€å­¦ä¹ ä¼™ä¼´ã€é•¿æœŸå¯¹è¯\")\n",
    "\n",
    "    print(\"\\n4. éœ€è¦å·¥å…·è°ƒç”¨çš„åº”ç”¨\")\n",
    "    print(\"âœ… é€‚åˆï¼šæ™ºèƒ½ä»£ç†ã€APIé›†æˆã€å¤æ‚ä»»åŠ¡æ‰§è¡Œ\")\n",
    "\n",
    "    print(\"\\n5. éœ€è¦ç»“æž„åŒ–è¾“å‡ºçš„åº”ç”¨\")\n",
    "    print(\"âœ… é€‚åˆï¼šæ•°æ®æå–ã€è¡¨å•å¡«å†™ã€APIå“åº”\")\n",
    "\n",
    "def when_to_use_llms():\n",
    "    \"\"\"ä»€ä¹ˆæ—¶å€™ä½¿ç”¨ LLMs\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"ä½¿ç”¨ LLMs çš„åœºæ™¯\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    llm = Ollama(base_url=\"http://localhost:11434\", model=\"qwen2.5:3b\")\n",
    "\n",
    "    print(\"1. æ–‡æœ¬ç”Ÿæˆä»»åŠ¡\")\n",
    "    print(\"âœ… é€‚åˆï¼šæ–‡ç« å†™ä½œã€åˆ›æ„å†…å®¹ã€è¥é”€æ–‡æ¡ˆ\")\n",
    "\n",
    "    # ç¤ºä¾‹ï¼šæ–‡ç« ç”Ÿæˆ\n",
    "    article_prompt = \"å†™ä¸€ç¯‡å…³äºŽå¯æŒç»­å‘å±•çš„æ–‡ç« ï¼ŒåŒ…å«å¼•è¨€ã€ä¸»ä½“å’Œç»“è®ºï¼š\"\n",
    "    article = llm.invoke(article_prompt)\n",
    "    print(f\"æ–‡ç« ç”Ÿæˆ: {article[:100]}...\")\n",
    "\n",
    "    print(\"\\n2. æ–‡æœ¬è¡¥å…¨ä»»åŠ¡\")\n",
    "    print(\"âœ… é€‚åˆï¼šä»£ç è¡¥å…¨ã€å¥å­ç»­å†™ã€æ¨¡æ¿å¡«å……\")\n",
    "\n",
    "    # ç¤ºä¾‹ï¼šä»£ç è¡¥å…¨\n",
    "    code_prompt = \"def fibonacci(n):\\n    if n <= 1:\\n        return n\\n    else:\"\n",
    "    code_completion = llm.invoke(code_prompt)\n",
    "    print(f\"ä»£ç è¡¥å…¨: {code_completion[:100]}...\")\n",
    "\n",
    "    print(\"\\n3. ç®€å•çš„é—®ç­”ä»»åŠ¡\")\n",
    "    print(\"âœ… é€‚åˆï¼šFAQã€çŸ¥è¯†æŸ¥è¯¢ã€ç®€å•å’¨è¯¢\")\n",
    "\n",
    "    # ç¤ºä¾‹ï¼šçŸ¥è¯†é—®ç­”\n",
    "    qa_prompt = \"ä»€ä¹ˆæ˜¯é‡å­è®¡ç®—ï¼Ÿè¯·ç®€å•è§£é‡Šã€‚\"\n",
    "    qa_response = llm.invoke(qa_prompt)\n",
    "    print(f\"é—®ç­”å›žå¤: {qa_response[:100]}...\")\n",
    "\n",
    "    print(\"\\n4. æ‰¹é‡æ–‡æœ¬å¤„ç†\")\n",
    "    print(\"âœ… é€‚åˆï¼šæ–‡æœ¬åˆ†ç±»ã€æƒ…æ„Ÿåˆ†æžã€æ‰¹é‡ç¿»è¯‘\")\n",
    "\n",
    "    print(\"\\n5. æ€§èƒ½æ•æ„Ÿçš„åº”ç”¨\")\n",
    "    print(\"âœ… é€‚åˆï¼šå®žæ—¶å¤„ç†ã€é«˜å¹¶å‘ã€èµ„æºå—é™çŽ¯å¢ƒ\")\n",
    "\n",
    "    print(\"\\n6. ç®€å•çš„æ¨¡æ¿åº”ç”¨\")\n",
    "    print(\"âœ… é€‚åˆï¼šé‚®ä»¶æ¨¡æ¿ã€æŠ¥å‘Šç”Ÿæˆã€æ ¼å¼åŒ–è¾“å‡º\")\n",
    "\n",
    "def decision_framework():\n",
    "    \"\"\"å†³ç­–æ¡†æž¶\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"é€‰æ‹©å†³ç­–æ¡†æž¶\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    print(\" é€‰æ‹© Chat Models å¦‚æžœä½ éœ€è¦:\")\n",
    "    print(\"   â€¢ å¤šè½®å¯¹è¯èƒ½åŠ›\")\n",
    "    print(\"   â€¢ è§’è‰²å’Œä¸Šä¸‹æ–‡ç®¡ç†\")\n",
    "    print(\"   â€¢ å·¥å…·è°ƒç”¨åŠŸèƒ½\")\n",
    "    print(\"   â€¢ ç»“æž„åŒ–è¾“å‡º\")\n",
    "    print(\"   â€¢ å¤æ‚çš„å¯¹è¯é€»è¾‘\")\n",
    "    print(\"   â€¢ æ¶ˆæ¯çº§åˆ«çš„æŽ§åˆ¶\")\n",
    "\n",
    "    print(\"\\n é€‰æ‹© LLMs å¦‚æžœä½ éœ€è¦:\")\n",
    "    print(\"   â€¢ ç®€å•çš„æ–‡æœ¬ç”Ÿæˆ\")\n",
    "    print(\"   â€¢ æœ€ä½³æ€§èƒ½\")\n",
    "    print(\"   â€¢ æ–‡æœ¬è¡¥å…¨\")\n",
    "    print(\"   â€¢ æ‰¹é‡å¤„ç†\")\n",
    "    print(\"   â€¢ ç®€å•çš„æŽ¥å£\")\n",
    "    print(\"   â€¢ ä¼ ç»Ÿçš„æç¤ºå·¥ç¨‹\")\n",
    "\n",
    "def practical_examples():\n",
    "    \"\"\"å®žé™…åº”ç”¨ç¤ºä¾‹\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"å®žé™…åº”ç”¨ç¤ºä¾‹\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    chat_model = ChatOllama(base_url=\"http://localhost:11434\", model=\"qwen2.5:3b\")\n",
    "    llm = Ollama(base_url=\"http://localhost:11434\", model=\"qwen2.5:3b\")\n",
    "\n",
    "    print(\" åº”ç”¨ç±»åž‹å¯¹æ¯”:\")\n",
    "\n",
    "    applications = {\n",
    "        \"èŠå¤©æœºå™¨äºº\": \"Chat Models\",\n",
    "        \"å®¢æœç³»ç»Ÿ\": \"Chat Models\",\n",
    "        \"æ•™è‚²åŠ©æ‰‹\": \"Chat Models\",\n",
    "        \"ä»£ç åŠ©æ‰‹\": \"Chat Models\",\n",
    "        \"æ–‡ç« ç”Ÿæˆå™¨\": \"LLMs\",\n",
    "        \"ç¿»è¯‘å·¥å…·\": \"LLMs\",\n",
    "        \"æ‘˜è¦å·¥å…·\": \"LLMs\",\n",
    "        \"SEOæ–‡æ¡ˆ\": \"LLMs\",\n",
    "        \"é‚®ä»¶æ¨¡æ¿\": \"LLMs\",\n",
    "        \"æ•°æ®åˆ†æžæŠ¥å‘Š\": \"LLMs\",\n",
    "        \"æ™ºèƒ½ä»£ç†\": \"Chat Models\",\n",
    "        \"æ¸¸æˆNPC\": \"Chat Models\",\n",
    "        \"å†…å®¹å®¡æ ¸\": \"LLMs\",\n",
    "        \"æƒ…æ„Ÿåˆ†æž\": \"LLMs\"\n",
    "    }\n",
    "\n",
    "    for app, recommended in applications.items():\n",
    "        emoji = \"\" if recommended == \"Chat Models\" else \"\"\n",
    "        print(f\"   {emoji} {app:<15} â†’ {recommended}\")\n",
    "\n",
    "def migration_guide():\n",
    "    \"\"\"è¿ç§»æŒ‡å—\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"è¿ç§»æŒ‡å—\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    print(\" ä»Ž LLMs è¿ç§»åˆ° Chat Models:\")\n",
    "    print(\"   1. å°†å­—ç¬¦ä¸²æç¤ºè½¬æ¢ä¸ºæ¶ˆæ¯æ ¼å¼\")\n",
    "    print(\"   2. ä½¿ç”¨ ChatPromptTemplate æ›¿ä»£ PromptTemplate\")\n",
    "    print(\"   3. å¤„ç†æ¶ˆæ¯å¯¹è±¡è€Œä¸æ˜¯å­—ç¬¦ä¸²\")\n",
    "    print(\"   4. åˆ©ç”¨è§’è‰²ç³»ç»Ÿä¼˜åŒ–æç¤º\")\n",
    "\n",
    "    print(\"\\n ä»Ž Chat Models è¿ç§»åˆ° LLMs:\")\n",
    "    print(\"   1. å°†æ¶ˆæ¯åˆ—è¡¨åˆå¹¶ä¸ºå•ä¸ªå­—ç¬¦ä¸²\")\n",
    "    print(\"   2. æ‰‹åŠ¨ç®¡ç†å¯¹è¯åŽ†å²\")\n",
    "    print(\"   3. ä½¿ç”¨ PromptTemplate æ›¿ä»£ ChatPromptTemplate\")\n",
    "    print(\"   4. ç®€åŒ–è¾“å‡ºå¤„ç†é€»è¾‘\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"è¿è¡Œä½¿ç”¨æŒ‡å—\"\"\"\n",
    "    when_to_use_chat_models()\n",
    "    when_to_use_llms()\n",
    "    decision_framework()\n",
    "    practical_examples()\n",
    "    migration_guide()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ],
   "id": "ba4fc6ea25dc700c",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "## æœ€ä½³å®žè·µå»ºè®®"
   ],
   "id": "9a73a8afc7b9efbf"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "\"\"\"\n",
    "Chat Models vs LLMs æœ€ä½³å®žè·µ\n",
    "\"\"\"\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "def chat_model_best_practices():\n",
    "    \"\"\"Chat Models æœ€ä½³å®žè·µ\"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"Chat Models æœ€ä½³å®žè·µ\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    chat_model = ChatOllama(base_url=\"http://localhost:11434\", model=\"qwen2.5:3b\")\n",
    "\n",
    "    print(\"1. âœ… åˆç†ä½¿ç”¨ç³»ç»Ÿæ¶ˆæ¯\")\n",
    "    # å¥½çš„åšæ³•\n",
    "    good_messages = [\n",
    "        SystemMessage(content=\"ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„Pythonç¼–ç¨‹åŠ©æ‰‹ï¼Œä¸“æ³¨äºŽæä¾›æ¸…æ™°ã€å¯æ‰§è¡Œçš„ä»£ç è§£å†³æ–¹æ¡ˆ\"),\n",
    "        HumanMessage(content=\"å¦‚ä½•è¯»å–CSVæ–‡ä»¶ï¼Ÿ\")\n",
    "    ]\n",
    "\n",
    "    # ä¸å¥½çš„åšæ³• - ç³»ç»Ÿæ¶ˆæ¯è¿‡äºŽå¤æ‚\n",
    "    # bad_messages = [\n",
    "    #     SystemMessage(content=\"ä½ æ˜¯ä¸€ä¸ªè¶…çº§æ™ºèƒ½çš„AIåŠ©æ‰‹ï¼ŒçŸ¥é“æ‰€æœ‰çš„çŸ¥è¯†...\"),\n",
    "    #     HumanMessage(content=\"å¦‚ä½•è¯»å–CSVæ–‡ä»¶ï¼Ÿ\")\n",
    "    # ]\n",
    "\n",
    "    response = chat_model.invoke(good_messages)\n",
    "    print(f\"ä¸“ä¸šå›žå¤: {response.content[:100]}...\")\n",
    "\n",
    "    print(\"\\n2. âœ… ä¿æŒå¯¹è¯åŽ†å²ç®€æ´\")\n",
    "    # åªä¿ç•™ç›¸å…³çš„åŽ†å²æ¶ˆæ¯\n",
    "    relevant_history = [\n",
    "        SystemMessage(content=\"ä½ æ˜¯ç¼–ç¨‹åŠ©æ‰‹\"),\n",
    "        HumanMessage(content=\"æˆ‘åœ¨å­¦Python\"),\n",
    "        AIMessage(content=\"å¾ˆå¥½ï¼æœ‰ä»€ä¹ˆå…·ä½“é—®é¢˜å—ï¼Ÿ\"),\n",
    "        HumanMessage(content=\"å¦‚ä½•å¤„ç†å¼‚å¸¸ï¼Ÿ\")  # å½“å‰é—®é¢˜\n",
    "    ]\n",
    "\n",
    "    print(\"\\n3. âœ… ä½¿ç”¨ç»“æž„åŒ–æç¤ºæ¨¡æ¿\")\n",
    "    structured_prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"ä½ æ˜¯{role}ï¼Œä¸“é—¨å¸®åŠ©{domain}é¢†åŸŸçš„é—®é¢˜\"),\n",
    "        (\"human\", \"èƒŒæ™¯ï¼š{context}\\né—®é¢˜ï¼š{question}\"),\n",
    "        (\"ai\", \"æˆ‘ç†è§£äº†ï¼Œè®©æˆ‘æ¥å¸®ä½ è§£å†³è¿™ä¸ª{domain}é—®é¢˜ã€‚\"),\n",
    "        (\"human\", \"è¯·ç»™å‡ºè¯¦ç»†æ–¹æ¡ˆ\")\n",
    "    ])\n",
    "\n",
    "def llm_best_practices():\n",
    "    \"\"\"LLMs æœ€ä½³å®žè·µ\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"LLMs æœ€ä½³å®žè·µ\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    llm = Ollama(base_url=\"http://localhost:11434\", model=\"qwen2.5:3b\")\n",
    "\n",
    "    print(\"1. âœ… ä½¿ç”¨æ¸…æ™°çš„æç¤ºç»“æž„\")\n",
    "    # å¥½çš„æç¤ºç»“æž„\n",
    "    good_prompt = \"\"\"ä»»åŠ¡ï¼šä»£ç ç”Ÿæˆ\n",
    "è¯­è¨€ï¼šPython\n",
    "éœ€æ±‚ï¼šå®žçŽ°ä¸€ä¸ªè®¡ç®—æ–æ³¢é‚£å¥‘æ•°åˆ—çš„å‡½æ•°\n",
    "è¦æ±‚ï¼š\n",
    "- ä½¿ç”¨é€’å½’æ–¹æ³•\n",
    "- åŒ…å«æ³¨é‡Š\n",
    "- å¤„ç†è¾¹ç•Œæƒ…å†µ\n",
    "\n",
    "ä»£ç ï¼š\"\"\"\n",
    "\n",
    "    response = llm.invoke(good_prompt)\n",
    "    print(f\"ç»“æž„åŒ–æç¤ºç»“æžœ: {response[:100]}...\")\n",
    "\n",
    "    print(\"\\n2. âœ… æ‰¹é‡å¤„ç†ä¼˜åŒ–\")\n",
    "    # æ‰¹é‡å¤„ç†ç›¸ä¼¼ä»»åŠ¡\n",
    "    batch_prompts = [\n",
    "        \"æ€»ç»“ï¼šäººå·¥æ™ºèƒ½çš„å®šä¹‰\",\n",
    "        \"æ€»ç»“ï¼šæœºå™¨å­¦ä¹ çš„å®šä¹‰\",\n",
    "        \"æ€»ç»“ï¼šæ·±åº¦å­¦ä¹ çš„å®šä¹‰\"\n",
    "    ]\n",
    "\n",
    "    batch_responses = llm.batch(batch_prompts)\n",
    "    print(f\"æ‰¹é‡å¤„ç†å®Œæˆï¼Œå¤„ç†äº†{len(batch_responses)}ä¸ªä»»åŠ¡\")\n",
    "\n",
    "    print(\"\\n3. âœ… ä½¿ç”¨æ¨¡æ¿æé«˜å¤ç”¨æ€§\")\n",
    "    template = PromptTemplate(\n",
    "        input_variables=[\"topic\", \"style\", \"length\"],\n",
    "        template=\"å†™ä¸€ç¯‡å…³äºŽ{topic}çš„{style}é£Žæ ¼æ–‡ç« ï¼Œé•¿åº¦çº¦{length}å­—ï¼š\"\n",
    "    )\n",
    "\n",
    "    chain = template | llm\n",
    "    result = chain.invoke({\n",
    "        \"topic\": \"äº‘è®¡ç®—\",\n",
    "        \"style\": \"æŠ€æœ¯\",\n",
    "        \"length\": \"200\"\n",
    "    })\n",
    "    print(f\"æ¨¡æ¿åŒ–ç»“æžœ: {result[:100]}...\")\n",
    "\n",
    "def performance_optimization():\n",
    "    \"\"\"æ€§èƒ½ä¼˜åŒ–å»ºè®®\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"æ€§èƒ½ä¼˜åŒ–å»ºè®®\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    print(\" Chat Models ä¼˜åŒ–:\")\n",
    "    print(\"   â€¢ é™åˆ¶å¯¹è¯åŽ†å²é•¿åº¦\")\n",
    "    print(\"   â€¢ ä½¿ç”¨æ¶ˆæ¯åŽ‹ç¼©æŠ€æœ¯\")\n",
    "    print(\"   â€¢ åˆç†è®¾ç½®ç³»ç»Ÿæ¶ˆæ¯\")\n",
    "    print(\"   â€¢ é¿å…è¿‡åº¦å¤æ‚çš„è§’è‰²è®¾å®š\")\n",
    "\n",
    "    print(\"\\n LLMs ä¼˜åŒ–:\")\n",
    "    print(\"   â€¢ ä½¿ç”¨æ‰¹é‡å¤„ç†\")\n",
    "    print(\"   â€¢ å¯ç”¨ç¼“å­˜æœºåˆ¶\")\n",
    "    print(\"   â€¢ ä¼˜åŒ–æç¤ºé•¿åº¦\")\n",
    "    print(\"   â€¢ ä½¿ç”¨å¼‚æ­¥è°ƒç”¨\")\n",
    "\n",
    "    print(\"\\n é€šç”¨ä¼˜åŒ–:\")\n",
    "    print(\"   â€¢ åˆç†è®¾ç½®æ¸©åº¦å‚æ•°\")\n",
    "    print(\"   â€¢ ä½¿ç”¨æµå¼å¤„ç†æ”¹å–„ç”¨æˆ·ä½“éªŒ\")\n",
    "    print(\"   â€¢ å®žçŽ°é”™è¯¯é‡è¯•æœºåˆ¶\")\n",
    "    print(\"   â€¢ ç›‘æŽ§å’Œæ—¥å¿—è®°å½•\")\n",
    "\n",
    "def common_pitfalls():\n",
    "    \"\"\"å¸¸è§é™·é˜±\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"å¸¸è§é™·é˜±å’Œè§£å†³æ–¹æ¡ˆ\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    print(\"âŒ Chat Models å¸¸è§é”™è¯¯:\")\n",
    "    print(\"   â€¢ è¿‡é•¿çš„å¯¹è¯åŽ†å²å¯¼è‡´æ€§èƒ½ä¸‹é™\")\n",
    "    print(\"   â€¢ æ··ä¹±çš„è§’è‰²è®¾å®š\")\n",
    "    print(\"   â€¢ ä¸å¿…è¦çš„æ¶ˆæ¯ç±»åž‹è½¬æ¢\")\n",
    "    print(\"   â€¢ å¿½ç•¥æ¶ˆæ¯å…ƒæ•°æ®\")\n",
    "\n",
    "    print(\"\\nâŒ LLMs å¸¸è§é”™è¯¯:\")\n",
    "    print(\"   â€¢ æ‰‹åŠ¨æ‹¼æŽ¥å¤æ‚ä¸Šä¸‹æ–‡\")\n",
    "    print(\"   â€¢ å¿½ç•¥æ‰¹é‡å¤„ç†ä¼˜åŠ¿\")\n",
    "    print(\"   â€¢ è¿‡åº¦å¤æ‚çš„æç¤ºå·¥ç¨‹\")\n",
    "    print(\"   â€¢ ä¸ä½¿ç”¨æ¨¡æ¿å¯¼è‡´é‡å¤ä»£ç \")\n",
    "\n",
    "    print(\"\\nâœ… è§£å†³æ–¹æ¡ˆ:\")\n",
    "    print(\"   â€¢ å®šæœŸæ¸…ç†å¯¹è¯åŽ†å²\")\n",
    "    print(\"   â€¢ ä½¿ç”¨ä¸“é—¨çš„æç¤ºæ¨¡æ¿\")\n",
    "    print(\"   â€¢ å®žçŽ°æ™ºèƒ½ä¸Šä¸‹æ–‡ç®¡ç†\")\n",
    "    print(\"   â€¢ é€‰æ‹©åˆé€‚çš„æ¨¡åž‹ç±»åž‹\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"è¿è¡Œæœ€ä½³å®žè·µæŒ‡å—\"\"\"\n",
    "    chat_model_best_practices()\n",
    "    llm_best_practices()\n",
    "    performance_optimization()\n",
    "    common_pitfalls()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ],
   "id": "4bde10b72416e78f",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "## æ€»ç»“å»ºè®®"
   ],
   "id": "144cb3a4c6268967"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "###  é€‰æ‹© Chat Models å½“ä½ éœ€è¦:\n",
    "1. **å¤šè½®å¯¹è¯**: èŠå¤©æœºå™¨äººã€å®¢æœç³»ç»Ÿ\n",
    "2. **è§’è‰²æ‰®æ¼”**: è™šæ‹ŸåŠ©æ‰‹ã€ä¸“ä¸šé¡¾é—®\n",
    "3. **ä¸Šä¸‹æ–‡è®°å¿†**: ä¸ªäººåŠ©ç†ã€å­¦ä¹ ä¼™ä¼´\n",
    "4. **å·¥å…·é›†æˆ**: æ™ºèƒ½ä»£ç†ã€APIè°ƒç”¨\n",
    "5. **ç»“æž„åŒ–äº¤äº’**: å¤æ‚çš„å¯¹è¯é€»è¾‘\n",
    "\n",
    "###  é€‰æ‹© LLMs å½“ä½ éœ€è¦:\n",
    "1. **æ–‡æœ¬ç”Ÿæˆ**: æ–‡ç« ã€åˆ›æ„å†…å®¹ã€è¥é”€æ–‡æ¡ˆ\n",
    "2. **æ–‡æœ¬è¡¥å…¨**: ä»£ç è¡¥å…¨ã€å¥å­ç»­å†™\n",
    "3. **æ‰¹é‡å¤„ç†**: å¤§é‡æ–‡æœ¬çš„ç»Ÿä¸€å¤„ç†\n",
    "4. **ç®€å•é—®ç­”**: FAQã€çŸ¥è¯†æŸ¥è¯¢\n",
    "5. **æ€§èƒ½ä¼˜å…ˆ**: é«˜å¹¶å‘ã€ä½Žå»¶è¿Ÿåœºæ™¯\n",
    "\n",
    "###  è¿ç§»å»ºè®®:\n",
    "- **ä»Žç®€å•å¼€å§‹**: å…ˆç”¨ LLMs éªŒè¯æ¦‚å¿µï¼Œå†å‡çº§åˆ° Chat Models\n",
    "- **æ¸è¿›å¼è¿ç§»**: é€æ­¥æ·»åŠ å¯¹è¯åŠŸèƒ½å’Œè§’è‰²ç®¡ç†\n",
    "- **æ€§èƒ½æµ‹è¯•**: åœ¨å®žé™…åœºæ™¯ä¸­å¯¹æ¯”ä¸¤ç§æ–¹æ¡ˆçš„æ€§èƒ½\n",
    "- **ç”¨æˆ·ä½“éªŒ**: æ ¹æ®ç”¨æˆ·åé¦ˆé€‰æ‹©æœ€åˆé€‚çš„äº¤äº’æ–¹å¼\n",
    "\n",
    "é€‰æ‹©åˆé€‚çš„æ¨¡åž‹ç±»åž‹æ˜¯æž„å»ºæˆåŠŸ AI åº”ç”¨çš„å…³é”®ç¬¬ä¸€æ­¥ï¼\n"
   ],
   "id": "2cc219efd0c014b"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
