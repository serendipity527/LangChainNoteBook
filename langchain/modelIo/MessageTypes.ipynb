{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# LangChain 0.3 消息处理详解",
   "id": "853d18f92282e86a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "LangChain 中的消息类型是与大语言模型交互的基础组件。消息类型提供了统一的格式，让你可以与不同的聊天模型无缝交互。\n",
    "\n",
    "## 主要消息类型\n",
    "\n",
    "LangChain 提供了几种核心消息类型：\n",
    "\n",
    "1. **HumanMessage** - 对应用户输入\n",
    "2. **AIMessage** - 对应AI助手回复\n",
    "3. **SystemMessage** - 对应系统指令\n",
    "4. **ToolMessage** - 对应工具调用结果\n",
    "5. **FunctionMessage** - 对应函数调用结果（旧版API）"
   ],
   "id": "11518492211542c3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "## 基础消息使用示例"
   ],
   "id": "21198c0818da3e02"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-22T08:12:02.499589Z",
     "start_time": "2025-07-22T08:11:52.394670Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_core.messages import (\n",
    "    HumanMessage,\n",
    "    AIMessage,\n",
    "    SystemMessage\n",
    ")\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "def basic_messages_demo():\n",
    "    \"\"\"基础消息类型示例\"\"\"\n",
    "\n",
    "    # 初始化模型\n",
    "    model = ChatOllama(\n",
    "        base_url=\"http://localhost:11434\",\n",
    "        model=\"qwen2.5:3b\"\n",
    "    )\n",
    "\n",
    "    # 创建消息列表\n",
    "    messages = [\n",
    "        SystemMessage(content=\"你是一个专业的翻译助手\"),\n",
    "        HumanMessage(content=\"请将'Hello World'翻译成中文\"),\n",
    "    ]\n",
    "\n",
    "    # 调用模型\n",
    "    response = model.invoke(messages)\n",
    "    print(f\"翻译结果: {response.content}\")\n",
    "    #\n",
    "    # 添加历史消息并继续对话\n",
    "    messages.append(response)  # 添加AI回复到历史\n",
    "    messages.append(HumanMessage(content=\"再翻译'Good morning'\"))\n",
    "\n",
    "    # 再次调用模型\n",
    "    response = model.invoke(messages)\n",
    "    print(f\"第二次翻译: {response.content}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    basic_messages_demo()"
   ],
   "id": "7a5ea34a941c3050",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "翻译结果: \"Hello World\" 翻译成中文是 \"你好，世界\"。不过，“Hello World”在编程和开发领域通常被用作一个示例字符串或程序的一部分，用来说明基本的语法或功能。如果你需要的是这句话的意思，则可以理解为“你好，世界”。\n",
      "第二次翻译: 'Good morning' 翻译成中文是 '早上好'。\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "## 带额外参数的消息"
   ],
   "id": "629962baea6b3d58"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-22T08:13:07.363458Z",
     "start_time": "2025-07-22T08:13:06.538033Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "# 消息可以包含额外的参数，用于传递更多信息：\n",
    "\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "def messages_with_kwargs_demo():\n",
    "    \"\"\"带额外参数的消息示例\"\"\"\n",
    "\n",
    "    model = ChatOllama(\n",
    "        base_url=\"http://localhost:11434\",\n",
    "        model=\"qwen2.5:3b\"\n",
    "    )\n",
    "\n",
    "    # 创建带额外参数的消息\n",
    "    detailed_message = HumanMessage(\n",
    "        content=\"分析这段代码\",\n",
    "        additional_kwargs={\n",
    "            \"code\": \"def hello(): print('Hello')\",\n",
    "            \"language\": \"python\"\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # 调用模型\n",
    "    response = model.invoke([detailed_message])\n",
    "    print(f\"代码分析: {response.content}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    messages_with_kwargs_demo()\n"
   ],
   "id": "dfb2e5db4c4a3ec8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "代码分析: 请提供你想要我进行分析的代码片段。\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 消息类型与对话记忆",
   "id": "a7be6a8802abc78d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "## 消息类型与对话记忆\n",
    "\n",
    "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "def message_memory_demo():\n",
    "    \"\"\"消息与对话记忆示例\"\"\"\n",
    "\n",
    "    model = ChatOllama(\n",
    "        base_url=\"http://localhost:11434\",\n",
    "        model=\"qwen2.5:3b\"\n",
    "    )\n",
    "\n",
    "    # 创建聊天历史\n",
    "    chat_history = []\n",
    "\n",
    "    # 创建带消息占位符的提示模板\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        SystemMessage(content=\"你是一个友好的AI助手\"),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"human\", \"{input}\")\n",
    "    ])\n",
    "\n",
    "    # 第一轮对话\n",
    "    user_input = \"你好，我叫张三\"\n",
    "    messages = prompt.format_messages(chat_history=chat_history, input=user_input)\n",
    "    response = model.invoke(messages)\n",
    "\n",
    "    # 更新聊天历史\n",
    "    chat_history.append(HumanMessage(content=user_input))\n",
    "    chat_history.append(AIMessage(content=response.content))\n",
    "\n",
    "    print(f\"用户: {user_input}\")\n",
    "    print(f\"AI: {response.content}\")\n",
    "\n",
    "    # 第二轮对话\n",
    "    user_input = \"你还记得我的名字吗？\"\n",
    "    messages = prompt.format_messages(chat_history=chat_history, input=user_input)\n",
    "    response = model.invoke(messages)\n",
    "\n",
    "    print(f\"用户: {user_input}\")\n",
    "    print(f\"AI: {response.content}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    message_memory_demo()\n"
   ],
   "id": "1569f40d5e51ad67",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 工具消息示例",
   "id": "ef85c2c020819ec4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-22T08:14:23.497336Z",
     "start_time": "2025-07-22T08:14:22.960891Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "## 工具消息示例\n",
    "\n",
    "\n",
    "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage, ToolMessage\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "import datetime\n",
    "\n",
    "def tool_messages_demo():\n",
    "    \"\"\"工具消息示例\"\"\"\n",
    "\n",
    "    model = ChatOllama(\n",
    "        base_url=\"http://localhost:11434\",\n",
    "        model=\"qwen2.5:3b\"\n",
    "    )\n",
    "\n",
    "    # 模拟工具调用结果\n",
    "    def get_current_time():\n",
    "        return datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "    # 创建消息列表\n",
    "    messages = [\n",
    "        SystemMessage(content=\"你是一个有用的AI助手\"),\n",
    "        HumanMessage(content=\"现在几点了？\"),\n",
    "        # 模拟工具调用并返回结果\n",
    "        ToolMessage(\n",
    "            content=get_current_time(),\n",
    "            tool_call_id=\"time_tool_1\",\n",
    "            name=\"get_current_time\"\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    # 调用模型\n",
    "    response = model.invoke(messages)\n",
    "    print(f\"AI回复: {response.content}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    tool_messages_demo()\n"
   ],
   "id": "70d095cd06cdbcd2",
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Received unsupported message type for Ollama.",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mValueError\u001B[39m                                Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[5]\u001B[39m\u001B[32m, line 37\u001B[39m\n\u001B[32m     34\u001B[39m     \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mAI回复: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mresponse.content\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m)\n\u001B[32m     36\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[34m__name__\u001B[39m == \u001B[33m\"\u001B[39m\u001B[33m__main__\u001B[39m\u001B[33m\"\u001B[39m:\n\u001B[32m---> \u001B[39m\u001B[32m37\u001B[39m     \u001B[43mtool_messages_demo\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[5]\u001B[39m\u001B[32m, line 33\u001B[39m, in \u001B[36mtool_messages_demo\u001B[39m\u001B[34m()\u001B[39m\n\u001B[32m     21\u001B[39m messages = [\n\u001B[32m     22\u001B[39m     SystemMessage(content=\u001B[33m\"\u001B[39m\u001B[33m你是一个有用的AI助手\u001B[39m\u001B[33m\"\u001B[39m),\n\u001B[32m     23\u001B[39m     HumanMessage(content=\u001B[33m\"\u001B[39m\u001B[33m现在几点了？\u001B[39m\u001B[33m\"\u001B[39m),\n\u001B[32m   (...)\u001B[39m\u001B[32m     29\u001B[39m     )\n\u001B[32m     30\u001B[39m ]\n\u001B[32m     32\u001B[39m \u001B[38;5;66;03m# 调用模型\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m33\u001B[39m response = \u001B[43mmodel\u001B[49m\u001B[43m.\u001B[49m\u001B[43minvoke\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmessages\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     34\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mAI回复: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mresponse.content\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/FastAPIProject/.venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:378\u001B[39m, in \u001B[36mBaseChatModel.invoke\u001B[39m\u001B[34m(self, input, config, stop, **kwargs)\u001B[39m\n\u001B[32m    366\u001B[39m \u001B[38;5;129m@override\u001B[39m\n\u001B[32m    367\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34minvoke\u001B[39m(\n\u001B[32m    368\u001B[39m     \u001B[38;5;28mself\u001B[39m,\n\u001B[32m   (...)\u001B[39m\u001B[32m    373\u001B[39m     **kwargs: Any,\n\u001B[32m    374\u001B[39m ) -> BaseMessage:\n\u001B[32m    375\u001B[39m     config = ensure_config(config)\n\u001B[32m    376\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m cast(\n\u001B[32m    377\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mChatGeneration\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m--> \u001B[39m\u001B[32m378\u001B[39m         \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mgenerate_prompt\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    379\u001B[39m \u001B[43m            \u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_convert_input\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    380\u001B[39m \u001B[43m            \u001B[49m\u001B[43mstop\u001B[49m\u001B[43m=\u001B[49m\u001B[43mstop\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    381\u001B[39m \u001B[43m            \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[43m=\u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m.\u001B[49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mcallbacks\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    382\u001B[39m \u001B[43m            \u001B[49m\u001B[43mtags\u001B[49m\u001B[43m=\u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m.\u001B[49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mtags\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    383\u001B[39m \u001B[43m            \u001B[49m\u001B[43mmetadata\u001B[49m\u001B[43m=\u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m.\u001B[49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mmetadata\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    384\u001B[39m \u001B[43m            \u001B[49m\u001B[43mrun_name\u001B[49m\u001B[43m=\u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m.\u001B[49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mrun_name\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    385\u001B[39m \u001B[43m            \u001B[49m\u001B[43mrun_id\u001B[49m\u001B[43m=\u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m.\u001B[49m\u001B[43mpop\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mrun_id\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    386\u001B[39m \u001B[43m            \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    387\u001B[39m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m.generations[\u001B[32m0\u001B[39m][\u001B[32m0\u001B[39m],\n\u001B[32m    388\u001B[39m     ).message\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/FastAPIProject/.venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:963\u001B[39m, in \u001B[36mBaseChatModel.generate_prompt\u001B[39m\u001B[34m(self, prompts, stop, callbacks, **kwargs)\u001B[39m\n\u001B[32m    954\u001B[39m \u001B[38;5;129m@override\u001B[39m\n\u001B[32m    955\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mgenerate_prompt\u001B[39m(\n\u001B[32m    956\u001B[39m     \u001B[38;5;28mself\u001B[39m,\n\u001B[32m   (...)\u001B[39m\u001B[32m    960\u001B[39m     **kwargs: Any,\n\u001B[32m    961\u001B[39m ) -> LLMResult:\n\u001B[32m    962\u001B[39m     prompt_messages = [p.to_messages() \u001B[38;5;28;01mfor\u001B[39;00m p \u001B[38;5;129;01min\u001B[39;00m prompts]\n\u001B[32m--> \u001B[39m\u001B[32m963\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mgenerate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mprompt_messages\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstop\u001B[49m\u001B[43m=\u001B[49m\u001B[43mstop\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/FastAPIProject/.venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:782\u001B[39m, in \u001B[36mBaseChatModel.generate\u001B[39m\u001B[34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001B[39m\n\u001B[32m    779\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m i, m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(input_messages):\n\u001B[32m    780\u001B[39m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m    781\u001B[39m         results.append(\n\u001B[32m--> \u001B[39m\u001B[32m782\u001B[39m             \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_generate_with_cache\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    783\u001B[39m \u001B[43m                \u001B[49m\u001B[43mm\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    784\u001B[39m \u001B[43m                \u001B[49m\u001B[43mstop\u001B[49m\u001B[43m=\u001B[49m\u001B[43mstop\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    785\u001B[39m \u001B[43m                \u001B[49m\u001B[43mrun_manager\u001B[49m\u001B[43m=\u001B[49m\u001B[43mrun_managers\u001B[49m\u001B[43m[\u001B[49m\u001B[43mi\u001B[49m\u001B[43m]\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mrun_managers\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m    786\u001B[39m \u001B[43m                \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    787\u001B[39m \u001B[43m            \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    788\u001B[39m         )\n\u001B[32m    789\u001B[39m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[32m    790\u001B[39m         \u001B[38;5;28;01mif\u001B[39;00m run_managers:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/FastAPIProject/.venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:1028\u001B[39m, in \u001B[36mBaseChatModel._generate_with_cache\u001B[39m\u001B[34m(self, messages, stop, run_manager, **kwargs)\u001B[39m\n\u001B[32m   1026\u001B[39m     result = generate_from_stream(\u001B[38;5;28miter\u001B[39m(chunks))\n\u001B[32m   1027\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m inspect.signature(\u001B[38;5;28mself\u001B[39m._generate).parameters.get(\u001B[33m\"\u001B[39m\u001B[33mrun_manager\u001B[39m\u001B[33m\"\u001B[39m):\n\u001B[32m-> \u001B[39m\u001B[32m1028\u001B[39m     result = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_generate\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1029\u001B[39m \u001B[43m        \u001B[49m\u001B[43mmessages\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstop\u001B[49m\u001B[43m=\u001B[49m\u001B[43mstop\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrun_manager\u001B[49m\u001B[43m=\u001B[49m\u001B[43mrun_manager\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\n\u001B[32m   1030\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1031\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m   1032\u001B[39m     result = \u001B[38;5;28mself\u001B[39m._generate(messages, stop=stop, **kwargs)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/FastAPIProject/.venv/lib/python3.11/site-packages/langchain_community/chat_models/ollama.py:291\u001B[39m, in \u001B[36mChatOllama._generate\u001B[39m\u001B[34m(self, messages, stop, run_manager, **kwargs)\u001B[39m\n\u001B[32m    267\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m_generate\u001B[39m(\n\u001B[32m    268\u001B[39m     \u001B[38;5;28mself\u001B[39m,\n\u001B[32m    269\u001B[39m     messages: List[BaseMessage],\n\u001B[32m   (...)\u001B[39m\u001B[32m    272\u001B[39m     **kwargs: Any,\n\u001B[32m    273\u001B[39m ) -> ChatResult:\n\u001B[32m    274\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"Call out to Ollama's generate endpoint.\u001B[39;00m\n\u001B[32m    275\u001B[39m \n\u001B[32m    276\u001B[39m \u001B[33;03m    Args:\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m    288\u001B[39m \u001B[33;03m            ])\u001B[39;00m\n\u001B[32m    289\u001B[39m \u001B[33;03m    \"\"\"\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m291\u001B[39m     final_chunk = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_chat_stream_with_aggregation\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    292\u001B[39m \u001B[43m        \u001B[49m\u001B[43mmessages\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    293\u001B[39m \u001B[43m        \u001B[49m\u001B[43mstop\u001B[49m\u001B[43m=\u001B[49m\u001B[43mstop\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    294\u001B[39m \u001B[43m        \u001B[49m\u001B[43mrun_manager\u001B[49m\u001B[43m=\u001B[49m\u001B[43mrun_manager\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    295\u001B[39m \u001B[43m        \u001B[49m\u001B[43mverbose\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mverbose\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    296\u001B[39m \u001B[43m        \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    297\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    298\u001B[39m     chat_generation = ChatGeneration(\n\u001B[32m    299\u001B[39m         message=AIMessage(content=final_chunk.text),\n\u001B[32m    300\u001B[39m         generation_info=final_chunk.generation_info,\n\u001B[32m    301\u001B[39m     )\n\u001B[32m    302\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m ChatResult(generations=[chat_generation])\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/FastAPIProject/.venv/lib/python3.11/site-packages/langchain_community/chat_models/ollama.py:222\u001B[39m, in \u001B[36mChatOllama._chat_stream_with_aggregation\u001B[39m\u001B[34m(self, messages, stop, run_manager, verbose, **kwargs)\u001B[39m\n\u001B[32m    213\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m_chat_stream_with_aggregation\u001B[39m(\n\u001B[32m    214\u001B[39m     \u001B[38;5;28mself\u001B[39m,\n\u001B[32m    215\u001B[39m     messages: List[BaseMessage],\n\u001B[32m   (...)\u001B[39m\u001B[32m    219\u001B[39m     **kwargs: Any,\n\u001B[32m    220\u001B[39m ) -> ChatGenerationChunk:\n\u001B[32m    221\u001B[39m     final_chunk: Optional[ChatGenerationChunk] = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m222\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mstream_resp\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_create_chat_stream\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmessages\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstop\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\u001B[43m:\u001B[49m\n\u001B[32m    223\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mstream_resp\u001B[49m\u001B[43m:\u001B[49m\n\u001B[32m    224\u001B[39m \u001B[43m            \u001B[49m\u001B[43mchunk\u001B[49m\u001B[43m \u001B[49m\u001B[43m=\u001B[49m\u001B[43m \u001B[49m\u001B[43m_chat_stream_response_to_chat_generation_chunk\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstream_resp\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/FastAPIProject/.venv/lib/python3.11/site-packages/langchain_community/chat_models/ollama.py:192\u001B[39m, in \u001B[36mChatOllama._create_chat_stream\u001B[39m\u001B[34m(self, messages, stop, **kwargs)\u001B[39m\n\u001B[32m    184\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m_create_chat_stream\u001B[39m(\n\u001B[32m    185\u001B[39m     \u001B[38;5;28mself\u001B[39m,\n\u001B[32m    186\u001B[39m     messages: List[BaseMessage],\n\u001B[32m    187\u001B[39m     stop: Optional[List[\u001B[38;5;28mstr\u001B[39m]] = \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[32m    188\u001B[39m     **kwargs: Any,\n\u001B[32m    189\u001B[39m ) -> Iterator[\u001B[38;5;28mstr\u001B[39m]:\n\u001B[32m    190\u001B[39m     payload = {\n\u001B[32m    191\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mmodel\u001B[39m\u001B[33m\"\u001B[39m: \u001B[38;5;28mself\u001B[39m.model,\n\u001B[32m--> \u001B[39m\u001B[32m192\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mmessages\u001B[39m\u001B[33m\"\u001B[39m: \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_convert_messages_to_ollama_messages\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmessages\u001B[49m\u001B[43m)\u001B[49m,\n\u001B[32m    193\u001B[39m     }\n\u001B[32m    194\u001B[39m     \u001B[38;5;28;01myield from\u001B[39;00m \u001B[38;5;28mself\u001B[39m._create_stream(\n\u001B[32m    195\u001B[39m         payload=payload, stop=stop, api_url=\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m.base_url\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m/api/chat\u001B[39m\u001B[33m\"\u001B[39m, **kwargs\n\u001B[32m    196\u001B[39m     )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/FastAPIProject/.venv/lib/python3.11/site-packages/langchain_community/chat_models/ollama.py:134\u001B[39m, in \u001B[36mChatOllama._convert_messages_to_ollama_messages\u001B[39m\u001B[34m(self, messages)\u001B[39m\n\u001B[32m    132\u001B[39m     role = \u001B[33m\"\u001B[39m\u001B[33msystem\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    133\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m134\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[33m\"\u001B[39m\u001B[33mReceived unsupported message type for Ollama.\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m    136\u001B[39m content = \u001B[33m\"\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    137\u001B[39m images = []\n",
      "\u001B[31mValueError\u001B[39m: Received unsupported message type for Ollama."
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "## 多模态消息示例"
   ],
   "id": "e9314c85d8604b7d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "\n",
    "def multimodal_messages_demo():\n",
    "    \"\"\"多模态消息示例 (需要支持多模态的模型)\"\"\"\n",
    "\n",
    "    # 注意：需要使用支持多模态的模型，如 llava 或 qwen-vl\n",
    "    model = ChatOllama(\n",
    "        base_url=\"http://localhost:11434\",\n",
    "        model=\"llava:7b-v1.6-mistral-q4_0\"  # 确保已下载此模型\n",
    "    )\n",
    "\n",
    "    # 创建带图像URL的消息\n",
    "    # 注意：实际使用时需要替换为真实可访问的图像URL\n",
    "    image_message = HumanMessage(\n",
    "        content=[\n",
    "            {\n",
    "                \"type\": \"text\",\n",
    "                \"text\": \"这张图片里有什么？\"\n",
    "            },\n",
    "            {\n",
    "                \"type\": \"image_url\",\n",
    "                \"image_url\": {\n",
    "                    \"url\": \"https://example.com/image.jpg\"  # 替换为实际图像URL\n",
    "                }\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # 如果有支持多模态的模型，可以取消下面的注释\n",
    "    # response = model.invoke([image_message])\n",
    "    # print(f\"图像分析: {response.content}\")\n",
    "\n",
    "    print(\"注意：多模态消息需要支持多模态的模型，如llava或qwen-vl\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 取消注释以运行多模态示例\n",
    "    # multimodal_messages_demo()\n",
    "    pass"
   ],
   "id": "3b3055250b9a8d8c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "## 消息流式处理"
   ],
   "id": "81514c28a2b749bd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "\n",
    "def streaming_messages_demo():\n",
    "    \"\"\"消息流式处理示例\"\"\"\n",
    "\n",
    "    model = ChatOllama(\n",
    "        base_url=\"http://localhost:11434\",\n",
    "        model=\"qwen2.5:3b\"\n",
    "    )\n",
    "\n",
    "    # 创建消息\n",
    "    messages = [HumanMessage(content=\"写一首关于人工智能的短诗\")]\n",
    "\n",
    "    # 流式调用\n",
    "    print(\"开始流式生成...\")\n",
    "    for chunk in model.stream(messages):\n",
    "        # 打印每个生成的块\n",
    "        print(chunk.content, end=\"\", flush=True)\n",
    "    print(\"\\n流式生成完成\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    streaming_messages_demo()\n"
   ],
   "id": "26992b7ee821e53b",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "## 消息与LangGraph结合\n",
    "\n",
    "LangGraph是LangChain 0.3推荐的对话管理方式，可以更好地处理消息和状态："
   ],
   "id": "d450895bed9cc590"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langgraph.graph import StateGraph, MessagesState\n",
    "from langgraph.graph import START, END\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "import uuid\n",
    "\n",
    "def langgraph_messages_demo():\n",
    "    \"\"\"LangGraph消息处理示例\"\"\"\n",
    "\n",
    "    model = ChatOllama(\n",
    "        base_url=\"http://localhost:11434\",\n",
    "        model=\"qwen2.5:3b\"\n",
    "    )\n",
    "\n",
    "    # 定义模型调用函数\n",
    "    def call_model(state: MessagesState):\n",
    "        \"\"\"调用模型并返回响应\"\"\"\n",
    "        system_msg = SystemMessage(content=\"你是一个友好的AI助手，能够记住对话历史。\")\n",
    "        messages = [system_msg] + state[\"messages\"]\n",
    "        response = model.invoke(messages)\n",
    "        return {\"messages\": [response]}\n",
    "\n",
    "    # 创建状态图\n",
    "    builder = StateGraph(MessagesState)\n",
    "    builder.add_node(\"call_model\", call_model)\n",
    "    builder.add_edge(START, \"call_model\")\n",
    "    builder.add_edge(\"call_model\", END)\n",
    "\n",
    "    # 编译图并添加内存检查点\n",
    "    memory = MemorySaver()\n",
    "    graph = builder.compile(checkpointer=memory)\n",
    "\n",
    "    # 创建会话配置\n",
    "    config = {\"configurable\": {\"thread_id\": str(uuid.uuid4())}}\n",
    "\n",
    "    # 第一轮对话\n",
    "    print(\"\\n第1轮对话:\")\n",
    "    user_input = \"你好，我叫张三，是一名程序员\"\n",
    "    print(f\"用户: {user_input}\")\n",
    "\n",
    "    result = graph.invoke(\n",
    "        {\"messages\": [HumanMessage(content=user_input)]},\n",
    "        config\n",
    "    )\n",
    "    print(f\"AI: {result['messages'][-1].content}\")\n",
    "\n",
    "    # 第二轮对话\n",
    "    print(\"\\n第2轮对话:\")\n",
    "    user_input = \"你还记得我是做什么的吗？\"\n",
    "    print(f\"用户: {user_input}\")\n",
    "\n",
    "    result = graph.invoke(\n",
    "        {\"messages\": [HumanMessage(content=user_input)]},\n",
    "        config\n",
    "    )\n",
    "    print(f\"AI: {result['messages'][-1].content}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    langgraph_messages_demo()\n"
   ],
   "id": "b15b32ed3ccefde",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "## 总结\n",
    "\n",
    "LangChain 0.3 的消息处理系统提供了：\n",
    "\n",
    "1. **统一接口** - 不同模型提供商使用相同的消息格式\n",
    "2. **丰富的消息类型** - 支持人类、AI、系统、工具等多种消息类型\n",
    "3. **额外参数支持** - 可以添加额外信息到消息中\n",
    "4. **流式处理** - 支持实时生成和显示\n",
    "5. **与LangGraph集成** - 更好地管理对话状态和记忆\n",
    "\n",
    "通过这些消息类型，你可以构建从简单对话到复杂多轮交互的各种应用。\n"
   ],
   "id": "af9093c24674010b"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
