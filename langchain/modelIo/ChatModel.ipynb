{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Chat Models - 聊天模型接口详解",
   "id": "a3f72d5841f5e121"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "## 概述\n",
    "\n",
    "Chat Models 是 LangChain 中用于与聊天模型交互的核心接口。它们实现了 `BaseChatModel` 接口，同时也实现了 `Runnable` 接口，支持标准的流式处理、批量处理等功能。\n",
    "\n",
    "### 核心特性\n",
    "\n",
    "- **消息格式**: 接受消息列表作为输入，返回消息作为输出\n",
    "- **角色支持**: 支持 `system`、`human`、`assistant` 等角色\n",
    "- **流式处理**: 支持实时流式输出\n",
    "- **批量处理**: 支持批量请求优化\n",
    "- **工具绑定**: 支持绑定外部工具\n",
    "- **结构化输出**: 支持结构化数据输出\n",
    "\n",
    "### 关键方法\n",
    "\n",
    "1. **`invoke`** - 主要交互方法\n",
    "2. **`stream`** - 流式输出\n",
    "3. **`batch`** - 批量处理\n",
    "4. **`bindTools`** - 绑定工具\n",
    "5. **`withStructuredOutput`** - 结构化输出"
   ],
   "id": "e23d243aa5458ed1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## LangChain Chat Models 完整示例",
   "id": "67301fb4920a61d2"
  },
  {
   "cell_type": "code",
   "id": "1cd0d466b70d6607",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-22T08:05:26.936911Z",
     "start_time": "2025-07-22T08:05:26.114561Z"
    }
   },
   "source": [
    "\"\"\"\n",
    "LangChain 0.3 Chat Models 完整示例\n",
    "\"\"\"\n",
    "# 引入依赖\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from typing import List, Dict, Any\n",
    "import asyncio"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "### 1. 基础聊天模型初始化"
   ],
   "id": "d831cfc9dde7ffd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-22T08:06:16.355203Z",
     "start_time": "2025-07-22T08:06:04.623381Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "def basic_chat_model():\n",
    "    \"\"\"基础聊天模型使用\"\"\"\n",
    "    print(\"=\" * 50)\n",
    "    print(\"1. 基础聊天模型使用\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    # 初始化模型\n",
    "    model = ChatOllama(\n",
    "        base_url=\"http://localhost:11434\",\n",
    "        model=\"qwen2.5:3b\",\n",
    "        temperature=0.7\n",
    "    )\n",
    "\n",
    "    # 单条消息\n",
    "    response = model.invoke([HumanMessage(content=\"你好，介绍一下自己\")])\n",
    "    print(f\"AI回复: {response.content}\")\n",
    "    print(f\"响应元数据: {response.response_metadata}\")\n",
    "\n",
    "    # 多轮对话\n",
    "    messages = [\n",
    "        SystemMessage(content=\"你是一个友好的AI助手\"),\n",
    "        HumanMessage(content=\"我叫张三\"),\n",
    "        AIMessage(content=\"你好张三！很高兴认识你。\"),\n",
    "        HumanMessage(content=\"我的名字是什么？\")\n",
    "    ]\n",
    "\n",
    "    response = model.invoke(messages)\n",
    "    print(f\"\\n多轮对话回复: {response.content}\")\n",
    "basic_chat_model()"
   ],
   "id": "576910155ed9161f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "1. 基础聊天模型使用\n",
      "==================================================\n",
      "AI回复: 你好！我叫Qwen，是阿里巴巴推出的一个预训练模型。我是由阿里云研发的一系列超大规模语言模型之一，旨在通过理解和模拟人类语言来提供帮助和支持。\n",
      "\n",
      "作为AI助手，我的能力包括但不限于回答问题、讲故事、提供建议和解决问题等。我可以生成文本内容，比如撰写故事、新闻文章或诗歌。此外，我也能参与对话并理解复杂的指令和幽默感。\n",
      "\n",
      "我被设计为在多种情境下都能提供有用的信息，并且随着更多的训练数据和支持系统的增强，我能够更好地理解和回应人类的需求。不过，请注意，尽管我在许多任务上都可以提供帮助，但在某些专业领域可能无法达到专家水平。总的来说，我的目标是成为一个友好、可靠并且尽可能满足用户需求的伙伴。\n",
      "响应元数据: {'model': 'qwen2.5:3b', 'created_at': '2025-07-22T08:06:14.515785Z', 'done': True, 'done_reason': 'stop', 'total_duration': 9856646869, 'load_duration': 41191128, 'prompt_eval_count': 33, 'prompt_eval_duration': 369625763, 'eval_count': 159, 'eval_duration': 9441715871, 'model_name': 'qwen2.5:3b'}\n",
      "\n",
      "多轮对话回复: 你的名字是张三。\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 2. 流式处理示例",
   "id": "85f649f7d761f0b4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-22T06:23:42.746084Z",
     "start_time": "2025-07-22T06:23:37.524449Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 2. 流式处理示例\n",
    "def streaming_chat():\n",
    "    \"\"\"流式处理示例\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"2. 流式处理示例\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    model = ChatOllama(\n",
    "        base_url=\"http://localhost:11434\",\n",
    "        model=\"qwen2.5:3b\"\n",
    "    )\n",
    "\n",
    "    messages = [HumanMessage(content=\"请写一首关于春天的诗\")]\n",
    "\n",
    "    print(\"流式输出:\")\n",
    "    for chunk in model.stream(messages):\n",
    "        print(chunk.content, end=\"\", flush=True)\n",
    "    print(\"\\n\")\n",
    "streaming_chat()"
   ],
   "id": "4f4c5c21e9a14c5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "2. 流式处理示例\n",
      "==================================================\n",
      "流式输出:\n",
      "春风如故人，轻轻绕我身。\n",
      "万物皆复苏，欣欣向阳新。\n",
      "\n",
      "桃花笑东风，柳絮舞轻尘。\n",
      "人间四月天，春色正浓时。 \n",
      "\n",
      "此情此景里，心自安处静。\n",
      "只愿时光长，共赏这美好。\n",
      "\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 3. 批量处理示例",
   "id": "c877396ee2c34a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-22T06:24:45.060530Z",
     "start_time": "2025-07-22T06:24:09.226897Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 3. 批量处理示例\n",
    "def batch_processing():\n",
    "    \"\"\"批量处理示例\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"3. 批量处理示例\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    model = ChatOllama(\n",
    "        base_url=\"http://localhost:11434\",\n",
    "        model=\"qwen2.5:3b\"\n",
    "    )\n",
    "\n",
    "    # 批量请求\n",
    "    batch_messages = [\n",
    "        [HumanMessage(content=\"1+1等于多少？\")],\n",
    "        [HumanMessage(content=\"Python是什么？\")],\n",
    "        [HumanMessage(content=\"解释一下机器学习\")]\n",
    "    ]\n",
    "\n",
    "    responses = model.batch(batch_messages)\n",
    "\n",
    "    for i, response in enumerate(responses):\n",
    "        print(f\"问题{i+1}回复: {response.content[:50]}...\")\n",
    "batch_processing()"
   ],
   "id": "a1727544d38c295b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "3. 批量处理示例\n",
      "==================================================\n",
      "问题1回复: 1+1等于2。...\n",
      "问题2回复: Python是一种高级编程语言，由吉米·赫茨和吉尔伯特·格雷在1989年发明。Python的设计哲学...\n",
      "问题3回复: 机器学习是一种人工智能的子领域，它的目标是设计和开发算法和统计模型，使计算机能够从数据中自动获取知识...\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 4. 使用提示模板",
   "id": "fccc096c4dc020c5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "# 4. 使用提示模板\n",
    "def with_prompt_template():\n",
    "    \"\"\"使用提示模板\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"4. 使用提示模板\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    model = ChatOllama(\n",
    "        base_url=\"http://localhost:11434\",\n",
    "        model=\"qwen2.5:3b\"\n",
    "    )\n",
    "\n",
    "    # 创建提示模板\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"你是一个{role}，请用{style}的风格回答问题\"),\n",
    "        (\"human\", \"{question}\")\n",
    "    ])\n",
    "\n",
    "    # 创建链\n",
    "    chain = prompt | model | StrOutputParser()\n",
    "\n",
    "    # 调用\n",
    "    response = chain.invoke({\n",
    "        \"role\": \"诗人\",\n",
    "        \"style\": \"古典\",\n",
    "        \"question\": \"描述一下秋天的景色\"\n",
    "    })\n",
    "\n",
    "    print(f\"诗人回复: {response}\")\n",
    "with_prompt_template()"
   ],
   "id": "bd4ead17d8f0e2c8",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 5. 带记忆的对话",
   "id": "b8bddde609cf39f5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "# 5. 带记忆的对话\n",
    "def conversation_with_memory():\n",
    "    \"\"\"带记忆的对话\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"5. 带记忆的对话\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    model = ChatOllama(\n",
    "        base_url=\"http://localhost:11434\",\n",
    "        model=\"qwen2.5:3b\"\n",
    "    )\n",
    "\n",
    "    # 对话历史\n",
    "    conversation_history = []\n",
    "\n",
    "    def chat_with_memory(user_input: str) -> str:\n",
    "        # 添加用户消息\n",
    "        conversation_history.append(HumanMessage(content=user_input))\n",
    "\n",
    "        # 调用模型\n",
    "        response = model.invoke(conversation_history)\n",
    "\n",
    "        # 添加AI回复到历史\n",
    "        conversation_history.append(response)\n",
    "\n",
    "        return response.content\n",
    "\n",
    "    # 模拟对话\n",
    "    print(\"用户: 我是一名程序员\")\n",
    "    ai_response = chat_with_memory(\"我是一名程序员\")\n",
    "    print(f\"AI: {ai_response}\")\n",
    "\n",
    "    print(\"\\n用户: 我主要使用什么编程语言？\")\n",
    "    ai_response = chat_with_memory(\"我主要使用什么编程语言？\")\n",
    "    print(f\"AI: {ai_response}\")\n",
    "conversation_with_memory()"
   ],
   "id": "17a327e3b50433d2",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 6. 异步处理示例",
   "id": "8a425d832875d340"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "# 6. 异步处理示例\n",
    "async def async_chat():\n",
    "    \"\"\"异步处理示例\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"6. 异步处理示例\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    model = ChatOllama(\n",
    "        base_url=\"http://localhost:11434\",\n",
    "        model=\"qwen2.5:3b\"\n",
    "    )\n",
    "\n",
    "    # 异步调用\n",
    "    response = await model.ainvoke([\n",
    "        HumanMessage(content=\"异步调用测试\")\n",
    "    ])\n",
    "    print(f\"异步回复: {response.content}\")\n",
    "\n",
    "    # 异步流式处理\n",
    "    print(\"\\n异步流式输出:\")\n",
    "    async for chunk in model.astream([\n",
    "        HumanMessage(content=\"请简单介绍Python\")\n",
    "    ]):\n",
    "        print(chunk.content, end=\"\", flush=True)\n",
    "    print(\"\\n\")"
   ],
   "id": "bd48250fbcf8d169",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 7. 多模型比较",
   "id": "e0b24d99ef2af45"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "# 7. 多模型比较\n",
    "def multi_model_comparison():\n",
    "    \"\"\"多模型比较\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"7. 多模型比较\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    # 不同模型\n",
    "    models = {\n",
    "        \"qwen2.5:3b\": ChatOllama(base_url=\"http://localhost:11434\", model=\"qwen2.5:3b\"),\n",
    "        \"qwen2.5:0.5b\": ChatOllama(base_url=\"http://localhost:11434\", model=\"qwen2.5:0.5b\")\n",
    "    }\n",
    "\n",
    "    question = [HumanMessage(content=\"什么是人工智能？\")]\n",
    "\n",
    "    for model_name, model in models.items():\n",
    "        try:\n",
    "            response = model.invoke(question)\n",
    "            print(f\"\\n{model_name}回复:\")\n",
    "            print(response.content[:100] + \"...\")\n",
    "        except Exception as e:\n",
    "            print(f\"{model_name}调用失败: {e}\")"
   ],
   "id": "3aafbc82945e38d7",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 8. 带参数配置的模型",
   "id": "d88c290d9d86b8f6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "# 8. 带参数配置的模型\n",
    "def model_with_parameters():\n",
    "    \"\"\"带参数配置的模型\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"8. 带参数配置的模型\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    # 不同温度设置\n",
    "    temperatures = [0.1, 0.7, 1.0]\n",
    "\n",
    "    for temp in temperatures:\n",
    "        model = ChatOllama(\n",
    "            base_url=\"http://localhost:11434\",\n",
    "            model=\"qwen2.5:3b\",\n",
    "            temperature=temp\n",
    "        )\n",
    "\n",
    "        response = model.invoke([\n",
    "            HumanMessage(content=\"创造一个有趣的故事开头\")\n",
    "        ])\n",
    "\n",
    "        print(f\"\\n温度{temp}的回复:\")\n",
    "        print(response.content[:80] + \"...\")"
   ],
   "id": "8bbc26c00b66d51f",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 9. 错误处理示例",
   "id": "65ad42af617a3c93"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "# 9. 错误处理示例\n",
    "def error_handling():\n",
    "    \"\"\"错误处理示例\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"9. 错误处理示例\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    model = ChatOllama(\n",
    "        base_url=\"http://localhost:11434\",\n",
    "        model=\"qwen2.5:3b\"\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        # 正常调用\n",
    "        response = model.invoke([HumanMessage(content=\"测试\")])\n",
    "        print(f\"成功: {response.content[:50]}...\")\n",
    "\n",
    "        # 模拟错误（使用不存在的模型）\n",
    "        error_model = ChatOllama(\n",
    "            base_url=\"http://localhost:11434\",\n",
    "            model=\"nonexistent-model\"\n",
    "        )\n",
    "        error_response = error_model.invoke([HumanMessage(content=\"测试\")])\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"捕获错误: {type(e).__name__}: {e}\")"
   ],
   "id": "6d1c5ccea91fd71e",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 10. 自定义输出解析",
   "id": "76c9d60c2a971f48"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "# 10. 自定义输出解析\n",
    "def custom_output_parsing():\n",
    "    \"\"\"自定义输出解析\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"10. 自定义输出解析\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    model = ChatOllama(\n",
    "        base_url=\"http://localhost:11434\",\n",
    "        model=\"qwen2.5:3b\"\n",
    "    )\n",
    "\n",
    "    # 自定义解析器\n",
    "    class CustomParser:\n",
    "        def parse(self, text: str) -> Dict[str, Any]:\n",
    "            return {\n",
    "                \"content\": text,\n",
    "                \"length\": len(text),\n",
    "                \"word_count\": len(text.split())\n",
    "            }\n",
    "\n",
    "    parser = CustomParser()\n",
    "\n",
    "    # 创建链\n",
    "    def custom_chain(input_text: str):\n",
    "        response = model.invoke([HumanMessage(content=input_text)])\n",
    "        return parser.parse(response.content)\n",
    "\n",
    "    result = custom_chain(\"请简单介绍一下自己\")\n",
    "    print(f\"解析结果: {result}\")"
   ],
   "id": "f961a97cae7d7e01",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChain 0.3 Chat Models 完整示例\n",
      "============================================================\n",
      "==================================================\n",
      "1. 基础聊天模型使用\n",
      "==================================================\n",
      "AI回复: 你好！我叫Qwen，是阿里云开发的一款超大规模语言模型。我是通过深度学习技术训练而成的，可以生成各种类型的文本内容，比如故事、诗歌、新闻报道等，还能回答关于历史、地理、科学和各种主题的问题。此外，我也能够帮助你完成许多任务，例如编写邮件、制作策划方案等。我还可以根据你的需求创作音乐或艺术作品。\n",
      "\n",
      "总之，我的目的是尽可能多地了解世界，并以多种方式与你进行互动，以便更好地服务您。不过，请注意虽然我能生成多样化的文本内容，但我并不具备任何超越自身功能的超能力。同时，我也能保护用户的隐私和安全，确保所有的交流都是在合法、合规的前提下进行的。\n",
      "\n",
      "如果您有任何问题或需要帮助的地方，欢迎随时提问。\n",
      "响应元数据: {'model': 'qwen2.5:3b', 'created_at': '2025-07-22T06:19:28.561232Z', 'message': {'role': 'assistant', 'content': ''}, 'done_reason': 'stop', 'done': True, 'total_duration': 11466729520, 'load_duration': 35680223, 'prompt_eval_count': 33, 'prompt_eval_duration': 325998399, 'eval_count': 164, 'eval_duration': 11104060353}\n",
      "\n",
      "多轮对话回复: 你的名字是张三。有什么我可以帮助你的吗？\n",
      "\n",
      "==================================================\n",
      "2. 流式处理示例\n",
      "==================================================\n",
      "流式输出:\n",
      "春风轻拂绿柳枝，  \n",
      "细雨绵绵润芳菲。  \n",
      "桃花笑开映朝霞，  \n",
      "万物欣欣向荣华。\n",
      "\n",
      "鸟鸣山谷间悠扬，  \n",
      "蝴蝶翩飞花丛中。  \n",
      "春光明媚驱寒去，  \n",
      "人间处处是画卷。\n",
      "\n",
      "\n",
      "==================================================\n",
      "3. 批量处理示例\n",
      "==================================================\n",
      "问题1回复: 1+1等于2。...\n",
      "问题2回复: Python是一种高级编程语言，由Guido van Rossum在1980年代末开发，并于1991...\n",
      "问题3回复: 机器学习是一种人工智能技术，它使计算机能够在不进行明确编程的情况下从数据中自动学习并改进其性能。换句...\n",
      "\n",
      "==================================================\n",
      "4. 使用提示模板\n",
      "==================================================\n",
      "诗人回复: 秋光染红枫叶，黄花遍地铺。\n",
      "残阳如血照千里，清风徐来送爽初。\n",
      "\n",
      "==================================================\n",
      "5. 带记忆的对话\n",
      "==================================================\n",
      "用户: 我是一名程序员\n",
      "AI: 很高兴您是一名程序员！如果您在编程过程中遇到任何问题或需要帮助，无论是技术难题还是学习资料查找，我都愿意尽力协助您。此外，如果您有任何关于云计算服务的问题，也欢迎随时向我提问。无论您的需求是什么，请告诉我更多细节，我会尽可能提供支持和建议。\n",
      "\n",
      "用户: 我主要使用什么编程语言？\n",
      "AI: 这取决于您的具体工作领域和个人偏好。不同的编程语言适用于不同类型的应用程序和开发任务。以下是一些常见的编程语言及其适用场景：\n",
      "\n",
      "1. **Python**：广泛用于数据科学、人工智能、机器学习、网络爬虫开发等。\n",
      "2. **JavaScript**（与Node.js一起）：前端网页交互，以及后端开发的 Node.js 环境中非常流行。\n",
      "3. **Java**：常用于企业级应用和Android应用程序开发。\n",
      "4. **C++** 或 **C#**：通常用于游戏开发、系统软件和高性能计算等需要高效率的应用程序。\n",
      "5. **Go (Golang)**：简单易用，适用于后端开发及云服务构建。\n",
      "6. **Ruby**：与Ruby on Rails框架结合使用时非常流行。\n",
      "7. **Swift**：苹果公司推荐用于iOS和macOS应用开发。\n",
      "\n",
      "选择哪种编程语言取决于您的项目需求、团队技能以及您希望实现的功能。如果您有具体领域的需求或正在考虑转型到某种编程语言，欢迎告诉我更多细节！\n",
      "\n",
      "==================================================\n",
      "7. 多模型比较\n",
      "==================================================\n",
      "\n",
      "qwen2.5:3b回复:\n",
      "人工智能（Artificial Intelligence，简称AI）是一种使机器能够执行通常需要人类智能的任务的技术。这意味着让计算机系统具备理解环境、学习新知识以及根据这些知识做出决策的能力。\n",
      "\n",
      "从...\n",
      "\n",
      "qwen2.5:0.5b回复:\n",
      "人工智能（Artificial Intelligence，简称AI）是指由计算机系统所表现出来的智能。这不仅仅是机器的智能，还包含人类的智能、人的能力以及复杂的人类行为。\n",
      "\n",
      "简要来说，人工智能是一种模...\n",
      "\n",
      "==================================================\n",
      "8. 带参数配置的模型\n",
      "==================================================\n",
      "\n",
      "温度0.1的回复:\n",
      "在一个遥远的星球上，有一片被彩虹色云朵环绕的神秘森林。这里的树木会唱歌，花朵能说话，而最神奇的是，每当满月之夜，整个森林都会变成一片璀璨的星空乐园。在这个故事里...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[2]\u001B[39m\u001B[32m, line 304\u001B[39m\n\u001B[32m    301\u001B[39m     asyncio.run(async_chat())\n\u001B[32m    303\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[34m__name__\u001B[39m == \u001B[33m\"\u001B[39m\u001B[33m__main__\u001B[39m\u001B[33m\"\u001B[39m:\n\u001B[32m--> \u001B[39m\u001B[32m304\u001B[39m     \u001B[43mmain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[2]\u001B[39m\u001B[32m, line 295\u001B[39m, in \u001B[36mmain\u001B[39m\u001B[34m()\u001B[39m\n\u001B[32m    293\u001B[39m conversation_with_memory()\n\u001B[32m    294\u001B[39m multi_model_comparison()\n\u001B[32m--> \u001B[39m\u001B[32m295\u001B[39m \u001B[43mmodel_with_parameters\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    296\u001B[39m error_handling()\n\u001B[32m    297\u001B[39m custom_output_parsing()\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[2]\u001B[39m\u001B[32m, line 217\u001B[39m, in \u001B[36mmodel_with_parameters\u001B[39m\u001B[34m()\u001B[39m\n\u001B[32m    210\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m temp \u001B[38;5;129;01min\u001B[39;00m temperatures:\n\u001B[32m    211\u001B[39m     model = ChatOllama(\n\u001B[32m    212\u001B[39m         base_url=\u001B[33m\"\u001B[39m\u001B[33mhttp://localhost:11434\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m    213\u001B[39m         model=\u001B[33m\"\u001B[39m\u001B[33mqwen2.5:3b\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m    214\u001B[39m         temperature=temp\n\u001B[32m    215\u001B[39m     )\n\u001B[32m--> \u001B[39m\u001B[32m217\u001B[39m     response = \u001B[43mmodel\u001B[49m\u001B[43m.\u001B[49m\u001B[43minvoke\u001B[49m\u001B[43m(\u001B[49m\u001B[43m[\u001B[49m\n\u001B[32m    218\u001B[39m \u001B[43m        \u001B[49m\u001B[43mHumanMessage\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcontent\u001B[49m\u001B[43m=\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43m创造一个有趣的故事开头\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[32m    219\u001B[39m \u001B[43m    \u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    221\u001B[39m     \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m温度\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtemp\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m的回复:\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m    222\u001B[39m     \u001B[38;5;28mprint\u001B[39m(response.content[:\u001B[32m80\u001B[39m] + \u001B[33m\"\u001B[39m\u001B[33m...\u001B[39m\u001B[33m\"\u001B[39m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/FastAPIProject/.venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:378\u001B[39m, in \u001B[36mBaseChatModel.invoke\u001B[39m\u001B[34m(self, input, config, stop, **kwargs)\u001B[39m\n\u001B[32m    366\u001B[39m \u001B[38;5;129m@override\u001B[39m\n\u001B[32m    367\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34minvoke\u001B[39m(\n\u001B[32m    368\u001B[39m     \u001B[38;5;28mself\u001B[39m,\n\u001B[32m   (...)\u001B[39m\u001B[32m    373\u001B[39m     **kwargs: Any,\n\u001B[32m    374\u001B[39m ) -> BaseMessage:\n\u001B[32m    375\u001B[39m     config = ensure_config(config)\n\u001B[32m    376\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m cast(\n\u001B[32m    377\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mChatGeneration\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m--> \u001B[39m\u001B[32m378\u001B[39m         \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mgenerate_prompt\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    379\u001B[39m \u001B[43m            \u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_convert_input\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    380\u001B[39m \u001B[43m            \u001B[49m\u001B[43mstop\u001B[49m\u001B[43m=\u001B[49m\u001B[43mstop\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    381\u001B[39m \u001B[43m            \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[43m=\u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m.\u001B[49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mcallbacks\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    382\u001B[39m \u001B[43m            \u001B[49m\u001B[43mtags\u001B[49m\u001B[43m=\u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m.\u001B[49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mtags\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    383\u001B[39m \u001B[43m            \u001B[49m\u001B[43mmetadata\u001B[49m\u001B[43m=\u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m.\u001B[49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mmetadata\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    384\u001B[39m \u001B[43m            \u001B[49m\u001B[43mrun_name\u001B[49m\u001B[43m=\u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m.\u001B[49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mrun_name\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    385\u001B[39m \u001B[43m            \u001B[49m\u001B[43mrun_id\u001B[49m\u001B[43m=\u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m.\u001B[49m\u001B[43mpop\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mrun_id\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    386\u001B[39m \u001B[43m            \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    387\u001B[39m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m.generations[\u001B[32m0\u001B[39m][\u001B[32m0\u001B[39m],\n\u001B[32m    388\u001B[39m     ).message\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/FastAPIProject/.venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:963\u001B[39m, in \u001B[36mBaseChatModel.generate_prompt\u001B[39m\u001B[34m(self, prompts, stop, callbacks, **kwargs)\u001B[39m\n\u001B[32m    954\u001B[39m \u001B[38;5;129m@override\u001B[39m\n\u001B[32m    955\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mgenerate_prompt\u001B[39m(\n\u001B[32m    956\u001B[39m     \u001B[38;5;28mself\u001B[39m,\n\u001B[32m   (...)\u001B[39m\u001B[32m    960\u001B[39m     **kwargs: Any,\n\u001B[32m    961\u001B[39m ) -> LLMResult:\n\u001B[32m    962\u001B[39m     prompt_messages = [p.to_messages() \u001B[38;5;28;01mfor\u001B[39;00m p \u001B[38;5;129;01min\u001B[39;00m prompts]\n\u001B[32m--> \u001B[39m\u001B[32m963\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mgenerate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mprompt_messages\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstop\u001B[49m\u001B[43m=\u001B[49m\u001B[43mstop\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/FastAPIProject/.venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:782\u001B[39m, in \u001B[36mBaseChatModel.generate\u001B[39m\u001B[34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001B[39m\n\u001B[32m    779\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m i, m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(input_messages):\n\u001B[32m    780\u001B[39m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m    781\u001B[39m         results.append(\n\u001B[32m--> \u001B[39m\u001B[32m782\u001B[39m             \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_generate_with_cache\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    783\u001B[39m \u001B[43m                \u001B[49m\u001B[43mm\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    784\u001B[39m \u001B[43m                \u001B[49m\u001B[43mstop\u001B[49m\u001B[43m=\u001B[49m\u001B[43mstop\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    785\u001B[39m \u001B[43m                \u001B[49m\u001B[43mrun_manager\u001B[49m\u001B[43m=\u001B[49m\u001B[43mrun_managers\u001B[49m\u001B[43m[\u001B[49m\u001B[43mi\u001B[49m\u001B[43m]\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mrun_managers\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m    786\u001B[39m \u001B[43m                \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    787\u001B[39m \u001B[43m            \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    788\u001B[39m         )\n\u001B[32m    789\u001B[39m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[32m    790\u001B[39m         \u001B[38;5;28;01mif\u001B[39;00m run_managers:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/FastAPIProject/.venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:1028\u001B[39m, in \u001B[36mBaseChatModel._generate_with_cache\u001B[39m\u001B[34m(self, messages, stop, run_manager, **kwargs)\u001B[39m\n\u001B[32m   1026\u001B[39m     result = generate_from_stream(\u001B[38;5;28miter\u001B[39m(chunks))\n\u001B[32m   1027\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m inspect.signature(\u001B[38;5;28mself\u001B[39m._generate).parameters.get(\u001B[33m\"\u001B[39m\u001B[33mrun_manager\u001B[39m\u001B[33m\"\u001B[39m):\n\u001B[32m-> \u001B[39m\u001B[32m1028\u001B[39m     result = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_generate\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1029\u001B[39m \u001B[43m        \u001B[49m\u001B[43mmessages\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstop\u001B[49m\u001B[43m=\u001B[49m\u001B[43mstop\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrun_manager\u001B[49m\u001B[43m=\u001B[49m\u001B[43mrun_manager\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\n\u001B[32m   1030\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1031\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m   1032\u001B[39m     result = \u001B[38;5;28mself\u001B[39m._generate(messages, stop=stop, **kwargs)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/FastAPIProject/.venv/lib/python3.11/site-packages/langchain_community/chat_models/ollama.py:291\u001B[39m, in \u001B[36mChatOllama._generate\u001B[39m\u001B[34m(self, messages, stop, run_manager, **kwargs)\u001B[39m\n\u001B[32m    267\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m_generate\u001B[39m(\n\u001B[32m    268\u001B[39m     \u001B[38;5;28mself\u001B[39m,\n\u001B[32m    269\u001B[39m     messages: List[BaseMessage],\n\u001B[32m   (...)\u001B[39m\u001B[32m    272\u001B[39m     **kwargs: Any,\n\u001B[32m    273\u001B[39m ) -> ChatResult:\n\u001B[32m    274\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"Call out to Ollama's generate endpoint.\u001B[39;00m\n\u001B[32m    275\u001B[39m \n\u001B[32m    276\u001B[39m \u001B[33;03m    Args:\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m    288\u001B[39m \u001B[33;03m            ])\u001B[39;00m\n\u001B[32m    289\u001B[39m \u001B[33;03m    \"\"\"\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m291\u001B[39m     final_chunk = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_chat_stream_with_aggregation\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    292\u001B[39m \u001B[43m        \u001B[49m\u001B[43mmessages\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    293\u001B[39m \u001B[43m        \u001B[49m\u001B[43mstop\u001B[49m\u001B[43m=\u001B[49m\u001B[43mstop\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    294\u001B[39m \u001B[43m        \u001B[49m\u001B[43mrun_manager\u001B[49m\u001B[43m=\u001B[49m\u001B[43mrun_manager\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    295\u001B[39m \u001B[43m        \u001B[49m\u001B[43mverbose\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mverbose\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    296\u001B[39m \u001B[43m        \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    297\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    298\u001B[39m     chat_generation = ChatGeneration(\n\u001B[32m    299\u001B[39m         message=AIMessage(content=final_chunk.text),\n\u001B[32m    300\u001B[39m         generation_info=final_chunk.generation_info,\n\u001B[32m    301\u001B[39m     )\n\u001B[32m    302\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m ChatResult(generations=[chat_generation])\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/FastAPIProject/.venv/lib/python3.11/site-packages/langchain_community/chat_models/ollama.py:222\u001B[39m, in \u001B[36mChatOllama._chat_stream_with_aggregation\u001B[39m\u001B[34m(self, messages, stop, run_manager, verbose, **kwargs)\u001B[39m\n\u001B[32m    213\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m_chat_stream_with_aggregation\u001B[39m(\n\u001B[32m    214\u001B[39m     \u001B[38;5;28mself\u001B[39m,\n\u001B[32m    215\u001B[39m     messages: List[BaseMessage],\n\u001B[32m   (...)\u001B[39m\u001B[32m    219\u001B[39m     **kwargs: Any,\n\u001B[32m    220\u001B[39m ) -> ChatGenerationChunk:\n\u001B[32m    221\u001B[39m     final_chunk: Optional[ChatGenerationChunk] = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m222\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mstream_resp\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_create_chat_stream\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmessages\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstop\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\u001B[43m:\u001B[49m\n\u001B[32m    223\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mstream_resp\u001B[49m\u001B[43m:\u001B[49m\n\u001B[32m    224\u001B[39m \u001B[43m            \u001B[49m\u001B[43mchunk\u001B[49m\u001B[43m \u001B[49m\u001B[43m=\u001B[49m\u001B[43m \u001B[49m\u001B[43m_chat_stream_response_to_chat_generation_chunk\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstream_resp\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/FastAPIProject/.venv/lib/python3.11/site-packages/langchain_community/chat_models/ollama.py:194\u001B[39m, in \u001B[36mChatOllama._create_chat_stream\u001B[39m\u001B[34m(self, messages, stop, **kwargs)\u001B[39m\n\u001B[32m    184\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m_create_chat_stream\u001B[39m(\n\u001B[32m    185\u001B[39m     \u001B[38;5;28mself\u001B[39m,\n\u001B[32m    186\u001B[39m     messages: List[BaseMessage],\n\u001B[32m    187\u001B[39m     stop: Optional[List[\u001B[38;5;28mstr\u001B[39m]] = \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[32m    188\u001B[39m     **kwargs: Any,\n\u001B[32m    189\u001B[39m ) -> Iterator[\u001B[38;5;28mstr\u001B[39m]:\n\u001B[32m    190\u001B[39m     payload = {\n\u001B[32m    191\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mmodel\u001B[39m\u001B[33m\"\u001B[39m: \u001B[38;5;28mself\u001B[39m.model,\n\u001B[32m    192\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mmessages\u001B[39m\u001B[33m\"\u001B[39m: \u001B[38;5;28mself\u001B[39m._convert_messages_to_ollama_messages(messages),\n\u001B[32m    193\u001B[39m     }\n\u001B[32m--> \u001B[39m\u001B[32m194\u001B[39m     \u001B[38;5;28;01myield from\u001B[39;00m \u001B[38;5;28mself\u001B[39m._create_stream(\n\u001B[32m    195\u001B[39m         payload=payload, stop=stop, api_url=\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m.base_url\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m/api/chat\u001B[39m\u001B[33m\"\u001B[39m, **kwargs\n\u001B[32m    196\u001B[39m     )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/FastAPIProject/.venv/lib/python3.11/site-packages/requests/models.py:869\u001B[39m, in \u001B[36mResponse.iter_lines\u001B[39m\u001B[34m(self, chunk_size, decode_unicode, delimiter)\u001B[39m\n\u001B[32m    860\u001B[39m \u001B[38;5;250m\u001B[39m\u001B[33;03m\"\"\"Iterates over the response data, one line at a time.  When\u001B[39;00m\n\u001B[32m    861\u001B[39m \u001B[33;03mstream=True is set on the request, this avoids reading the\u001B[39;00m\n\u001B[32m    862\u001B[39m \u001B[33;03mcontent at once into memory for large responses.\u001B[39;00m\n\u001B[32m    863\u001B[39m \n\u001B[32m    864\u001B[39m \u001B[33;03m.. note:: This method is not reentrant safe.\u001B[39;00m\n\u001B[32m    865\u001B[39m \u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m    867\u001B[39m pending = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m869\u001B[39m \u001B[43m\u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mchunk\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43miter_content\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    870\u001B[39m \u001B[43m    \u001B[49m\u001B[43mchunk_size\u001B[49m\u001B[43m=\u001B[49m\u001B[43mchunk_size\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdecode_unicode\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdecode_unicode\u001B[49m\n\u001B[32m    871\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\u001B[43m:\u001B[49m\n\u001B[32m    872\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mpending\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mis\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mnot\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m:\u001B[49m\n\u001B[32m    873\u001B[39m \u001B[43m        \u001B[49m\u001B[43mchunk\u001B[49m\u001B[43m \u001B[49m\u001B[43m=\u001B[49m\u001B[43m \u001B[49m\u001B[43mpending\u001B[49m\u001B[43m \u001B[49m\u001B[43m+\u001B[49m\u001B[43m \u001B[49m\u001B[43mchunk\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/FastAPIProject/.venv/lib/python3.11/site-packages/requests/utils.py:562\u001B[39m, in \u001B[36mstream_decode_response_unicode\u001B[39m\u001B[34m(iterator, r)\u001B[39m\n\u001B[32m    559\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m\n\u001B[32m    561\u001B[39m decoder = codecs.getincrementaldecoder(r.encoding)(errors=\u001B[33m\"\u001B[39m\u001B[33mreplace\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m--> \u001B[39m\u001B[32m562\u001B[39m \u001B[43m\u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mchunk\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43miterator\u001B[49m\u001B[43m:\u001B[49m\n\u001B[32m    563\u001B[39m \u001B[43m    \u001B[49m\u001B[43mrv\u001B[49m\u001B[43m \u001B[49m\u001B[43m=\u001B[49m\u001B[43m \u001B[49m\u001B[43mdecoder\u001B[49m\u001B[43m.\u001B[49m\u001B[43mdecode\u001B[49m\u001B[43m(\u001B[49m\u001B[43mchunk\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    564\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mrv\u001B[49m\u001B[43m:\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/FastAPIProject/.venv/lib/python3.11/site-packages/requests/models.py:820\u001B[39m, in \u001B[36mResponse.iter_content.<locals>.generate\u001B[39m\u001B[34m()\u001B[39m\n\u001B[32m    818\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(\u001B[38;5;28mself\u001B[39m.raw, \u001B[33m\"\u001B[39m\u001B[33mstream\u001B[39m\u001B[33m\"\u001B[39m):\n\u001B[32m    819\u001B[39m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m820\u001B[39m         \u001B[38;5;28;01myield from\u001B[39;00m \u001B[38;5;28mself\u001B[39m.raw.stream(chunk_size, decode_content=\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[32m    821\u001B[39m     \u001B[38;5;28;01mexcept\u001B[39;00m ProtocolError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[32m    822\u001B[39m         \u001B[38;5;28;01mraise\u001B[39;00m ChunkedEncodingError(e)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/FastAPIProject/.venv/lib/python3.11/site-packages/urllib3/response.py:1088\u001B[39m, in \u001B[36mHTTPResponse.stream\u001B[39m\u001B[34m(self, amt, decode_content)\u001B[39m\n\u001B[32m   1072\u001B[39m \u001B[38;5;250m\u001B[39m\u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m   1073\u001B[39m \u001B[33;03mA generator wrapper for the read() method. A call will block until\u001B[39;00m\n\u001B[32m   1074\u001B[39m \u001B[33;03m``amt`` bytes have been read from the connection or until the\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m   1085\u001B[39m \u001B[33;03m    'content-encoding' header.\u001B[39;00m\n\u001B[32m   1086\u001B[39m \u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m   1087\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.chunked \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m.supports_chunked_reads():\n\u001B[32m-> \u001B[39m\u001B[32m1088\u001B[39m     \u001B[38;5;28;01myield from\u001B[39;00m \u001B[38;5;28mself\u001B[39m.read_chunked(amt, decode_content=decode_content)\n\u001B[32m   1089\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m   1090\u001B[39m     \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_fp_closed(\u001B[38;5;28mself\u001B[39m._fp) \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m._decoded_buffer) > \u001B[32m0\u001B[39m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/FastAPIProject/.venv/lib/python3.11/site-packages/urllib3/response.py:1248\u001B[39m, in \u001B[36mHTTPResponse.read_chunked\u001B[39m\u001B[34m(self, amt, decode_content)\u001B[39m\n\u001B[32m   1245\u001B[39m     amt = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1247\u001B[39m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1248\u001B[39m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_update_chunk_length\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1249\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.chunk_left == \u001B[32m0\u001B[39m:\n\u001B[32m   1250\u001B[39m         \u001B[38;5;28;01mbreak\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/FastAPIProject/.venv/lib/python3.11/site-packages/urllib3/response.py:1167\u001B[39m, in \u001B[36mHTTPResponse._update_chunk_length\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m   1165\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.chunk_left \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m   1166\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m1167\u001B[39m line = \u001B[38;5;28mself\u001B[39m._fp.fp.readline()  \u001B[38;5;66;03m# type: ignore[union-attr]\u001B[39;00m\n\u001B[32m   1168\u001B[39m line = line.split(\u001B[33mb\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m;\u001B[39m\u001B[33m\"\u001B[39m, \u001B[32m1\u001B[39m)[\u001B[32m0\u001B[39m]\n\u001B[32m   1169\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/usr/local/Cellar/python@3.11/3.11.12_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/socket.py:718\u001B[39m, in \u001B[36mSocketIO.readinto\u001B[39m\u001B[34m(self, b)\u001B[39m\n\u001B[32m    716\u001B[39m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[32m    717\u001B[39m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m718\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_sock\u001B[49m\u001B[43m.\u001B[49m\u001B[43mrecv_into\u001B[49m\u001B[43m(\u001B[49m\u001B[43mb\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    719\u001B[39m     \u001B[38;5;28;01mexcept\u001B[39;00m timeout:\n\u001B[32m    720\u001B[39m         \u001B[38;5;28mself\u001B[39m._timeout_occurred = \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 2,
   "source": [
    "\n",
    "# 主函数\n",
    "def main():\n",
    "    \"\"\"运行所有示例\"\"\"\n",
    "    print(\"LangChain 0.3 Chat Models 完整示例\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # 运行所有示例\n",
    "    basic_chat_model()\n",
    "    streaming_chat()\n",
    "    batch_processing()\n",
    "    with_prompt_template()\n",
    "    conversation_with_memory()\n",
    "    multi_model_comparison()\n",
    "    model_with_parameters()\n",
    "    error_handling()\n",
    "    custom_output_parsing()\n",
    "\n",
    "    # 异步示例需要单独运行\n",
    "    print(\"\\n运行异步示例...\")\n",
    "    asyncio.run(async_chat())\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "id": "initial_id"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 通用初始化方法",
   "id": "580fbc27537e7161"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-22T06:27:45.562755Z",
     "start_time": "2025-07-22T06:27:45.472812Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "使用 initChatModel 通用初始化方法\n",
    "\"\"\"\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "# 通用初始化 - 支持多种模型提供商\n",
    "def universal_init_examples():\n",
    "    \"\"\"通用初始化示例\"\"\"\n",
    "\n",
    "    # OpenAI\n",
    "    gpt4o = init_chat_model(\"gpt-4o\", {\n",
    "        \"model_provider\": \"openai\",\n",
    "        \"temperature\": 0\n",
    "    })\n",
    "\n",
    "    # Anthropic\n",
    "    claude = init_chat_model(\"claude-3-opus-20240229\", {\n",
    "        \"model_provider\": \"anthropic\",\n",
    "        \"temperature\": 0\n",
    "    })\n",
    "\n",
    "    # Google Vertex AI\n",
    "    gemini = init_chat_model(\"gemini-1.5-pro\", {\n",
    "        \"model_provider\": \"google-vertexai\",\n",
    "        \"temperature\": 0\n",
    "    })\n",
    "\n",
    "    # 也可以在模型名中指定提供商\n",
    "    claude_alt = init_chat_model(\"anthropic:claude-3-opus-20240229\", {\n",
    "        \"temperature\": 0\n",
    "    })\n",
    "\n",
    "    # 统一接口调用\n",
    "    models = [gpt4o, claude, gemini]\n",
    "    for model in models:\n",
    "        try:\n",
    "            response = model.invoke(\"你好\")\n",
    "            print(f\"{model.__class__.__name__}: {response.content}\")\n",
    "        except Exception as e:\n",
    "            print(f\"模型调用失败: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    universal_init_examples()"
   ],
   "id": "971956e151862825",
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "init_chat_model() takes from 0 to 1 positional arguments but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mTypeError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[7]\u001B[39m\u001B[32m, line 43\u001B[39m\n\u001B[32m     40\u001B[39m             \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m模型调用失败: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00me\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m)\n\u001B[32m     42\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[34m__name__\u001B[39m == \u001B[33m\"\u001B[39m\u001B[33m__main__\u001B[39m\u001B[33m\"\u001B[39m:\n\u001B[32m---> \u001B[39m\u001B[32m43\u001B[39m     \u001B[43muniversal_init_examples\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[7]\u001B[39m\u001B[32m, line 11\u001B[39m, in \u001B[36muniversal_init_examples\u001B[39m\u001B[34m()\u001B[39m\n\u001B[32m      8\u001B[39m \u001B[38;5;250m\u001B[39m\u001B[33;03m\"\"\"通用初始化示例\"\"\"\u001B[39;00m\n\u001B[32m     10\u001B[39m \u001B[38;5;66;03m# OpenAI\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m11\u001B[39m gpt4o = \u001B[43minit_chat_model\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mgpt-4o\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m{\u001B[49m\n\u001B[32m     12\u001B[39m \u001B[43m    \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mmodel_provider\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mopenai\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m     13\u001B[39m \u001B[43m    \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mtemperature\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[32;43m0\u001B[39;49m\n\u001B[32m     14\u001B[39m \u001B[43m\u001B[49m\u001B[43m}\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     16\u001B[39m \u001B[38;5;66;03m# Anthropic\u001B[39;00m\n\u001B[32m     17\u001B[39m claude = init_chat_model(\u001B[33m\"\u001B[39m\u001B[33mclaude-3-opus-20240229\u001B[39m\u001B[33m\"\u001B[39m, {\n\u001B[32m     18\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mmodel_provider\u001B[39m\u001B[33m\"\u001B[39m: \u001B[33m\"\u001B[39m\u001B[33manthropic\u001B[39m\u001B[33m\"\u001B[39m, \n\u001B[32m     19\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mtemperature\u001B[39m\u001B[33m\"\u001B[39m: \u001B[32m0\u001B[39m\n\u001B[32m     20\u001B[39m })\n",
      "\u001B[31mTypeError\u001B[39m: init_chat_model() takes from 0 to 1 positional arguments but 2 were given"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 高级功能",
   "id": "a5b1f1d7e93c176c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-22T06:30:01.556696Z",
     "start_time": "2025-07-22T06:29:42.273933Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "Chat Models 高级功能示例\n",
    "\"\"\"\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableParallel, RunnableLambda\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "import json\n",
    "\n",
    "def advanced_features_demo():\n",
    "    \"\"\"高级功能演示\"\"\"\n",
    "\n",
    "    model = ChatOllama(\n",
    "        base_url=\"http://localhost:11434\",\n",
    "        model=\"qwen2.5:3b\"\n",
    "    )\n",
    "\n",
    "    # 1. 并行处理\n",
    "    parallel_chain = RunnableParallel({\n",
    "        \"translation\": ChatPromptTemplate.from_template(\"翻译: {text}\") | model | StrOutputParser(),\n",
    "        \"summary\": ChatPromptTemplate.from_template(\"总结: {text}\") | model | StrOutputParser(),\n",
    "        \"sentiment\": ChatPromptTemplate.from_template(\"情感分析: {text}\") | model | StrOutputParser()\n",
    "    })\n",
    "\n",
    "    result = parallel_chain.invoke({\"text\": \"今天天气真好，我很开心\"})\n",
    "    print(\"并行处理结果:\")\n",
    "    for key, value in result.items():\n",
    "        print(f\"{key}: {value[:50]}...\")\n",
    "\n",
    "    # 2. 条件分支\n",
    "    def route_question(input_dict):\n",
    "        question = input_dict[\"question\"]\n",
    "        if \"数学\" in question or \"计算\" in question:\n",
    "            return \"math\"\n",
    "        elif \"翻译\" in question:\n",
    "            return \"translation\"\n",
    "        else:\n",
    "            return \"general\"\n",
    "\n",
    "    # 3. 自定义处理链\n",
    "    def create_specialized_chain(question_type):\n",
    "        if question_type == \"math\":\n",
    "            prompt = \"作为数学专家，请解答: {question}\"\n",
    "        elif question_type == \"translation\":\n",
    "            prompt = \"作为翻译专家，请翻译: {question}\"\n",
    "        else:\n",
    "            prompt = \"请回答: {question}\"\n",
    "\n",
    "        return ChatPromptTemplate.from_template(prompt) | model | StrOutputParser()\n",
    "\n",
    "    # 测试路由\n",
    "    questions = [\n",
    "        \"1+1等于多少？\",\n",
    "        \"请翻译'Hello'\",\n",
    "        \"什么是人工智能？\"\n",
    "    ]\n",
    "\n",
    "    for q in questions:\n",
    "        route = route_question({\"question\": q})\n",
    "        chain = create_specialized_chain(route)\n",
    "        response = chain.invoke({\"question\": q})\n",
    "        print(f\"\\n问题类型: {route}\")\n",
    "        print(f\"问题: {q}\")\n",
    "        print(f\"回答: {response[:100]}...\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    advanced_features_demo()"
   ],
   "id": "d8af32b6d0495174",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "并行处理结果:\n",
      "translation: 今天的天气真是太好了，我感到非常高兴。...\n",
      "summary: 今天的天气非常好，这让您感到很开心。这正是美好的一天的缩影。...\n",
      "sentiment: 这条信息的情感倾向是积极的。句子表达了说话者对今天的天气感到满意，并因此感到开心。...\n",
      "\n",
      "问题类型: general\n",
      "问题: 1+1等于多少？\n",
      "回答: 1+1等于2。...\n",
      "\n",
      "问题类型: translation\n",
      "问题: 请翻译'Hello'\n",
      "回答: 'Hello' 在英文中直接对应 '你好' 在中文中。所以它的翻译是 '你好'。...\n",
      "\n",
      "问题类型: general\n",
      "问题: 什么是人工智能？\n",
      "回答: 人工智能（Artificial Intelligence，简称AI）是指由人设计出的一套处理信息的系统或者算法。这些系统或算法能完成一些通常需要人类智能才能执行的任务，比如视觉识别、语音识别、决策制定...\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 关键特性总结\n",
    "1. 统一接口: 所有聊天模型都实现相同的接口\n",
    "2. 消息驱动: 基于消息列表的交互模式\n",
    "3. 流式支持: 实时流式输出能力\n",
    "4. 批量优化: 支持批量请求提高效率\n",
    "5. 异步支持: 完整的异步操作支持\n",
    "6. 可组合性: 与其他LangChain组件无缝集成\n",
    "7. 标准化参数: 温度、最大令牌等标准参数\n",
    "8. 错误处理: 完善的异常处理机制\n",
    "这些示例展示了 LangChain 0.3 中 Chat Models 的核心功能和使用方法，可以根据具体需求选择合适的模式进行开发"
   ],
   "id": "d61f9ffc1deab818"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
