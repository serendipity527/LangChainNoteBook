{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 1. Document Loaders ç¤ºä¾‹",
   "id": "83f2a19cec7294c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 1. æ–‡æœ¬æ–‡ä»¶åŠ è½½å™¨",
   "id": "9dec91e179be942a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T03:18:31.722562Z",
     "start_time": "2025-07-23T03:18:31.715330Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ–‡æ¡£æ•°é‡: 1\n",
      "å†…å®¹: äººå·¥æ™ºèƒ½æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªåˆ†æ”¯ã€‚\n",
      "æœºå™¨å­¦ä¹ æ˜¯AIçš„å­é›†ã€‚\n",
      "å…ƒæ•°æ®: {'source': 'sample.txt'}\n"
     ]
    }
   ],
   "execution_count": 14,
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_core.documents import Document\n",
    "import os\n",
    "\n",
    "def text_loader_examples():\n",
    "    \"\"\"æ–‡æœ¬æ–‡ä»¶åŠ è½½å™¨ç¤ºä¾‹\"\"\"\n",
    "\n",
    "    # 1.1 åŸºç¡€æ–‡æœ¬åŠ è½½\n",
    "    with open(\"sample.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"äººå·¥æ™ºèƒ½æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªåˆ†æ”¯ã€‚\\næœºå™¨å­¦ä¹ æ˜¯AIçš„å­é›†ã€‚\")\n",
    "\n",
    "    loader = TextLoader(\"sample.txt\", encoding=\"utf-8\")\n",
    "    documents = loader.load()\n",
    "    print(f\"æ–‡æ¡£æ•°é‡: {len(documents)}\")\n",
    "    print(f\"å†…å®¹: {documents[0].page_content}\")\n",
    "    print(f\"å…ƒæ•°æ®: {documents[0].metadata}\")\n",
    "\n",
    "    # # 1.2 å¤„ç†å¤§æ–‡ä»¶\n",
    "    # loader_large = TextLoader(\"large_file.txt\", encoding=\"utf-8\")\n",
    "    # try:\n",
    "    #     docs = loader_large.load()\n",
    "    #     print(f\"å¤§æ–‡ä»¶åŠ è½½æˆåŠŸï¼Œæ–‡æ¡£æ•°: {len(docs)}\")\n",
    "    # except Exception as e:\n",
    "    #     print(f\"åŠ è½½å¤±è´¥: {e}\")\n",
    "\n",
    "    # # 1.3 è‡ªåŠ¨ç¼–ç æ£€æµ‹\n",
    "    # loader_auto = TextLoader(\"file.txt\", autodetect_encoding=True)\n",
    "    # docs = loader_auto.load()\n",
    "text_loader_examples()"
   ],
   "id": "208b154333b1e647"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 2. PDF æ–‡æ¡£åŠ è½½å™¨",
   "id": "54520ffd6591457b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T03:25:40.450347Z",
     "start_time": "2025-07-23T03:25:36.529217Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDFé¡µæ•°: 10\n",
      "ç¬¬1é¡µå†…å®¹: Multi-level Wavelet-CNN for Image Restoration\n",
      "Pengju Liu1, Hongzhi Zhang âˆ—1, Kai Zhang1, Liang Lin2,...\n",
      "é¡µé¢å…ƒæ•°æ®: {'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2018-05-23T00:19:39+00:00', 'author': '', 'keywords': '', 'moddate': '2018-05-23T00:19:39+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'docs/Multi-level Wavelet-CNN for Image Restoration.pdf', 'total_pages': 10, 'page': 0, 'page_label': '1'}\n",
      "ç¬¬2é¡µå†…å®¹: is adopted to enlarge receptive ï¬eld without the sacriï¬ce\n",
      "of computational cost. Dilated ï¬ltering, h...\n",
      "é¡µé¢å…ƒæ•°æ®: {'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2018-05-23T00:19:39+00:00', 'author': '', 'keywords': '', 'moddate': '2018-05-23T00:19:39+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'docs/Multi-level Wavelet-CNN for Image Restoration.pdf', 'total_pages': 10, 'page': 1, 'page_label': '2'}\n"
     ]
    }
   ],
   "execution_count": 20,
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader, PDFMinerLoader, PDFPlumberLoader\n",
    "\n",
    "def pdf_loader_examples():\n",
    "    \"\"\"PDFåŠ è½½å™¨ç¤ºä¾‹\"\"\"\n",
    "\n",
    "    # 2.1 PyPDFLoader - æœ€å¸¸ç”¨\n",
    "    pdf_loader = PyPDFLoader(\"docs/Multi-level Wavelet-CNN for Image Restoration.pdf\")\n",
    "    pages = pdf_loader.load()\n",
    "    print(f\"PDFé¡µæ•°: {len(pages)}\")\n",
    "\n",
    "    for i, page in enumerate(pages[:2]):\n",
    "        print(f\"ç¬¬{i+1}é¡µå†…å®¹: {page.page_content[:100]}...\")\n",
    "        print(f\"é¡µé¢å…ƒæ•°æ®: {page.metadata}\")\n",
    "\n",
    "    # 2.2 PDFMinerLoader - æ›´å¥½çš„æ–‡æœ¬æå–\n",
    "    pdf_miner_loader = PDFMinerLoader(\"docs/Multi-level Wavelet-CNN for Image Restoration.pdf\")\n",
    "    docs = pdf_miner_loader.load()\n",
    "\n",
    "    # 2.3 PDFPlumberLoader - è¡¨æ ¼å¤„ç†æ›´å¥½\n",
    "    pdf_plumber_loader = PDFPlumberLoader(\"docs/Multi-level Wavelet-CNN for Image Restoration.pdf\")\n",
    "    docs = pdf_plumber_loader.load()\n",
    "\n",
    "    # 2.4 åˆ†é¡µåŠ è½½\n",
    "    pdf_loader = PyPDFLoader(\"docs/Multi-level Wavelet-CNN for Image Restoration.pdf\")\n",
    "    pages = pdf_loader.load_and_split()\n",
    "\n",
    "    # # 2.5 å¯†ç ä¿æŠ¤çš„PDF\n",
    "    # protected_loader = PyPDFLoader(\"docs/Multi-level Wavelet-CNN for Image Restoration.pdf\", password=\"password123\")\n",
    "    # docs = protected_loader.load()\n",
    "\n",
    "pdf_loader_examples()"
   ],
   "id": "664516871d1342fa"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 3. CSV æ•°æ®åŠ è½½å™¨",
   "id": "44d5e6a2e7dec69e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from langchain_community.document_loaders import CSVLoader\n",
    "import pandas as pd\n",
    "\n",
    "def csv_loader_examples():\n",
    "    \"\"\"CSVåŠ è½½å™¨ç¤ºä¾‹\"\"\"\n",
    "\n",
    "    # åˆ›å»ºç¤ºä¾‹CSV\n",
    "    df = pd.DataFrame({\n",
    "        'name': ['å¼ ä¸‰', 'æå››', 'ç‹äº”'],\n",
    "        'age': [25, 30, 35],\n",
    "        'department': ['æŠ€æœ¯éƒ¨', 'é”€å”®éƒ¨', 'å¸‚åœºéƒ¨'],\n",
    "        'description': ['Pythonå¼€å‘å·¥ç¨‹å¸ˆ', 'é”€å”®ç»ç†', 'å¸‚åœºä¸“å‘˜']\n",
    "    })\n",
    "    df.to_csv(\"employees.csv\", index=False, encoding=\"utf-8\")\n",
    "\n",
    "    # 3.1 åŸºç¡€CSVåŠ è½½\n",
    "    csv_loader = CSVLoader(\"employees.csv\", encoding=\"utf-8\")\n",
    "    docs = csv_loader.load()\n",
    "    print(f\"CSVæ–‡æ¡£æ•°é‡: {len(docs)}\")\n",
    "    print(f\"ç¬¬ä¸€æ¡è®°å½•: {docs[0].page_content}\")\n",
    "\n",
    "    # 3.2 æŒ‡å®šæºåˆ—\n",
    "    csv_loader_with_source = CSVLoader(\n",
    "        \"employees.csv\",\n",
    "        source_column=\"name\",\n",
    "        encoding=\"utf-8\"\n",
    "    )\n",
    "    docs = csv_loader_with_source.load()\n",
    "\n",
    "    # 3.3 è‡ªå®šä¹‰CSVå‚æ•°\n",
    "    csv_loader_custom = CSVLoader(\n",
    "        \"employees.csv\",\n",
    "        csv_args={\n",
    "            'delimiter': ',',\n",
    "            'quotechar': '\"',\n",
    "            'fieldnames': ['å§“å', 'å¹´é¾„', 'éƒ¨é—¨', 'æè¿°']\n",
    "        }\n",
    "    )\n",
    "    docs = csv_loader_custom.load()\n",
    "\n",
    "    # 3.4 è¿‡æ»¤ç‰¹å®šåˆ—\n",
    "    csv_loader_filtered = CSVLoader(\n",
    "        \"employees.csv\",\n",
    "        content_columns=['name', 'description'],\n",
    "        encoding=\"utf-8\"\n",
    "    )\n",
    "    docs = csv_loader_filtered.load()\n",
    "csv_loader_examples()"
   ],
   "id": "fe626b1b43fe80cb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 4. JSON æ•°æ®åŠ è½½å™¨",
   "id": "bbedfc377236d261"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from langchain_community.document_loaders import JSONLoader\n",
    "import json\n",
    "\n",
    "def json_loader_examples():\n",
    "    \"\"\"JSONåŠ è½½å™¨ç¤ºä¾‹\"\"\"\n",
    "\n",
    "    # åˆ›å»ºç¤ºä¾‹JSONæ•°æ®\n",
    "    data = [\n",
    "        {\n",
    "            \"id\": 1,\n",
    "            \"title\": \"Pythonç¼–ç¨‹æŒ‡å—\",\n",
    "            \"content\": \"Pythonæ˜¯ä¸€ç§é«˜çº§ç¼–ç¨‹è¯­è¨€ï¼Œè¯­æ³•ç®€æ´ä¼˜é›…ã€‚\",\n",
    "            \"author\": \"å¼ ä¸‰\",\n",
    "            \"tags\": [\"ç¼–ç¨‹\", \"Python\", \"æ•™ç¨‹\"]\n",
    "        },\n",
    "        {\n",
    "            \"id\": 2,\n",
    "            \"title\": \"æœºå™¨å­¦ä¹ å…¥é—¨\",\n",
    "            \"content\": \"æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªé‡è¦åˆ†æ”¯ã€‚\",\n",
    "            \"author\": \"æå››\",\n",
    "            \"tags\": [\"AI\", \"æœºå™¨å­¦ä¹ \", \"æ•°æ®ç§‘å­¦\"]\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    with open(\"articles.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    # 4.1 æå–ç‰¹å®šå­—æ®µ\n",
    "    json_loader = JSONLoader(\n",
    "        \"articles.json\",\n",
    "        jq_schema=\".[].content\",\n",
    "        text_content=False\n",
    "    )\n",
    "    docs = json_loader.load()\n",
    "    print(f\"JSONæ–‡æ¡£æ•°é‡: {len(docs)}\")\n",
    "\n",
    "    # 4.2 æå–å¤šä¸ªå­—æ®µ\n",
    "    json_loader_multi = JSONLoader(\n",
    "        \"articles.json\",\n",
    "        jq_schema=\".[]\",\n",
    "        content_key=\"content\"\n",
    "    )\n",
    "    docs = json_loader_multi.load()\n",
    "\n",
    "    # 4.3 å¤æ‚JSONç»“æ„\n",
    "    complex_data = {\n",
    "        \"articles\": {\n",
    "            \"tech\": [\n",
    "                {\"title\": \"AIå‘å±•\", \"body\": \"äººå·¥æ™ºèƒ½å¿«é€Ÿå‘å±•\"},\n",
    "                {\"title\": \"äº‘è®¡ç®—\", \"body\": \"äº‘è®¡ç®—æ”¹å˜äº†ITæ¶æ„\"}\n",
    "            ],\n",
    "            \"business\": [\n",
    "                {\"title\": \"æ•°å­—åŒ–è½¬å‹\", \"body\": \"ä¼ä¸šæ•°å­—åŒ–è½¬å‹åŠ¿åœ¨å¿…è¡Œ\"}\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "\n",
    "    with open(\"complex.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(complex_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    # æå–åµŒå¥—æ•°æ®\n",
    "    json_loader_nested = JSONLoader(\n",
    "        \"complex.json\",\n",
    "        jq_schema=\".articles.tech[].body\"\n",
    "    )\n",
    "    docs = json_loader_nested.load()\n",
    "\n",
    "    # 4.4 JSONLæ ¼å¼\n",
    "    jsonl_data = [\n",
    "        {\"text\": \"ç¬¬ä¸€è¡Œæ•°æ®\", \"label\": \"A\"},\n",
    "        {\"text\": \"ç¬¬äºŒè¡Œæ•°æ®\", \"label\": \"B\"}\n",
    "    ]\n",
    "\n",
    "    with open(\"data.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
    "        for item in jsonl_data:\n",
    "            f.write(json.dumps(item, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "    from langchain_community.document_loaders import JSONLinesLoader\n",
    "    jsonl_loader = JSONLinesLoader(\"data.jsonl\", jq_schema=\".text\")\n",
    "    docs = jsonl_loader.load()"
   ],
   "id": "109c70795f3818f2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 5. ç½‘é¡µå†…å®¹åŠ è½½å™¨",
   "id": "6cf9d2a19d105ec2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader, AsyncHtmlLoader\n",
    "from langchain_community.document_transformers import Html2TextTransformer\n",
    "\n",
    "def web_loader_examples():\n",
    "    \"\"\"ç½‘é¡µåŠ è½½å™¨ç¤ºä¾‹\"\"\"\n",
    "\n",
    "    # 5.1 åŸºç¡€ç½‘é¡µåŠ è½½\n",
    "    web_loader = WebBaseLoader(\"https://example.com\")\n",
    "    docs = web_loader.load()\n",
    "    print(f\"ç½‘é¡µæ–‡æ¡£: {docs[0].page_content[:200]}...\")\n",
    "\n",
    "    # 5.2 å¤šä¸ªURLæ‰¹é‡åŠ è½½\n",
    "    urls = [\n",
    "        \"https://example.com/page1\",\n",
    "        \"https://example.com/page2\",\n",
    "        \"https://example.com/page3\"\n",
    "    ]\n",
    "    web_loader_multi = WebBaseLoader(urls)\n",
    "    docs = web_loader_multi.load()\n",
    "\n",
    "    # 5.3 è‡ªå®šä¹‰è¯·æ±‚å¤´\n",
    "    web_loader_headers = WebBaseLoader(\n",
    "        \"https://api.example.com/data\",\n",
    "        header_template={\n",
    "            \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\",\n",
    "            \"Authorization\": \"Bearer your-token\"\n",
    "        }\n",
    "    )\n",
    "    docs = web_loader_headers.load()\n",
    "\n",
    "    # 5.4 CSSé€‰æ‹©å™¨è¿‡æ»¤\n",
    "    from bs4 import BeautifulSoup\n",
    "\n",
    "    web_loader_css = WebBaseLoader(\n",
    "        \"https://news.example.com\",\n",
    "        bs_kwargs={\n",
    "            \"parse_only\": BeautifulSoup.SoupStrainer(\"div\", {\"class\": \"article-content\"})\n",
    "        }\n",
    "    )\n",
    "    docs = web_loader_css.load()\n",
    "\n",
    "    # 5.5 å¼‚æ­¥ç½‘é¡µåŠ è½½\n",
    "    async def async_web_loading():\n",
    "        urls = [\"https://example.com/1\", \"https://example.com/2\"]\n",
    "        async_loader = AsyncHtmlLoader(urls)\n",
    "        html_docs = async_loader.load()\n",
    "\n",
    "        # HTMLè½¬æ–‡æœ¬\n",
    "        html2text = Html2TextTransformer()\n",
    "        text_docs = html2text.transform_documents(html_docs)\n",
    "        return text_docs\n",
    "\n",
    "    # 5.6 å¤„ç†JavaScriptæ¸²æŸ“é¡µé¢\n",
    "    from langchain_community.document_loaders import SeleniumURLLoader\n",
    "\n",
    "    selenium_loader = SeleniumURLLoader(\n",
    "        urls=[\"https://spa-example.com\"],\n",
    "        browser=\"chrome\",\n",
    "        headless=True\n",
    "    )\n",
    "    docs = selenium_loader.load()"
   ],
   "id": "59e94f75d42d0811"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 6. ç›®å½•æ‰¹é‡åŠ è½½å™¨",
   "id": "d49b53e50f00e400"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "from langchain_community.document_loaders import TextLoader, PyPDFLoader, CSVLoader\n",
    "\n",
    "def directory_loader_examples():\n",
    "    \"\"\"ç›®å½•åŠ è½½å™¨ç¤ºä¾‹\"\"\"\n",
    "\n",
    "    # åˆ›å»ºæµ‹è¯•ç›®å½•ç»“æ„\n",
    "    os.makedirs(\"documents/texts\", exist_ok=True)\n",
    "    os.makedirs(\"documents/pdfs\", exist_ok=True)\n",
    "    os.makedirs(\"documents/data\", exist_ok=True)\n",
    "\n",
    "    # åˆ›å»ºæµ‹è¯•æ–‡ä»¶\n",
    "    for i in range(3):\n",
    "        with open(f\"documents/texts/doc_{i}.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(f\"è¿™æ˜¯æ–‡æ¡£{i}çš„å†…å®¹ï¼ŒåŒ…å«é‡è¦ä¿¡æ¯ã€‚\")\n",
    "\n",
    "    # 6.1 åŠ è½½ç‰¹å®šç±»å‹æ–‡ä»¶\n",
    "    txt_loader = DirectoryLoader(\n",
    "        \"documents/texts\",\n",
    "        glob=\"*.txt\",\n",
    "        loader_cls=TextLoader,\n",
    "        loader_kwargs={\"encoding\": \"utf-8\"}\n",
    "    )\n",
    "    txt_docs = txt_loader.load()\n",
    "    print(f\"æ–‡æœ¬æ–‡æ¡£æ•°é‡: {len(txt_docs)}\")\n",
    "\n",
    "    # 6.2 å¤šç§æ–‡ä»¶ç±»å‹æ··åˆåŠ è½½\n",
    "    from langchain_community.document_loaders import UnstructuredFileLoader\n",
    "\n",
    "    mixed_loader = DirectoryLoader(\n",
    "        \"documents\",\n",
    "        glob=\"**/*\",  # é€’å½’æœç´¢\n",
    "        loader_cls=UnstructuredFileLoader,\n",
    "        recursive=True,\n",
    "        show_progress=True\n",
    "    )\n",
    "    mixed_docs = mixed_loader.load()\n",
    "\n",
    "    # 6.3 è‡ªå®šä¹‰æ–‡ä»¶ç±»å‹æ˜ å°„\n",
    "    def get_loader_for_file(file_path: str):\n",
    "        if file_path.endswith('.txt'):\n",
    "            return TextLoader(file_path, encoding=\"utf-8\")\n",
    "        elif file_path.endswith('.pdf'):\n",
    "            return PyPDFLoader(file_path)\n",
    "        elif file_path.endswith('.csv'):\n",
    "            return CSVLoader(file_path, encoding=\"utf-8\")\n",
    "        else:\n",
    "            return UnstructuredFileLoader(file_path)\n",
    "\n",
    "    # 6.4 è¿‡æ»¤å’Œæ’é™¤æ–‡ä»¶\n",
    "    filtered_loader = DirectoryLoader(\n",
    "        \"documents\",\n",
    "        glob=\"*.txt\",\n",
    "        exclude=[\"temp_*\", \"*.tmp\"],\n",
    "        loader_cls=TextLoader,\n",
    "        loader_kwargs={\"encoding\": \"utf-8\"}\n",
    "    )\n",
    "    filtered_docs = filtered_loader.load()\n",
    "\n",
    "    # 6.5 å¹¶è¡ŒåŠ è½½\n",
    "    parallel_loader = DirectoryLoader(\n",
    "        \"documents\",\n",
    "        glob=\"**/*\",\n",
    "        loader_cls=UnstructuredFileLoader,\n",
    "        use_multithreading=True,\n",
    "        max_concurrency=4\n",
    "    )\n",
    "    parallel_docs = parallel_loader.load()"
   ],
   "id": "8577899e5591bf76"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 7. æ•°æ®åº“åŠ è½½å™¨",
   "id": "5058e0a9d84a1083"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from langchain_community.document_loaders import SQLDatabaseLoader\n",
    "from sqlalchemy import create_engine, text\n",
    "\n",
    "def database_loader_examples():\n",
    "    \"\"\"æ•°æ®åº“åŠ è½½å™¨ç¤ºä¾‹\"\"\"\n",
    "\n",
    "    # 7.1 SQLiteæ•°æ®åº“åŠ è½½\n",
    "    engine = create_engine(\"sqlite:///example.db\")\n",
    "\n",
    "    # åˆ›å»ºç¤ºä¾‹è¡¨å’Œæ•°æ®\n",
    "    with engine.connect() as conn:\n",
    "        conn.execute(text(\"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS articles (\n",
    "                id INTEGER PRIMARY KEY,\n",
    "                title TEXT,\n",
    "                content TEXT,\n",
    "                author TEXT,\n",
    "                created_at TIMESTAMP\n",
    "            )\n",
    "        \"\"\"))\n",
    "\n",
    "        conn.execute(text(\"\"\"\n",
    "            INSERT OR REPLACE INTO articles VALUES\n",
    "            (1, 'Pythonæ•™ç¨‹', 'Pythonæ˜¯ä¸€ç§ç¼–ç¨‹è¯­è¨€', 'å¼ ä¸‰', '2024-01-01'),\n",
    "            (2, 'AIå‘å±•', 'äººå·¥æ™ºèƒ½å¿«é€Ÿå‘å±•', 'æå››', '2024-01-02')\n",
    "        \"\"\"))\n",
    "        conn.commit()\n",
    "\n",
    "    # åŠ è½½æ•°æ®åº“å†…å®¹\n",
    "    db_loader = SQLDatabaseLoader(\n",
    "        query=\"SELECT title, content, author FROM articles\",\n",
    "        db=engine,\n",
    "        page_content_columns=[\"title\", \"content\"],\n",
    "        metadata_columns=[\"author\"]\n",
    "    )\n",
    "    docs = db_loader.load()\n",
    "    print(f\"æ•°æ®åº“æ–‡æ¡£æ•°é‡: {len(docs)}\")\n",
    "\n",
    "    # 7.2 PostgreSQLç¤ºä¾‹\n",
    "    # pg_engine = create_engine(\"postgresql://user:password@localhost/dbname\")\n",
    "    # pg_loader = SQLDatabaseLoader(\n",
    "    #     query=\"SELECT * FROM documents WHERE category = 'tech'\",\n",
    "    #     db=pg_engine\n",
    "    # )\n",
    "    # pg_docs = pg_loader.load()\n",
    "\n",
    "    # 7.3 MongoDBåŠ è½½å™¨\n",
    "    from langchain_community.document_loaders import MongodbLoader\n",
    "\n",
    "    # mongodb_loader = MongodbLoader(\n",
    "    #     connection_string=\"mongodb://localhost:27017/\",\n",
    "    #     db_name=\"mydb\",\n",
    "    #     collection_name=\"documents\",\n",
    "    #     filter_criteria={\"status\": \"published\"}\n",
    "    # )\n",
    "    # mongo_docs = mongodb_loader.load()"
   ],
   "id": "c3c2f03c0f1169fc"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 8. äº‘å­˜å‚¨åŠ è½½å™¨",
   "id": "6a8319f1d2c492e0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def cloud_storage_examples():\n",
    "    \"\"\"äº‘å­˜å‚¨åŠ è½½å™¨ç¤ºä¾‹\"\"\"\n",
    "\n",
    "    # 8.1 AWS S3åŠ è½½å™¨\n",
    "    from langchain_community.document_loaders import S3DirectoryLoader, S3FileLoader\n",
    "\n",
    "    # å•ä¸ªS3æ–‡ä»¶\n",
    "    s3_file_loader = S3FileLoader(\n",
    "        bucket=\"my-bucket\",\n",
    "        key=\"documents/report.pdf\"\n",
    "    )\n",
    "    s3_docs = s3_file_loader.load()\n",
    "\n",
    "    # S3ç›®å½•\n",
    "    s3_dir_loader = S3DirectoryLoader(\n",
    "        bucket=\"my-bucket\",\n",
    "        prefix=\"documents/\",\n",
    "        aws_access_key_id=\"your-access-key\",\n",
    "        aws_secret_access_key=\"your-secret-key\"\n",
    "    )\n",
    "    s3_dir_docs = s3_dir_loader.load()\n",
    "\n",
    "    # 8.2 Google DriveåŠ è½½å™¨\n",
    "    from langchain_community.document_loaders import GoogleDriveLoader\n",
    "\n",
    "    # gdrive_loader = GoogleDriveLoader(\n",
    "    #     folder_id=\"your-folder-id\",\n",
    "    #     credentials_path=\"path/to/credentials.json\",\n",
    "    #     token_path=\"path/to/token.json\"\n",
    "    # )\n",
    "    # gdrive_docs = gdrive_loader.load()\n",
    "\n",
    "    # 8.3 Azure Blob Storage\n",
    "    from langchain_community.document_loaders import AzureBlobStorageContainerLoader\n",
    "\n",
    "    # azure_loader = AzureBlobStorageContainerLoader(\n",
    "    #     conn_str=\"your-connection-string\",\n",
    "    #     container=\"documents\"\n",
    "    # )\n",
    "    # azure_docs = azure_loader.load()"
   ],
   "id": "1615b7c80aae4fa7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 9. è‡ªå®šä¹‰æ–‡æ¡£åŠ è½½å™¨",
   "id": "850524eee9b0ec0f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from langchain_core.document_loaders import BaseLoader\n",
    "from langchain_core.documents import Document\n",
    "from typing import List, Iterator\n",
    "import requests\n",
    "\n",
    "class CustomAPILoader(BaseLoader):\n",
    "    \"\"\"è‡ªå®šä¹‰APIåŠ è½½å™¨\"\"\"\n",
    "\n",
    "    def __init__(self, api_url: str, headers: dict = None):\n",
    "        self.api_url = api_url\n",
    "        self.headers = headers or {}\n",
    "\n",
    "    def load(self) -> List[Document]:\n",
    "        \"\"\"åŠ è½½æ–‡æ¡£\"\"\"\n",
    "        response = requests.get(self.api_url, headers=self.headers)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        data = response.json()\n",
    "        documents = []\n",
    "\n",
    "        for item in data.get('items', []):\n",
    "            doc = Document(\n",
    "                page_content=item.get('content', ''),\n",
    "                metadata={\n",
    "                    'source': self.api_url,\n",
    "                    'id': item.get('id'),\n",
    "                    'title': item.get('title'),\n",
    "                    'timestamp': item.get('created_at')\n",
    "                }\n",
    "            )\n",
    "            documents.append(doc)\n",
    "\n",
    "        return documents\n",
    "\n",
    "    def lazy_load(self) -> Iterator[Document]:\n",
    "        \"\"\"æ‡’åŠ è½½æ–‡æ¡£\"\"\"\n",
    "        response = requests.get(self.api_url, headers=self.headers)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        data = response.json()\n",
    "        for item in data.get('items', []):\n",
    "            yield Document(\n",
    "                page_content=item.get('content', ''),\n",
    "                metadata={\n",
    "                    'source': self.api_url,\n",
    "                    'id': item.get('id'),\n",
    "                    'title': item.get('title')\n",
    "                }\n",
    "            )\n",
    "\n",
    "class DatabaseStreamLoader(BaseLoader):\n",
    "    \"\"\"æµå¼æ•°æ®åº“åŠ è½½å™¨\"\"\"\n",
    "\n",
    "    def __init__(self, connection_string: str, query: str, batch_size: int = 1000):\n",
    "        self.connection_string = connection_string\n",
    "        self.query = query\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def lazy_load(self) -> Iterator[Document]:\n",
    "        \"\"\"åˆ†æ‰¹åŠ è½½å¤§é‡æ•°æ®\"\"\"\n",
    "        from sqlalchemy import create_engine, text\n",
    "\n",
    "        engine = create_engine(self.connection_string)\n",
    "        offset = 0\n",
    "\n",
    "        while True:\n",
    "            paginated_query = f\"{self.query} LIMIT {self.batch_size} OFFSET {offset}\"\n",
    "\n",
    "            with engine.connect() as conn:\n",
    "                result = conn.execute(text(paginated_query))\n",
    "                rows = result.fetchall()\n",
    "\n",
    "                if not rows:\n",
    "                    break\n",
    "\n",
    "                for row in rows:\n",
    "                    yield Document(\n",
    "                        page_content=str(row[1]),  # å‡è®¾ç¬¬äºŒåˆ—æ˜¯å†…å®¹\n",
    "                        metadata={\n",
    "                            'id': row[0],  # å‡è®¾ç¬¬ä¸€åˆ—æ˜¯ID\n",
    "                            'source': 'database',\n",
    "                            'batch': offset // self.batch_size\n",
    "                        }\n",
    "                    )\n",
    "\n",
    "                offset += self.batch_size\n",
    "\n",
    "def custom_loader_examples():\n",
    "    \"\"\"è‡ªå®šä¹‰åŠ è½½å™¨ä½¿ç”¨ç¤ºä¾‹\"\"\"\n",
    "\n",
    "    # ä½¿ç”¨è‡ªå®šä¹‰APIåŠ è½½å™¨\n",
    "    api_loader = CustomAPILoader(\n",
    "        api_url=\"https://api.example.com/articles\",\n",
    "        headers={\"Authorization\": \"Bearer your-token\"}\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        api_docs = api_loader.load()\n",
    "        print(f\"APIæ–‡æ¡£æ•°é‡: {len(api_docs)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"APIåŠ è½½å¤±è´¥: {e}\")\n",
    "\n",
    "    # ä½¿ç”¨æµå¼æ•°æ®åº“åŠ è½½å™¨\n",
    "    db_stream_loader = DatabaseStreamLoader(\n",
    "        connection_string=\"sqlite:///large_db.db\",\n",
    "        query=\"SELECT id, content FROM large_table\",\n",
    "        batch_size=500\n",
    "    )\n",
    "\n",
    "    # æ‡’åŠ è½½å¤„ç†å¤§é‡æ•°æ®\n",
    "    for i, doc in enumerate(db_stream_loader.lazy_load()):\n",
    "        if i >= 10:  # åªå¤„ç†å‰10ä¸ªæ–‡æ¡£ä½œä¸ºç¤ºä¾‹\n",
    "            break\n",
    "        print(f\"æ–‡æ¡£ {i}: {doc.page_content[:50]}...\")"
   ],
   "id": "8f910eccf4132fb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 10. å®Œæ•´ä½¿ç”¨ç¤ºä¾‹",
   "id": "39e78bfcdabd7a36"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def complete_document_loader_example():\n",
    "    \"\"\"å®Œæ•´çš„æ–‡æ¡£åŠ è½½å™¨ä½¿ç”¨ç¤ºä¾‹\"\"\"\n",
    "\n",
    "    print(\"ğŸš€ LangChain 0.3 Document Loaders å®Œæ•´ç¤ºä¾‹\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    all_documents = []\n",
    "\n",
    "    # 1. æ–‡æœ¬æ–‡ä»¶\n",
    "    print(\"\\nğŸ“„ åŠ è½½æ–‡æœ¬æ–‡ä»¶...\")\n",
    "    text_docs = text_loader_examples()\n",
    "    all_documents.extend(text_docs)\n",
    "\n",
    "    # 2. CSVæ•°æ®\n",
    "    print(\"\\nğŸ“Š åŠ è½½CSVæ•°æ®...\")\n",
    "    csv_docs = csv_loader_examples()\n",
    "    all_documents.extend(csv_docs)\n",
    "\n",
    "    # 3. JSONæ•°æ®\n",
    "    print(\"\\nğŸ”§ åŠ è½½JSONæ•°æ®...\")\n",
    "    json_docs = json_loader_examples()\n",
    "    all_documents.extend(json_docs)\n",
    "\n",
    "    # 4. ç›®å½•æ‰¹é‡åŠ è½½\n",
    "    print(\"\\nğŸ“ æ‰¹é‡åŠ è½½ç›®å½•...\")\n",
    "    dir_docs = directory_loader_examples()\n",
    "    all_documents.extend(dir_docs)\n",
    "\n",
    "    # 5. æ•°æ®åº“åŠ è½½\n",
    "    print(\"\\nğŸ—„ï¸ åŠ è½½æ•°æ®åº“...\")\n",
    "    db_docs = database_loader_examples()\n",
    "    all_documents.extend(db_docs)\n",
    "\n",
    "    # 6. è‡ªå®šä¹‰åŠ è½½å™¨\n",
    "    print(\"\\nâš™ï¸ è‡ªå®šä¹‰åŠ è½½å™¨...\")\n",
    "    custom_docs = custom_loader_examples()\n",
    "\n",
    "    # ç»Ÿè®¡ä¿¡æ¯\n",
    "    print(f\"\\nğŸ“ˆ åŠ è½½ç»Ÿè®¡:\")\n",
    "    print(f\"æ€»æ–‡æ¡£æ•°é‡: {len(all_documents)}\")\n",
    "\n",
    "    # æŒ‰æ¥æºåˆ†ç»„\n",
    "    sources = {}\n",
    "    for doc in all_documents:\n",
    "        source = doc.metadata.get('source', 'unknown')\n",
    "        sources[source] = sources.get(source, 0) + 1\n",
    "\n",
    "    print(\"æŒ‰æ¥æºåˆ†å¸ƒ:\")\n",
    "    for source, count in sources.items():\n",
    "        print(f\"  {source}: {count} ä¸ªæ–‡æ¡£\")\n",
    "\n",
    "    # å†…å®¹é•¿åº¦ç»Ÿè®¡\n",
    "    lengths = [len(doc.page_content) for doc in all_documents]\n",
    "    if lengths:\n",
    "        print(f\"å†…å®¹é•¿åº¦ç»Ÿè®¡:\")\n",
    "        print(f\"  å¹³å‡é•¿åº¦: {sum(lengths) / len(lengths):.0f} å­—ç¬¦\")\n",
    "        print(f\"  æœ€çŸ­: {min(lengths)} å­—ç¬¦\")\n",
    "        print(f\"  æœ€é•¿: {max(lengths)} å­—ç¬¦\")\n",
    "\n",
    "    return all_documents\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    documents = complete_document_loader_example()\n",
    "\n",
    "    # æ¸…ç†ä¸´æ—¶æ–‡ä»¶\n",
    "    import shutil\n",
    "    for path in [\"sample.txt\", \"employees.csv\", \"articles.json\", \"documents\"]:\n",
    "        if os.path.exists(path):\n",
    "            if os.path.isdir(path):\n",
    "                shutil.rmtree(path)\n",
    "            else:\n",
    "                os.remove(path)\n",
    "\n",
    "    print(\"\\nğŸ§¹ ä¸´æ—¶æ–‡ä»¶å·²æ¸…ç†\")"
   ],
   "id": "93b85cf29d69650b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### æ€»ç»“\n",
    "1. LangChain 0.3 çš„ Document Loaders æä¾›äº†ä¸°å¯Œçš„æ•°æ®æºæ”¯æŒï¼š\n",
    "#### ä¸»è¦ç‰¹ç‚¹ï¼š\n",
    "3. ç»Ÿä¸€çš„ Document æ¥å£\n",
    "4. ä¸°å¯Œçš„æ–‡ä»¶æ ¼å¼æ”¯æŒ\n",
    "5. äº‘å­˜å‚¨é›†æˆ\n",
    "6. è‡ªå®šä¹‰åŠ è½½å™¨æ‰©å±•\n",
    "7. æ‰¹é‡å’Œæµå¼å¤„ç†\n",
    "8. å…ƒæ•°æ®ä¿ç•™\n",
    "\n",
    "#### é€‰æ‹©å»ºè®®ï¼š\n",
    "10. ç®€å•æ–‡æœ¬ï¼šä½¿ç”¨ TextLoader\n",
    "11. PDFæ–‡æ¡£ï¼šæ¨è PyPDFLoader\n",
    "12. ç»“æ„åŒ–æ•°æ®ï¼šä½¿ç”¨ CSVLoader æˆ– JSONLoader\n",
    "13. ç½‘é¡µå†…å®¹ï¼šä½¿ç”¨ WebBaseLoader\n",
    "14. å¤§é‡æ–‡ä»¶ï¼šä½¿ç”¨ DirectoryLoader\n",
    "15. äº‘å­˜å‚¨ï¼šä½¿ç”¨å¯¹åº”çš„äº‘å­˜å‚¨åŠ è½½å™¨\n",
    "16. ç‰¹æ®Šéœ€æ±‚ï¼šå®ç°è‡ªå®šä¹‰åŠ è½½å™¨"
   ],
   "id": "d6e603eca3abf24f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
