{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Data Connection 核心组件",
   "id": "d2588d34d8fa6662"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "* Data Connection 是 LangChain 中处理外部数据的核心模块，包含以下主要组件：\n",
    "* Document Loaders - 文档加载器\n",
    "* Text Splitters - 文本分割器\n",
    "* Embedding Models - 嵌入模型\n",
    "* Vector Stores - 向量存储\n",
    "* Retrievers - 检索器"
   ],
   "id": "1d00345d4349eba6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-22T16:04:43.826163Z",
     "start_time": "2025-07-22T16:04:41.880180Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "LangChain 0.3 Data Connection 完整示例\n",
    "包含文档加载、文本分割、向量化、存储和检索的完整流程\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "from typing import List, Dict, Any\n",
    "import asyncio\n",
    "\n",
    "# 核心导入\n",
    "from langchain_community.document_loaders import (\n",
    "    TextLoader,\n",
    "    PyPDFLoader,\n",
    "    CSVLoader,\n",
    "    JSONLoader,\n",
    "    WebBaseLoader,\n",
    "    DirectoryLoader\n",
    ")\n",
    "from langchain.text_splitter import (\n",
    "    RecursiveCharacterTextSplitter,\n",
    "    CharacterTextSplitter,\n",
    "    TokenTextSplitter,\n",
    "    MarkdownHeaderTextSplitter\n",
    ")\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_community.vectorstores import (\n",
    "    FAISS,\n",
    "    Chroma,\n",
    "    Qdrant\n",
    ")\n",
    "from langchain_core.documents import Document\n",
    "from langchain.retrievers import (\n",
    "    BM25Retriever,\n",
    "    EnsembleRetriever,\n",
    "    MultiQueryRetriever\n",
    ")\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "print(\"✅ 所有库导入成功\")"
   ],
   "id": "ed791ba631680edb",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 所有库导入成功\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 1. Document Loaders 示例",
   "id": "e09a684636812a07"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-22T16:04:47.356957Z",
     "start_time": "2025-07-22T16:04:46.516827Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# 1. Document Loaders 示例\n",
    "def document_loaders_example():\n",
    "    \"\"\"文档加载器示例\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"1. Document Loaders 文档加载器示例\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # 1.1 文本文件加载\n",
    "    print(\"\\n1.1 文本文件加载\")\n",
    "    # 创建示例文本文件\n",
    "    with open(\"sample.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"\"\"\n",
    "        人工智能（AI）是计算机科学的一个分支，致力于创建能够执行通常需要人类智能的任务的系统。\n",
    "        机器学习是AI的一个子集，它使计算机能够从数据中学习而无需明确编程。\n",
    "        深度学习是机器学习的一个子集，使用神经网络来模拟人脑的工作方式。\n",
    "        \"\"\")\n",
    "\n",
    "    loader = TextLoader(\"sample.txt\", encoding=\"utf-8\")\n",
    "    documents = loader.load()\n",
    "    print(f\"加载的文档数量: {len(documents)}\")\n",
    "    print(f\"文档内容预览: {documents[0].page_content[:100]}...\")\n",
    "\n",
    "    # 1.2 CSV文件加载\n",
    "    print(\"\\n1.2 CSV文件加载\")\n",
    "    import pandas as pd\n",
    "\n",
    "    # 创建示例CSV\n",
    "    df = pd.DataFrame({\n",
    "        'name': ['张三', '李四', '王五'],\n",
    "        'age': [25, 30, 35],\n",
    "        'city': ['北京', '上海', '深圳'],\n",
    "        'description': ['软件工程师', '数据科学家', '产品经理']\n",
    "    })\n",
    "    df.to_csv(\"sample.csv\", index=False, encoding=\"utf-8\")\n",
    "\n",
    "    csv_loader = CSVLoader(\"sample.csv\", encoding=\"utf-8\")\n",
    "    csv_docs = csv_loader.load()\n",
    "    print(f\"CSV文档数量: {len(csv_docs)}\")\n",
    "    print(f\"CSV文档示例: {csv_docs[0].page_content}\")\n",
    "\n",
    "    # 1.3 JSON文件加载\n",
    "    print(\"\\n1.3 JSON文件加载\")\n",
    "    import json\n",
    "\n",
    "    sample_data = [\n",
    "        {\"title\": \"Python编程\", \"content\": \"Python是一种高级编程语言\", \"category\": \"技术\"},\n",
    "        {\"title\": \"数据分析\", \"content\": \"数据分析是从数据中提取洞察的过程\", \"category\": \"数据科学\"}\n",
    "    ]\n",
    "\n",
    "    with open(\"sample.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(sample_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    json_loader = JSONLoader(\"sample.json\", jq_schema=\".[].content\")\n",
    "    json_docs = json_loader.load()\n",
    "    print(f\"JSON文档数量: {len(json_docs)}\")\n",
    "    print(f\"JSON文档示例: {json_docs[0].page_content}\")\n",
    "\n",
    "    # 1.4 目录批量加载\n",
    "    print(\"\\n1.4 目录批量加载\")\n",
    "    os.makedirs(\"docs\", exist_ok=True)\n",
    "\n",
    "    # 创建多个文档\n",
    "    for i in range(3):\n",
    "        with open(f\"docs/doc_{i}.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(f\"这是第{i + 1}个文档的内容。包含关于技术{i + 1}的详细信息。\")\n",
    "\n",
    "    dir_loader = DirectoryLoader(\"docs\", glob=\"*.txt\",\n",
    "                                 loader_cls=TextLoader,\n",
    "                                 loader_kwargs={\"encoding\": \"utf-8\"})\n",
    "    dir_docs = dir_loader.load()\n",
    "    print(f\"目录文档数量: {len(dir_docs)}\")\n",
    "\n",
    "    return documents + csv_docs + json_docs + dir_docs\n",
    "\n",
    "documents = document_loaders_example()"
   ],
   "id": "ba3eef49f7f1be9e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "1. Document Loaders 文档加载器示例\n",
      "============================================================\n",
      "\n",
      "1.1 文本文件加载\n",
      "加载的文档数量: 1\n",
      "文档内容预览: \n",
      "        人工智能（AI）是计算机科学的一个分支，致力于创建能够执行通常需要人类智能的任务的系统。\n",
      "        机器学习是AI的一个子集，它使计算机能够从数据中学习而无需明确编程。\n",
      "   ...\n",
      "\n",
      "1.2 CSV文件加载\n",
      "CSV文档数量: 3\n",
      "CSV文档示例: name: 张三\n",
      "age: 25\n",
      "city: 北京\n",
      "description: 软件工程师\n",
      "\n",
      "1.3 JSON文件加载\n",
      "JSON文档数量: 2\n",
      "JSON文档示例: Python是一种高级编程语言\n",
      "\n",
      "1.4 目录批量加载\n",
      "目录文档数量: 3\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 2. Text Splitters 示例",
   "id": "5fd3ca1111ba6401"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-22T16:04:50.352310Z",
     "start_time": "2025-07-22T16:04:50.110133Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# 2. Text Splitters 示例\n",
    "def text_splitters_example(documents: List[Document]):\n",
    "    \"\"\"文本分割器示例\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"2. Text Splitters 文本分割器示例\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # 创建长文本用于分割\n",
    "    long_text = \"\"\"\n",
    "    人工智能的发展历程可以追溯到20世纪50年代。当时，计算机科学家开始探索让机器模拟人类智能的可能性。\n",
    "\n",
    "    在1956年的达特茅斯会议上，人工智能这个术语首次被正式提出。这标志着AI作为一个独立学科的诞生。\n",
    "\n",
    "    随后的几十年里，AI经历了多次起伏。60-70年代是第一个AI春天，专家系统得到了广泛应用。\n",
    "\n",
    "    80年代末到90年代初，由于技术限制和过高期望，AI进入了所谓的\"AI冬天\"。\n",
    "\n",
    "    21世纪以来，随着大数据、云计算和深度学习的发展，AI迎来了新的春天。\n",
    "\n",
    "    今天，AI已经在图像识别、自然语言处理、推荐系统等领域取得了突破性进展。\n",
    "    \"\"\"\n",
    "\n",
    "    long_doc = Document(page_content=long_text, metadata={\"source\": \"ai_history\"})\n",
    "\n",
    "    # 2.1 递归字符分割器（推荐）\n",
    "    print(\"\\n2.1 RecursiveCharacterTextSplitter\")\n",
    "    recursive_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=200,\n",
    "        chunk_overlap=50,\n",
    "        length_function=len,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \"。\", \"，\", \" \", \"\"]\n",
    "    )\n",
    "\n",
    "    recursive_chunks = recursive_splitter.split_documents([long_doc])\n",
    "    print(f\"递归分割块数: {len(recursive_chunks)}\")\n",
    "    for i, chunk in enumerate(recursive_chunks[:2]):\n",
    "        print(f\"块 {i + 1}: {chunk.page_content[:100]}...\")\n",
    "\n",
    "    # 2.2 字符分割器\n",
    "    print(\"\\n2.2 CharacterTextSplitter\")\n",
    "    char_splitter = CharacterTextSplitter(\n",
    "        chunk_size=300,\n",
    "        chunk_overlap=50,\n",
    "        separator=\"\\n\\n\"\n",
    "    )\n",
    "\n",
    "    char_chunks = char_splitter.split_documents([long_doc])\n",
    "    print(f\"字符分割块数: {len(char_chunks)}\")\n",
    "\n",
    "    # 2.3 Token分割器\n",
    "    print(\"\\n2.3 TokenTextSplitter\")\n",
    "    token_splitter = TokenTextSplitter(\n",
    "        chunk_size=100,\n",
    "        chunk_overlap=20\n",
    "    )\n",
    "\n",
    "    token_chunks = token_splitter.split_documents([long_doc])\n",
    "    print(f\"Token分割块数: {len(token_chunks)}\")\n",
    "\n",
    "    # 2.4 Markdown分割器\n",
    "    print(\"\\n2.4 MarkdownHeaderTextSplitter\")\n",
    "    markdown_text = \"\"\"\n",
    "# 人工智能概述\n",
    "\n",
    "## 什么是人工智能\n",
    "人工智能是计算机科学的一个分支。\n",
    "\n",
    "## AI的应用领域\n",
    "\n",
    "### 自然语言处理\n",
    "NLP是AI的重要分支。\n",
    "\n",
    "### 计算机视觉\n",
    "计算机视觉让机器能够\"看见\"。\n",
    "\n",
    "## 未来发展\n",
    "AI将继续快速发展。\n",
    "\"\"\"\n",
    "\n",
    "    markdown_splitter = MarkdownHeaderTextSplitter(\n",
    "        headers_to_split_on=[\n",
    "            (\"#\", \"Header 1\"),\n",
    "            (\"##\", \"Header 2\"),\n",
    "            (\"###\", \"Header 3\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    md_doc = Document(page_content=markdown_text)\n",
    "    md_chunks = markdown_splitter.split_text(markdown_text)\n",
    "    print(f\"Markdown分割块数: {len(md_chunks)}\")\n",
    "\n",
    "    return recursive_chunks\n",
    "# 2. 文本分割\n",
    "chunks = text_splitters_example(documents)"
   ],
   "id": "1279205a511e600e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "2. Text Splitters 文本分割器示例\n",
      "============================================================\n",
      "\n",
      "2.1 RecursiveCharacterTextSplitter\n",
      "递归分割块数: 2\n",
      "块 1: 人工智能的发展历程可以追溯到20世纪50年代。当时，计算机科学家开始探索让机器模拟人类智能的可能性。\n",
      "\n",
      "    在1956年的达特茅斯会议上，人工智能这个术语首次被正式提出。这标志着AI作为一个独立学...\n",
      "块 2: 80年代末到90年代初，由于技术限制和过高期望，AI进入了所谓的\"AI冬天\"。\n",
      "\n",
      "    21世纪以来，随着大数据、云计算和深度学习的发展，AI迎来了新的春天。\n",
      "\n",
      "    今天，AI已经在图像识别、自...\n",
      "\n",
      "2.2 CharacterTextSplitter\n",
      "字符分割块数: 1\n",
      "\n",
      "2.3 TokenTextSplitter\n",
      "Token分割块数: 7\n",
      "\n",
      "2.4 MarkdownHeaderTextSplitter\n",
      "Markdown分割块数: 4\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 3. Embedding Models 示例",
   "id": "88b30c524eb356d8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-22T16:04:55.634387Z",
     "start_time": "2025-07-22T16:04:55.103348Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "# 3. Embedding Models 示例\n",
    "def embedding_models_example():\n",
    "    \"\"\"嵌入模型示例\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"3. Embedding Models 嵌入模型示例\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # 3.1 Ollama嵌入模型\n",
    "    print(\"\\n3.1 Ollama嵌入模型\")\n",
    "    try:\n",
    "        embeddings = OllamaEmbeddings(\n",
    "            base_url=\"http://localhost:11434\",\n",
    "            model=\"nomic-embed-text\"  # 或使用其他嵌入模型\n",
    "        )\n",
    "\n",
    "        # 测试文本\n",
    "        texts = [\n",
    "            \"人工智能是计算机科学的分支\",\n",
    "            \"机器学习是AI的子集\",\n",
    "            \"深度学习使用神经网络\",\n",
    "            \"今天天气很好\"\n",
    "        ]\n",
    "\n",
    "        # 生成嵌入向量\n",
    "        text_embeddings = embeddings.embed_documents(texts)\n",
    "        query_embedding = embeddings.embed_query(\"什么是人工智能？\")\n",
    "\n",
    "        print(f\"文档嵌入数量: {len(text_embeddings)}\")\n",
    "        print(f\"嵌入向量维度: {len(text_embeddings[0])}\")\n",
    "        print(f\"查询嵌入维度: {len(query_embedding)}\")\n",
    "\n",
    "        # 计算相似度\n",
    "        import numpy as np\n",
    "\n",
    "        def cosine_similarity(a, b):\n",
    "            return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "\n",
    "        print(\"\\n相似度计算:\")\n",
    "        for i, text in enumerate(texts):\n",
    "            similarity = cosine_similarity(query_embedding, text_embeddings[i])\n",
    "            print(f\"'{text}' 相似度: {similarity:.4f}\")\n",
    "\n",
    "        return embeddings\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Ollama嵌入模型初始化失败: {e}\")\n",
    "        print(\"请确保Ollama服务正在运行并安装了嵌入模型\")\n",
    "        return None\n",
    "# 3. 嵌入模型\n",
    "embeddings = embedding_models_example()"
   ],
   "id": "95925b2d3a7b8cd2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "3. Embedding Models 嵌入模型示例\n",
      "============================================================\n",
      "\n",
      "3.1 Ollama嵌入模型\n",
      "文档嵌入数量: 4\n",
      "嵌入向量维度: 768\n",
      "查询嵌入维度: 768\n",
      "\n",
      "相似度计算:\n",
      "'人工智能是计算机科学的分支' 相似度: 0.8551\n",
      "'机器学习是AI的子集' 相似度: 0.6135\n",
      "'深度学习使用神经网络' 相似度: 0.5818\n",
      "'今天天气很好' 相似度: 0.5851\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 4. Vector Stores 示例",
   "id": "8127affda365bc76"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-22T16:07:24.198103Z",
     "start_time": "2025-07-22T16:07:17.981530Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "# 4. Vector Stores 示例\n",
    "def vector_stores_example(chunks: List[Document], embeddings):\n",
    "    \"\"\"向量存储示例\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"4. Vector Stores 向量存储示例\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    if embeddings is None:\n",
    "        print(\"跳过向量存储示例（嵌入模型不可用）\")\n",
    "        return None, None\n",
    "\n",
    "    # 4.1 FAISS向量存储\n",
    "    print(\"\\n4.1 FAISS向量存储\")\n",
    "    try:\n",
    "        # 创建FAISS向量存储\n",
    "        faiss_vectorstore = FAISS.from_documents(chunks, embeddings)\n",
    "\n",
    "        # 保存到本地\n",
    "        faiss_vectorstore.save_local(\"faiss_index\")\n",
    "        print(\"✅ FAISS索引已保存\")\n",
    "\n",
    "        # 相似性搜索\n",
    "        query = \"人工智能的发展\"\n",
    "        similar_docs = faiss_vectorstore.similarity_search(query, k=3)\n",
    "\n",
    "        print(f\"\\n查询: '{query}'\")\n",
    "        print(\"相似文档:\")\n",
    "        for i, doc in enumerate(similar_docs):\n",
    "            print(f\"{i + 1}. {doc.page_content[:100]}...\")\n",
    "\n",
    "        # 带分数的相似性搜索\n",
    "        similar_docs_with_scores = faiss_vectorstore.similarity_search_with_score(query, k=3)\n",
    "        print(\"\\n带分数的搜索结果:\")\n",
    "        for i, (doc, score) in enumerate(similar_docs_with_scores):\n",
    "            print(f\"{i + 1}. 分数: {score:.4f} - {doc.page_content[:80]}...\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"FAISS创建失败: {e}\")\n",
    "        faiss_vectorstore = None\n",
    "\n",
    "    # 4.2 Chroma向量存储\n",
    "    print(\"\\n4.2 Chroma向量存储\")\n",
    "    try:\n",
    "        chroma_vectorstore = Chroma.from_documents(\n",
    "            chunks,\n",
    "            embeddings,\n",
    "            persist_directory=\"./chroma_db\"\n",
    "        )\n",
    "\n",
    "        # 持久化\n",
    "        chroma_vectorstore.persist()\n",
    "        print(\"✅ Chroma数据库已持久化\")\n",
    "\n",
    "        # 搜索测试\n",
    "        chroma_results = chroma_vectorstore.similarity_search(\"机器学习\", k=2)\n",
    "        print(f\"Chroma搜索结果数量: {len(chroma_results)}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Chroma创建失败: {e}\")\n",
    "        chroma_vectorstore = None\n",
    "\n",
    "    return faiss_vectorstore, chroma_vectorstore\n",
    "\n",
    "# 4. 向量存储\n",
    "faiss_store, chroma_store = vector_stores_example(chunks, embeddings)"
   ],
   "id": "6e01b3deffa2493e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "4. Vector Stores 向量存储示例\n",
      "============================================================\n",
      "\n",
      "4.1 FAISS向量存储\n",
      "✅ FAISS索引已保存\n",
      "\n",
      "查询: '人工智能的发展'\n",
      "相似文档:\n",
      "1. 人工智能的发展历程可以追溯到20世纪50年代。当时，计算机科学家开始探索让机器模拟人类智能的可能性。\n",
      "\n",
      "    在1956年的达特茅斯会议上，人工智能这个术语首次被正式提出。这标志着AI作为一个独立学...\n",
      "2. 80年代末到90年代初，由于技术限制和过高期望，AI进入了所谓的\"AI冬天\"。\n",
      "\n",
      "    21世纪以来，随着大数据、云计算和深度学习的发展，AI迎来了新的春天。\n",
      "\n",
      "    今天，AI已经在图像识别、自...\n",
      "\n",
      "带分数的搜索结果:\n",
      "1. 分数: 0.4337 - 人工智能的发展历程可以追溯到20世纪50年代。当时，计算机科学家开始探索让机器模拟人类智能的可能性。\n",
      "\n",
      "    在1956年的达特茅斯会议上，人工智能这个术语首...\n",
      "2. 分数: 0.7495 - 80年代末到90年代初，由于技术限制和过高期望，AI进入了所谓的\"AI冬天\"。\n",
      "\n",
      "    21世纪以来，随着大数据、云计算和深度学习的发展，AI迎来了新的春天。...\n",
      "\n",
      "4.2 Chroma向量存储\n",
      "✅ Chroma数据库已持久化\n",
      "Chroma搜索结果数量: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\34769\\AppData\\Local\\Temp\\ipykernel_20760\\3854282456.py:51: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
      "  chroma_vectorstore.persist()\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 5. Retrievers 示例",
   "id": "ad86b09d47e42ea4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-22T16:08:59.479200Z",
     "start_time": "2025-07-22T16:08:52.140594Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "# 5. Retrievers 示例\n",
    "def retrievers_example(vectorstore, chunks: List[Document]):\n",
    "    \"\"\"检索器示例\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"5. Retrievers 检索器示例\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # 5.1 向量存储检索器\n",
    "    print(\"\\n5.1 向量存储检索器\")\n",
    "    if vectorstore:\n",
    "        vector_retriever = vectorstore.as_retriever(\n",
    "            search_type=\"similarity\",\n",
    "            search_kwargs={\"k\": 3}\n",
    "        )\n",
    "\n",
    "        results = vector_retriever.invoke(\"人工智能的应用\")\n",
    "        print(f\"向量检索结果数量: {len(results)}\")\n",
    "        for i, doc in enumerate(results):\n",
    "            print(f\"{i + 1}. {doc.page_content[:100]}...\")\n",
    "\n",
    "    # 5.2 BM25检索器\n",
    "    print(\"\\n5.2 BM25检索器\")\n",
    "    try:\n",
    "        bm25_retriever = BM25Retriever.from_documents(chunks)\n",
    "        bm25_retriever.k = 3\n",
    "\n",
    "        bm25_results = bm25_retriever.invoke(\"人工智能发展\")\n",
    "        print(f\"BM25检索结果数量: {len(bm25_results)}\")\n",
    "        for i, doc in enumerate(bm25_results):\n",
    "            print(f\"{i + 1}. {doc.page_content[:100]}...\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"BM25检索器创建失败: {e}\")\n",
    "        bm25_retriever = None\n",
    "\n",
    "    # 5.3 集成检索器\n",
    "    print(\"\\n5.3 集成检索器\")\n",
    "    if vectorstore and bm25_retriever:\n",
    "        try:\n",
    "            ensemble_retriever = EnsembleRetriever(\n",
    "                retrievers=[vector_retriever, bm25_retriever],\n",
    "                weights=[0.7, 0.3]  # 向量搜索权重0.7，BM25权重0.3\n",
    "            )\n",
    "\n",
    "            ensemble_results = ensemble_retriever.invoke(\"机器学习技术\")\n",
    "            print(f\"集成检索结果数量: {len(ensemble_results)}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"集成检索器创建失败: {e}\")\n",
    "\n",
    "    # 5.4 多查询检索器\n",
    "    print(\"\\n5.4 多查询检索器\")\n",
    "    if vectorstore:\n",
    "        try:\n",
    "            llm = ChatOllama(\n",
    "                base_url=\"http://localhost:11434\",\n",
    "                model=\"gemma3:4b\"\n",
    "            )\n",
    "\n",
    "            multi_query_retriever = MultiQueryRetriever.from_llm(\n",
    "                retriever=vector_retriever,\n",
    "                llm=llm\n",
    "            )\n",
    "\n",
    "            multi_results = multi_query_retriever.invoke(\"AI的未来发展趋势\")\n",
    "            print(f\"多查询检索结果数量: {len(multi_results)}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"多查询检索器创建失败: {e}\")\n",
    "# 5. 检索器\n",
    "retrievers_example(faiss_store, chunks)"
   ],
   "id": "f14c88e8a32df855",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "5. Retrievers 检索器示例\n",
      "============================================================\n",
      "\n",
      "5.1 向量存储检索器\n",
      "向量检索结果数量: 2\n",
      "1. 人工智能的发展历程可以追溯到20世纪50年代。当时，计算机科学家开始探索让机器模拟人类智能的可能性。\n",
      "\n",
      "    在1956年的达特茅斯会议上，人工智能这个术语首次被正式提出。这标志着AI作为一个独立学...\n",
      "2. 80年代末到90年代初，由于技术限制和过高期望，AI进入了所谓的\"AI冬天\"。\n",
      "\n",
      "    21世纪以来，随着大数据、云计算和深度学习的发展，AI迎来了新的春天。\n",
      "\n",
      "    今天，AI已经在图像识别、自...\n",
      "\n",
      "5.2 BM25检索器\n",
      "BM25检索结果数量: 2\n",
      "1. 80年代末到90年代初，由于技术限制和过高期望，AI进入了所谓的\"AI冬天\"。\n",
      "\n",
      "    21世纪以来，随着大数据、云计算和深度学习的发展，AI迎来了新的春天。\n",
      "\n",
      "    今天，AI已经在图像识别、自...\n",
      "2. 人工智能的发展历程可以追溯到20世纪50年代。当时，计算机科学家开始探索让机器模拟人类智能的可能性。\n",
      "\n",
      "    在1956年的达特茅斯会议上，人工智能这个术语首次被正式提出。这标志着AI作为一个独立学...\n",
      "\n",
      "5.3 集成检索器\n",
      "集成检索结果数量: 2\n",
      "\n",
      "5.4 多查询检索器\n",
      "多查询检索结果数量: 2\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 6. 完整RAG流程示例",
   "id": "f90a79c3cfcb7331"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-22T16:09:36.054903Z",
     "start_time": "2025-07-22T16:09:27.884710Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# 6. 完整RAG流程示例\n",
    "def complete_rag_example():\n",
    "    \"\"\"完整的RAG流程示例\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"6. 完整RAG流程示例\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    try:\n",
    "        # 6.1 准备数据\n",
    "        documents = [\n",
    "            Document(page_content=\"LangChain是一个用于构建LLM应用的框架\", metadata={\"source\": \"doc1\"}),\n",
    "            Document(page_content=\"向量数据库可以存储和检索高维向量\", metadata={\"source\": \"doc2\"}),\n",
    "            Document(page_content=\"RAG结合了检索和生成，提高了AI回答的准确性\", metadata={\"source\": \"doc3\"}),\n",
    "            Document(page_content=\"嵌入模型将文本转换为数值向量表示\", metadata={\"source\": \"doc4\"})\n",
    "        ]\n",
    "\n",
    "        # 6.2 文本分割\n",
    "        splitter = RecursiveCharacterTextSplitter(chunk_size=100, chunk_overlap=20)\n",
    "        chunks = splitter.split_documents(documents)\n",
    "\n",
    "        # 6.3 创建嵌入和向量存储\n",
    "        embeddings = OllamaEmbeddings(\n",
    "            base_url=\"http://localhost:11434\",\n",
    "            model=\"nomic-embed-text\"\n",
    "        )\n",
    "\n",
    "        vectorstore = FAISS.from_documents(chunks, embeddings)\n",
    "        retriever = vectorstore.as_retriever(k=2)\n",
    "\n",
    "        # 6.4 创建RAG链\n",
    "        from langchain_core.prompts import ChatPromptTemplate\n",
    "        from langchain_core.output_parsers import StrOutputParser\n",
    "        from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "        llm = ChatOllama(\n",
    "            base_url=\"http://localhost:11434\",\n",
    "            model=\"gemma3:4b\"\n",
    "        )\n",
    "\n",
    "        prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "        基于以下上下文回答问题：\n",
    "\n",
    "        上下文：{context}\n",
    "\n",
    "        问题：{question}\n",
    "\n",
    "        请提供准确、简洁的回答：\n",
    "        \"\"\")\n",
    "\n",
    "        def format_docs(docs):\n",
    "            return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "        rag_chain = (\n",
    "                {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "                | prompt\n",
    "                | llm\n",
    "                | StrOutputParser()\n",
    "        )\n",
    "\n",
    "        # 6.5 测试RAG系统\n",
    "        questions = [\n",
    "            \"什么是LangChain？\",\n",
    "            \"向量数据库的作用是什么？\",\n",
    "            \"RAG技术有什么优势？\"\n",
    "        ]\n",
    "\n",
    "        for question in questions:\n",
    "            print(f\"\\n问题: {question}\")\n",
    "            answer = rag_chain.invoke(question)\n",
    "            print(f\"回答: {answer}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"RAG流程执行失败: {e}\")\n",
    "complete_rag_example()"
   ],
   "id": "e78b5eb261c8ae41",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "6. 完整RAG流程示例\n",
      "============================================================\n",
      "\n",
      "问题: 什么是LangChain？\n",
      "回答: LangChain是一个用于构建LLM应用的框架。\n",
      "\n",
      "\n",
      "问题: 向量数据库的作用是什么？\n",
      "回答: 向量数据库的作用是存储和检索高维向量。\n",
      "\n",
      "\n",
      "问题: RAG技术有什么优势？\n",
      "回答: RAG技术的优势在于它提高了AI回答的准确性。\n",
      "\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 7. 高级功能示例",
   "id": "7af8f010e7932d07"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-22T16:09:51.206635Z",
     "start_time": "2025-07-22T16:09:51.200341Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# 7. 高级功能示例\n",
    "def advanced_features_example():\n",
    "    \"\"\"高级功能示例\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"7. 高级功能示例\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # 7.1 自定义文档加载器\n",
    "    print(\"\\n7.1 自定义文档加载器\")\n",
    "\n",
    "    class CustomLoader:\n",
    "        def __init__(self, data_source):\n",
    "            self.data_source = data_source\n",
    "\n",
    "        def load(self):\n",
    "            # 模拟从API或数据库加载数据\n",
    "            documents = []\n",
    "            for i, item in enumerate(self.data_source):\n",
    "                doc = Document(\n",
    "                    page_content=item[\"content\"],\n",
    "                    metadata={\"id\": i, \"type\": item[\"type\"]}\n",
    "                )\n",
    "                documents.append(doc)\n",
    "            return documents\n",
    "\n",
    "    # 使用自定义加载器\n",
    "    custom_data = [\n",
    "        {\"content\": \"Python是一种编程语言\", \"type\": \"技术\"},\n",
    "        {\"content\": \"数据科学需要统计知识\", \"type\": \"科学\"},\n",
    "        {\"content\": \"机器学习算法很重要\", \"type\": \"AI\"}\n",
    "    ]\n",
    "\n",
    "    custom_loader = CustomLoader(custom_data)\n",
    "    custom_docs = custom_loader.load()\n",
    "    print(f\"自定义加载器文档数量: {len(custom_docs)}\")\n",
    "\n",
    "    # 7.2 文档过滤和预处理\n",
    "    print(\"\\n7.2 文档过滤和预处理\")\n",
    "\n",
    "    def preprocess_documents(documents):\n",
    "        \"\"\"文档预处理函数\"\"\"\n",
    "        processed_docs = []\n",
    "        for doc in documents:\n",
    "            # 清理文本\n",
    "            content = doc.page_content.strip()\n",
    "            content = content.replace(\"\\n\", \" \").replace(\"\\t\", \" \")\n",
    "\n",
    "            # 过滤短文档\n",
    "            if len(content) > 10:\n",
    "                doc.page_content = content\n",
    "                processed_docs.append(doc)\n",
    "\n",
    "        return processed_docs\n",
    "\n",
    "    processed_docs = preprocess_documents(custom_docs)\n",
    "    print(f\"预处理后文档数量: {len(processed_docs)}\")\n",
    "advanced_features_example()"
   ],
   "id": "1fd34ff050068f21",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "7. 高级功能示例\n",
      "============================================================\n",
      "\n",
      "7.1 自定义文档加载器\n",
      "自定义加载器文档数量: 3\n",
      "\n",
      "7.2 文档过滤和预处理\n",
      "预处理后文档数量: 1\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 8. 性能优化示例",
   "id": "7bddd0a43c7a49f4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-22T16:10:06.180453Z",
     "start_time": "2025-07-22T16:10:06.170023Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 8. 性能优化示例\n",
    "async def performance_optimization_example():\n",
    "    \"\"\"性能优化示例\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"8. 性能优化示例\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # 8.1 批量处理\n",
    "    print(\"\\n8.1 批量嵌入处理\")\n",
    "\n",
    "    try:\n",
    "        embeddings = OllamaEmbeddings(\n",
    "            base_url=\"http://localhost:11434\",\n",
    "            model=\"nomic-embed-text\"\n",
    "        )\n",
    "\n",
    "        # 大量文本\n",
    "        texts = [f\"这是第{i}个文档的内容\" for i in range(10)]\n",
    "\n",
    "        # 批量生成嵌入\n",
    "        batch_embeddings = embeddings.embed_documents(texts)\n",
    "        print(f\"批量处理文档数量: {len(batch_embeddings)}\")\n",
    "\n",
    "        # 8.2 异步处理\n",
    "        print(\"\\n8.2 异步处理示例\")\n",
    "\n",
    "        async def async_embed_text(text):\n",
    "            # 模拟异步嵌入\n",
    "            await asyncio.sleep(0.1)\n",
    "            return embeddings.embed_query(text)\n",
    "\n",
    "        # 并发处理\n",
    "        tasks = [async_embed_text(f\"异步文本{i}\") for i in range(5)]\n",
    "        results = await asyncio.gather(*tasks)\n",
    "        print(f\"异步处理结果数量: {len(results)}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"性能优化示例失败: {e}\")\n",
    "performance_optimization_example()\n"
   ],
   "id": "35a7d46e00e601f",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<coroutine object performance_optimization_example at 0x000002743F9A9BE0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 主函数",
   "id": "2e5e75e1f7ea7e1f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-22T16:10:14.713908Z",
     "start_time": "2025-07-22T16:10:14.709565Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# 主函数\n",
    "def main():\n",
    "    \"\"\"运行所有示例\"\"\"\n",
    "    print(\"🚀 LangChain 0.3 Data Connection 完整示例\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    # 1. 文档加载\n",
    "    documents = document_loaders_example()\n",
    "\n",
    "    # 2. 文本分割\n",
    "    chunks = text_splitters_example(documents)\n",
    "\n",
    "    # 3. 嵌入模型\n",
    "    embeddings = embedding_models_example()\n",
    "\n",
    "    # 4. 向量存储\n",
    "    faiss_store, chroma_store = vector_stores_example(chunks, embeddings)\n",
    "\n",
    "    # 5. 检索器\n",
    "    retrievers_example(faiss_store, chunks)\n",
    "\n",
    "    # 6. 完整RAG流程\n",
    "    complete_rag_example()\n",
    "\n",
    "    # 7. 高级功能\n",
    "    advanced_features_example()\n",
    "\n",
    "    print(\"\\n🎉 所有示例运行完成！\")\n",
    "\n",
    "    # 清理临时文件\n",
    "    cleanup_files()\n",
    "\n",
    "\n",
    "def cleanup_files():\n",
    "    \"\"\"清理临时文件\"\"\"\n",
    "    import shutil\n",
    "\n",
    "    files_to_remove = [\"sample.txt\", \"sample.csv\", \"sample.json\"]\n",
    "    dirs_to_remove = [\"docs\", \"faiss_index\", \"chroma_db\"]\n",
    "\n",
    "    for file in files_to_remove:\n",
    "        if os.path.exists(file):\n",
    "            os.remove(file)\n",
    "\n",
    "    for dir in dirs_to_remove:\n",
    "        if os.path.exists(dir):\n",
    "            shutil.rmtree(dir)\n",
    "\n",
    "    print(\"🧹 临时文件已清理\")\n"
   ],
   "id": "7f6e6f544e1599ac",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-22T16:10:35.612852Z",
     "start_time": "2025-07-22T16:10:17.081402Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 运行同步示例\n",
    "    main()\n",
    "\n",
    "    # 运行异步示例\n",
    "    # asyncio.run(performance_optimization_example())"
   ],
   "id": "1588268b0cb2662",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 LangChain 0.3 Data Connection 完整示例\n",
      "================================================================================\n",
      "\n",
      "============================================================\n",
      "1. Document Loaders 文档加载器示例\n",
      "============================================================\n",
      "\n",
      "1.1 文本文件加载\n",
      "加载的文档数量: 1\n",
      "文档内容预览: \n",
      "        人工智能（AI）是计算机科学的一个分支，致力于创建能够执行通常需要人类智能的任务的系统。\n",
      "        机器学习是AI的一个子集，它使计算机能够从数据中学习而无需明确编程。\n",
      "   ...\n",
      "\n",
      "1.2 CSV文件加载\n",
      "CSV文档数量: 3\n",
      "CSV文档示例: name: 张三\n",
      "age: 25\n",
      "city: 北京\n",
      "description: 软件工程师\n",
      "\n",
      "1.3 JSON文件加载\n",
      "JSON文档数量: 2\n",
      "JSON文档示例: Python是一种高级编程语言\n",
      "\n",
      "1.4 目录批量加载\n",
      "目录文档数量: 3\n",
      "\n",
      "============================================================\n",
      "2. Text Splitters 文本分割器示例\n",
      "============================================================\n",
      "\n",
      "2.1 RecursiveCharacterTextSplitter\n",
      "递归分割块数: 2\n",
      "块 1: 人工智能的发展历程可以追溯到20世纪50年代。当时，计算机科学家开始探索让机器模拟人类智能的可能性。\n",
      "\n",
      "    在1956年的达特茅斯会议上，人工智能这个术语首次被正式提出。这标志着AI作为一个独立学...\n",
      "块 2: 80年代末到90年代初，由于技术限制和过高期望，AI进入了所谓的\"AI冬天\"。\n",
      "\n",
      "    21世纪以来，随着大数据、云计算和深度学习的发展，AI迎来了新的春天。\n",
      "\n",
      "    今天，AI已经在图像识别、自...\n",
      "\n",
      "2.2 CharacterTextSplitter\n",
      "字符分割块数: 1\n",
      "\n",
      "2.3 TokenTextSplitter\n",
      "Token分割块数: 7\n",
      "\n",
      "2.4 MarkdownHeaderTextSplitter\n",
      "Markdown分割块数: 4\n",
      "\n",
      "============================================================\n",
      "3. Embedding Models 嵌入模型示例\n",
      "============================================================\n",
      "\n",
      "3.1 Ollama嵌入模型\n",
      "文档嵌入数量: 4\n",
      "嵌入向量维度: 768\n",
      "查询嵌入维度: 768\n",
      "\n",
      "相似度计算:\n",
      "'人工智能是计算机科学的分支' 相似度: 0.8551\n",
      "'机器学习是AI的子集' 相似度: 0.6135\n",
      "'深度学习使用神经网络' 相似度: 0.5818\n",
      "'今天天气很好' 相似度: 0.5851\n",
      "\n",
      "============================================================\n",
      "4. Vector Stores 向量存储示例\n",
      "============================================================\n",
      "\n",
      "4.1 FAISS向量存储\n",
      "✅ FAISS索引已保存\n",
      "\n",
      "查询: '人工智能的发展'\n",
      "相似文档:\n",
      "1. 人工智能的发展历程可以追溯到20世纪50年代。当时，计算机科学家开始探索让机器模拟人类智能的可能性。\n",
      "\n",
      "    在1956年的达特茅斯会议上，人工智能这个术语首次被正式提出。这标志着AI作为一个独立学...\n",
      "2. 80年代末到90年代初，由于技术限制和过高期望，AI进入了所谓的\"AI冬天\"。\n",
      "\n",
      "    21世纪以来，随着大数据、云计算和深度学习的发展，AI迎来了新的春天。\n",
      "\n",
      "    今天，AI已经在图像识别、自...\n",
      "\n",
      "带分数的搜索结果:\n",
      "1. 分数: 0.4337 - 人工智能的发展历程可以追溯到20世纪50年代。当时，计算机科学家开始探索让机器模拟人类智能的可能性。\n",
      "\n",
      "    在1956年的达特茅斯会议上，人工智能这个术语首...\n",
      "2. 分数: 0.7495 - 80年代末到90年代初，由于技术限制和过高期望，AI进入了所谓的\"AI冬天\"。\n",
      "\n",
      "    21世纪以来，随着大数据、云计算和深度学习的发展，AI迎来了新的春天。...\n",
      "\n",
      "4.2 Chroma向量存储\n",
      "✅ Chroma数据库已持久化\n",
      "Chroma搜索结果数量: 2\n",
      "\n",
      "============================================================\n",
      "5. Retrievers 检索器示例\n",
      "============================================================\n",
      "\n",
      "5.1 向量存储检索器\n",
      "向量检索结果数量: 2\n",
      "1. 人工智能的发展历程可以追溯到20世纪50年代。当时，计算机科学家开始探索让机器模拟人类智能的可能性。\n",
      "\n",
      "    在1956年的达特茅斯会议上，人工智能这个术语首次被正式提出。这标志着AI作为一个独立学...\n",
      "2. 80年代末到90年代初，由于技术限制和过高期望，AI进入了所谓的\"AI冬天\"。\n",
      "\n",
      "    21世纪以来，随着大数据、云计算和深度学习的发展，AI迎来了新的春天。\n",
      "\n",
      "    今天，AI已经在图像识别、自...\n",
      "\n",
      "5.2 BM25检索器\n",
      "BM25检索结果数量: 2\n",
      "1. 80年代末到90年代初，由于技术限制和过高期望，AI进入了所谓的\"AI冬天\"。\n",
      "\n",
      "    21世纪以来，随着大数据、云计算和深度学习的发展，AI迎来了新的春天。\n",
      "\n",
      "    今天，AI已经在图像识别、自...\n",
      "2. 人工智能的发展历程可以追溯到20世纪50年代。当时，计算机科学家开始探索让机器模拟人类智能的可能性。\n",
      "\n",
      "    在1956年的达特茅斯会议上，人工智能这个术语首次被正式提出。这标志着AI作为一个独立学...\n",
      "\n",
      "5.3 集成检索器\n",
      "集成检索结果数量: 2\n",
      "\n",
      "5.4 多查询检索器\n",
      "多查询检索结果数量: 2\n",
      "\n",
      "============================================================\n",
      "6. 完整RAG流程示例\n",
      "============================================================\n",
      "\n",
      "问题: 什么是LangChain？\n",
      "回答: LangChain是一个用于构建LLM应用的框架。\n",
      "\n",
      "\n",
      "问题: 向量数据库的作用是什么？\n",
      "回答: 向量数据库的作用是存储和检索高维向量。\n",
      "\n",
      "\n",
      "问题: RAG技术有什么优势？\n",
      "回答: RAG技术通过结合检索和生成，提高了AI回答的准确性。\n",
      "\n",
      "\n",
      "============================================================\n",
      "7. 高级功能示例\n",
      "============================================================\n",
      "\n",
      "7.1 自定义文档加载器\n",
      "自定义加载器文档数量: 3\n",
      "\n",
      "7.2 文档过滤和预处理\n",
      "预处理后文档数量: 1\n",
      "\n",
      "🎉 所有示例运行完成！\n"
     ]
    },
    {
     "ename": "PermissionError",
     "evalue": "[WinError 32] 另一个程序正在使用此文件，进程无法访问。: 'chroma_db\\\\44933678-68e8-40b1-b8d1-9afc146a6630\\\\data_level0.bin'",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mPermissionError\u001B[39m                           Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[15]\u001B[39m\u001B[32m, line 3\u001B[39m\n\u001B[32m      1\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[34m__name__\u001B[39m == \u001B[33m\"\u001B[39m\u001B[33m__main__\u001B[39m\u001B[33m\"\u001B[39m:\n\u001B[32m      2\u001B[39m     \u001B[38;5;66;03m# 运行同步示例\u001B[39;00m\n\u001B[32m----> \u001B[39m\u001B[32m3\u001B[39m     \u001B[43mmain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m      5\u001B[39m     \u001B[38;5;66;03m# 运行异步示例\u001B[39;00m\n\u001B[32m      6\u001B[39m     \u001B[38;5;66;03m# asyncio.run(performance_optimization_example())\u001B[39;00m\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[14]\u001B[39m\u001B[32m, line 31\u001B[39m, in \u001B[36mmain\u001B[39m\u001B[34m()\u001B[39m\n\u001B[32m     28\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m🎉 所有示例运行完成！\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m     30\u001B[39m \u001B[38;5;66;03m# 清理临时文件\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m31\u001B[39m \u001B[43mcleanup_files\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[14]\u001B[39m\u001B[32m, line 47\u001B[39m, in \u001B[36mcleanup_files\u001B[39m\u001B[34m()\u001B[39m\n\u001B[32m     45\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m \u001B[38;5;28mdir\u001B[39m \u001B[38;5;129;01min\u001B[39;00m dirs_to_remove:\n\u001B[32m     46\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m os.path.exists(\u001B[38;5;28mdir\u001B[39m):\n\u001B[32m---> \u001B[39m\u001B[32m47\u001B[39m         \u001B[43mshutil\u001B[49m\u001B[43m.\u001B[49m\u001B[43mrmtree\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mdir\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[32m     49\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33m\"\u001B[39m\u001B[33m🧹 临时文件已清理\u001B[39m\u001B[33m\"\u001B[39m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\shutil.py:758\u001B[39m, in \u001B[36mrmtree\u001B[39m\u001B[34m(path, ignore_errors, onerror, dir_fd)\u001B[39m\n\u001B[32m    756\u001B[39m     \u001B[38;5;66;03m# can't continue even if onerror hook returns\u001B[39;00m\n\u001B[32m    757\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m758\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_rmtree_unsafe\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpath\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43monerror\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\shutil.py:616\u001B[39m, in \u001B[36m_rmtree_unsafe\u001B[39m\u001B[34m(path, onerror)\u001B[39m\n\u001B[32m    614\u001B[39m         onerror(os.path.islink, fullname, sys.exc_info())\n\u001B[32m    615\u001B[39m         \u001B[38;5;28;01mcontinue\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m616\u001B[39m     \u001B[43m_rmtree_unsafe\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfullname\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43monerror\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    617\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    618\u001B[39m     \u001B[38;5;28;01mtry\u001B[39;00m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\shutil.py:621\u001B[39m, in \u001B[36m_rmtree_unsafe\u001B[39m\u001B[34m(path, onerror)\u001B[39m\n\u001B[32m    619\u001B[39m             os.unlink(fullname)\n\u001B[32m    620\u001B[39m         \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mOSError\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m621\u001B[39m             \u001B[43monerror\u001B[49m\u001B[43m(\u001B[49m\u001B[43mos\u001B[49m\u001B[43m.\u001B[49m\u001B[43munlink\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfullname\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msys\u001B[49m\u001B[43m.\u001B[49m\u001B[43mexc_info\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    622\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m    623\u001B[39m     os.rmdir(path)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\shutil.py:619\u001B[39m, in \u001B[36m_rmtree_unsafe\u001B[39m\u001B[34m(path, onerror)\u001B[39m\n\u001B[32m    617\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    618\u001B[39m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m619\u001B[39m         os.unlink(fullname)\n\u001B[32m    620\u001B[39m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mOSError\u001B[39;00m:\n\u001B[32m    621\u001B[39m         onerror(os.unlink, fullname, sys.exc_info())\n",
      "\u001B[31mPermissionError\u001B[39m: [WinError 32] 另一个程序正在使用此文件，进程无法访问。: 'chroma_db\\\\44933678-68e8-40b1-b8d1-9afc146a6630\\\\data_level0.bin'"
     ]
    }
   ],
   "execution_count": 15
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
