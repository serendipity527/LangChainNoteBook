{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Data Connection æ ¸å¿ƒç»„ä»¶",
   "id": "d2588d34d8fa6662"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "* Data Connection æ˜¯ LangChain ä¸­å¤„ç†å¤–éƒ¨æ•°æ®çš„æ ¸å¿ƒæ¨¡å—ï¼ŒåŒ…å«ä»¥ä¸‹ä¸»è¦ç»„ä»¶ï¼š\n",
    "* Document Loaders - æ–‡æ¡£åŠ è½½å™¨\n",
    "* Text Splitters - æ–‡æœ¬åˆ†å‰²å™¨\n",
    "* Embedding Models - åµŒå…¥æ¨¡å‹\n",
    "* Vector Stores - å‘é‡å­˜å‚¨\n",
    "* Retrievers - æ£€ç´¢å™¨"
   ],
   "id": "1d00345d4349eba6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T08:40:09.251671Z",
     "start_time": "2025-07-23T08:40:08.277635Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "LangChain 0.3 Data Connection å®Œæ•´ç¤ºä¾‹\n",
    "åŒ…å«æ–‡æ¡£åŠ è½½ã€æ–‡æœ¬åˆ†å‰²ã€å‘é‡åŒ–ã€å­˜å‚¨å’Œæ£€ç´¢çš„å®Œæ•´æµç¨‹\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "from typing import List, Dict, Any\n",
    "import asyncio\n",
    "\n",
    "# æ ¸å¿ƒå¯¼å…¥\n",
    "from langchain_community.document_loaders import (\n",
    "    TextLoader,\n",
    "    PyPDFLoader,\n",
    "    CSVLoader,\n",
    "    JSONLoader,\n",
    "    WebBaseLoader,\n",
    "    DirectoryLoader\n",
    ")\n",
    "from langchain.text_splitter import (\n",
    "    RecursiveCharacterTextSplitter,\n",
    "    CharacterTextSplitter,\n",
    "    TokenTextSplitter,\n",
    "    MarkdownHeaderTextSplitter\n",
    ")\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_community.vectorstores import (\n",
    "    FAISS,\n",
    "    Chroma,\n",
    "    Qdrant\n",
    ")\n",
    "from langchain_core.documents import Document\n",
    "from langchain.retrievers import (\n",
    "    BM25Retriever,\n",
    "    EnsembleRetriever,\n",
    "    MultiQueryRetriever\n",
    ")\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "print(\"âœ… æ‰€æœ‰åº“å¯¼å…¥æˆåŠŸ\")"
   ],
   "id": "ed791ba631680edb",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… æ‰€æœ‰åº“å¯¼å…¥æˆåŠŸ\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 1. Document Loaders ç¤ºä¾‹",
   "id": "e09a684636812a07"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 1. æ–‡æœ¬æ–‡ä»¶åŠ è½½å™¨",
   "id": "5a516641a4f1e1d0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T03:18:31.722562Z",
     "start_time": "2025-07-23T03:18:31.715330Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_core.documents import Document\n",
    "import os\n",
    "\n",
    "def text_loader_examples():\n",
    "    \"\"\"æ–‡æœ¬æ–‡ä»¶åŠ è½½å™¨ç¤ºä¾‹\"\"\"\n",
    "\n",
    "    # 1.1 åŸºç¡€æ–‡æœ¬åŠ è½½\n",
    "    with open(\"sample.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"äººå·¥æ™ºèƒ½æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªåˆ†æ”¯ã€‚\\næœºå™¨å­¦ä¹ æ˜¯AIçš„å­é›†ã€‚\")\n",
    "\n",
    "    loader = TextLoader(\"sample.txt\", encoding=\"utf-8\")\n",
    "    documents = loader.load()\n",
    "    print(f\"æ–‡æ¡£æ•°é‡: {len(documents)}\")\n",
    "    print(f\"å†…å®¹: {documents[0].page_content}\")\n",
    "    print(f\"å…ƒæ•°æ®: {documents[0].metadata}\")\n",
    "\n",
    "    # # 1.2 å¤„ç†å¤§æ–‡ä»¶\n",
    "    # loader_large = TextLoader(\"large_file.txt\", encoding=\"utf-8\")\n",
    "    # try:\n",
    "    #     docs = loader_large.load()\n",
    "    #     print(f\"å¤§æ–‡ä»¶åŠ è½½æˆåŠŸï¼Œæ–‡æ¡£æ•°: {len(docs)}\")\n",
    "    # except Exception as e:\n",
    "    #     print(f\"åŠ è½½å¤±è´¥: {e}\")\n",
    "\n",
    "    # # 1.3 è‡ªåŠ¨ç¼–ç æ£€æµ‹\n",
    "    # loader_auto = TextLoader(\"file.txt\", autodetect_encoding=True)\n",
    "    # docs = loader_auto.load()\n",
    "text_loader_examples()"
   ],
   "id": "b21efe1b5e41c515",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ–‡æ¡£æ•°é‡: 1\n",
      "å†…å®¹: äººå·¥æ™ºèƒ½æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªåˆ†æ”¯ã€‚\n",
      "æœºå™¨å­¦ä¹ æ˜¯AIçš„å­é›†ã€‚\n",
      "å…ƒæ•°æ®: {'source': 'sample.txt'}\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 2. PDF æ–‡æ¡£åŠ è½½å™¨",
   "id": "7be3a16ae64b0f31"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T03:25:40.450347Z",
     "start_time": "2025-07-23T03:25:36.529217Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader, PDFMinerLoader, PDFPlumberLoader\n",
    "\n",
    "def pdf_loader_examples():\n",
    "    \"\"\"PDFåŠ è½½å™¨ç¤ºä¾‹\"\"\"\n",
    "\n",
    "    # 2.1 PyPDFLoader - æœ€å¸¸ç”¨\n",
    "    pdf_loader = PyPDFLoader(\"docs/Multi-level Wavelet-CNN for Image Restoration.pdf\")\n",
    "    pages = pdf_loader.load()\n",
    "    print(f\"PDFé¡µæ•°: {len(pages)}\")\n",
    "\n",
    "    for i, page in enumerate(pages[:2]):\n",
    "        print(f\"ç¬¬{i+1}é¡µå†…å®¹: {page.page_content[:100]}...\")\n",
    "        print(f\"é¡µé¢å…ƒæ•°æ®: {page.metadata}\")\n",
    "\n",
    "    # 2.2 PDFMinerLoader - æ›´å¥½çš„æ–‡æœ¬æå–\n",
    "    pdf_miner_loader = PDFMinerLoader(\"docs/Multi-level Wavelet-CNN for Image Restoration.pdf\")\n",
    "    docs = pdf_miner_loader.load()\n",
    "\n",
    "    # 2.3 PDFPlumberLoader - è¡¨æ ¼å¤„ç†æ›´å¥½\n",
    "    pdf_plumber_loader = PDFPlumberLoader(\"docs/Multi-level Wavelet-CNN for Image Restoration.pdf\")\n",
    "    docs = pdf_plumber_loader.load()\n",
    "\n",
    "    # 2.4 åˆ†é¡µåŠ è½½\n",
    "    pdf_loader = PyPDFLoader(\"docs/Multi-level Wavelet-CNN for Image Restoration.pdf\")\n",
    "    pages = pdf_loader.load_and_split()\n",
    "\n",
    "    # # 2.5 å¯†ç ä¿æŠ¤çš„PDF\n",
    "    # protected_loader = PyPDFLoader(\"docs/Multi-level Wavelet-CNN for Image Restoration.pdf\", password=\"password123\")\n",
    "    # docs = protected_loader.load()\n",
    "\n",
    "pdf_loader_examples()"
   ],
   "id": "31f4771d335c1e14",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDFé¡µæ•°: 10\n",
      "ç¬¬1é¡µå†…å®¹: Multi-level Wavelet-CNN for Image Restoration\n",
      "Pengju Liu1, Hongzhi Zhang âˆ—1, Kai Zhang1, Liang Lin2,...\n",
      "é¡µé¢å…ƒæ•°æ®: {'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2018-05-23T00:19:39+00:00', 'author': '', 'keywords': '', 'moddate': '2018-05-23T00:19:39+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'docs/Multi-level Wavelet-CNN for Image Restoration.pdf', 'total_pages': 10, 'page': 0, 'page_label': '1'}\n",
      "ç¬¬2é¡µå†…å®¹: is adopted to enlarge receptive ï¬eld without the sacriï¬ce\n",
      "of computational cost. Dilated ï¬ltering, h...\n",
      "é¡µé¢å…ƒæ•°æ®: {'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2018-05-23T00:19:39+00:00', 'author': '', 'keywords': '', 'moddate': '2018-05-23T00:19:39+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'docs/Multi-level Wavelet-CNN for Image Restoration.pdf', 'total_pages': 10, 'page': 1, 'page_label': '2'}\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 3. CSV æ•°æ®åŠ è½½å™¨",
   "id": "e13650d820fdd7bd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from langchain_community.document_loaders import CSVLoader\n",
    "import pandas as pd\n",
    "\n",
    "def csv_loader_examples():\n",
    "    \"\"\"CSVåŠ è½½å™¨ç¤ºä¾‹\"\"\"\n",
    "\n",
    "    # åˆ›å»ºç¤ºä¾‹CSV\n",
    "    df = pd.DataFrame({\n",
    "        'name': ['å¼ ä¸‰', 'æå››', 'ç‹äº”'],\n",
    "        'age': [25, 30, 35],\n",
    "        'department': ['æŠ€æœ¯éƒ¨', 'é”€å”®éƒ¨', 'å¸‚åœºéƒ¨'],\n",
    "        'description': ['Pythonå¼€å‘å·¥ç¨‹å¸ˆ', 'é”€å”®ç»ç†', 'å¸‚åœºä¸“å‘˜']\n",
    "    })\n",
    "    df.to_csv(\"employees.csv\", index=False, encoding=\"utf-8\")\n",
    "\n",
    "    # 3.1 åŸºç¡€CSVåŠ è½½\n",
    "    csv_loader = CSVLoader(\"employees.csv\", encoding=\"utf-8\")\n",
    "    docs = csv_loader.load()\n",
    "    print(f\"CSVæ–‡æ¡£æ•°é‡: {len(docs)}\")\n",
    "    print(f\"ç¬¬ä¸€æ¡è®°å½•: {docs[0].page_content}\")\n",
    "\n",
    "    # 3.2 æŒ‡å®šæºåˆ—\n",
    "    csv_loader_with_source = CSVLoader(\n",
    "        \"employees.csv\",\n",
    "        source_column=\"name\",\n",
    "        encoding=\"utf-8\"\n",
    "    )\n",
    "    docs = csv_loader_with_source.load()\n",
    "\n",
    "    # 3.3 è‡ªå®šä¹‰CSVå‚æ•°\n",
    "    csv_loader_custom = CSVLoader(\n",
    "        \"employees.csv\",\n",
    "        csv_args={\n",
    "            'delimiter': ',',\n",
    "            'quotechar': '\"',\n",
    "            'fieldnames': ['å§“å', 'å¹´é¾„', 'éƒ¨é—¨', 'æè¿°']\n",
    "        }\n",
    "    )\n",
    "    docs = csv_loader_custom.load()\n",
    "\n",
    "    # 3.4 è¿‡æ»¤ç‰¹å®šåˆ—\n",
    "    csv_loader_filtered = CSVLoader(\n",
    "        \"employees.csv\",\n",
    "        content_columns=['name', 'description'],\n",
    "        encoding=\"utf-8\"\n",
    "    )\n",
    "    docs = csv_loader_filtered.load()\n",
    "csv_loader_examples()"
   ],
   "id": "e6361d2440059a5f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 4. JSON æ•°æ®åŠ è½½å™¨",
   "id": "9ded7230b64c406b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from langchain_community.document_loaders import JSONLoader\n",
    "import json\n",
    "\n",
    "def json_loader_examples():\n",
    "    \"\"\"JSONåŠ è½½å™¨ç¤ºä¾‹\"\"\"\n",
    "\n",
    "    # åˆ›å»ºç¤ºä¾‹JSONæ•°æ®\n",
    "    data = [\n",
    "        {\n",
    "            \"id\": 1,\n",
    "            \"title\": \"Pythonç¼–ç¨‹æŒ‡å—\",\n",
    "            \"content\": \"Pythonæ˜¯ä¸€ç§é«˜çº§ç¼–ç¨‹è¯­è¨€ï¼Œè¯­æ³•ç®€æ´ä¼˜é›…ã€‚\",\n",
    "            \"author\": \"å¼ ä¸‰\",\n",
    "            \"tags\": [\"ç¼–ç¨‹\", \"Python\", \"æ•™ç¨‹\"]\n",
    "        },\n",
    "        {\n",
    "            \"id\": 2,\n",
    "            \"title\": \"æœºå™¨å­¦ä¹ å…¥é—¨\",\n",
    "            \"content\": \"æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªé‡è¦åˆ†æ”¯ã€‚\",\n",
    "            \"author\": \"æå››\",\n",
    "            \"tags\": [\"AI\", \"æœºå™¨å­¦ä¹ \", \"æ•°æ®ç§‘å­¦\"]\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    with open(\"articles.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    # 4.1 æå–ç‰¹å®šå­—æ®µ\n",
    "    json_loader = JSONLoader(\n",
    "        \"articles.json\",\n",
    "        jq_schema=\".[].content\",\n",
    "        text_content=False\n",
    "    )\n",
    "    docs = json_loader.load()\n",
    "    print(f\"JSONæ–‡æ¡£æ•°é‡: {len(docs)}\")\n",
    "\n",
    "    # 4.2 æå–å¤šä¸ªå­—æ®µ\n",
    "    json_loader_multi = JSONLoader(\n",
    "        \"articles.json\",\n",
    "        jq_schema=\".[]\",\n",
    "        content_key=\"content\"\n",
    "    )\n",
    "    docs = json_loader_multi.load()\n",
    "\n",
    "    # 4.3 å¤æ‚JSONç»“æ„\n",
    "    complex_data = {\n",
    "        \"articles\": {\n",
    "            \"tech\": [\n",
    "                {\"title\": \"AIå‘å±•\", \"body\": \"äººå·¥æ™ºèƒ½å¿«é€Ÿå‘å±•\"},\n",
    "                {\"title\": \"äº‘è®¡ç®—\", \"body\": \"äº‘è®¡ç®—æ”¹å˜äº†ITæ¶æ„\"}\n",
    "            ],\n",
    "            \"business\": [\n",
    "                {\"title\": \"æ•°å­—åŒ–è½¬å‹\", \"body\": \"ä¼ä¸šæ•°å­—åŒ–è½¬å‹åŠ¿åœ¨å¿…è¡Œ\"}\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "\n",
    "    with open(\"complex.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(complex_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    # æå–åµŒå¥—æ•°æ®\n",
    "    json_loader_nested = JSONLoader(\n",
    "        \"complex.json\",\n",
    "        jq_schema=\".articles.tech[].body\"\n",
    "    )\n",
    "    docs = json_loader_nested.load()\n",
    "\n",
    "    # 4.4 JSONLæ ¼å¼\n",
    "    jsonl_data = [\n",
    "        {\"text\": \"ç¬¬ä¸€è¡Œæ•°æ®\", \"label\": \"A\"},\n",
    "        {\"text\": \"ç¬¬äºŒè¡Œæ•°æ®\", \"label\": \"B\"}\n",
    "    ]\n",
    "\n",
    "    with open(\"data.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
    "        for item in jsonl_data:\n",
    "            f.write(json.dumps(item, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "    from langchain_community.document_loaders import JSONLinesLoader\n",
    "    jsonl_loader = JSONLinesLoader(\"data.jsonl\", jq_schema=\".text\")\n",
    "    docs = jsonl_loader.load()"
   ],
   "id": "c7f3dd2d242dc05b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 5. ç½‘é¡µå†…å®¹åŠ è½½å™¨",
   "id": "3d0a379fe033b34"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader, AsyncHtmlLoader\n",
    "from langchain_community.document_transformers import Html2TextTransformer\n",
    "\n",
    "def web_loader_examples():\n",
    "    \"\"\"ç½‘é¡µåŠ è½½å™¨ç¤ºä¾‹\"\"\"\n",
    "\n",
    "    # 5.1 åŸºç¡€ç½‘é¡µåŠ è½½\n",
    "    web_loader = WebBaseLoader(\"https://example.com\")\n",
    "    docs = web_loader.load()\n",
    "    print(f\"ç½‘é¡µæ–‡æ¡£: {docs[0].page_content[:200]}...\")\n",
    "\n",
    "    # 5.2 å¤šä¸ªURLæ‰¹é‡åŠ è½½\n",
    "    urls = [\n",
    "        \"https://example.com/page1\",\n",
    "        \"https://example.com/page2\",\n",
    "        \"https://example.com/page3\"\n",
    "    ]\n",
    "    web_loader_multi = WebBaseLoader(urls)\n",
    "    docs = web_loader_multi.load()\n",
    "\n",
    "    # 5.3 è‡ªå®šä¹‰è¯·æ±‚å¤´\n",
    "    web_loader_headers = WebBaseLoader(\n",
    "        \"https://api.example.com/data\",\n",
    "        header_template={\n",
    "            \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\",\n",
    "            \"Authorization\": \"Bearer your-token\"\n",
    "        }\n",
    "    )\n",
    "    docs = web_loader_headers.load()\n",
    "\n",
    "    # 5.4 CSSé€‰æ‹©å™¨è¿‡æ»¤\n",
    "    from bs4 import BeautifulSoup\n",
    "\n",
    "    web_loader_css = WebBaseLoader(\n",
    "        \"https://news.example.com\",\n",
    "        bs_kwargs={\n",
    "            \"parse_only\": BeautifulSoup.SoupStrainer(\"div\", {\"class\": \"article-content\"})\n",
    "        }\n",
    "    )\n",
    "    docs = web_loader_css.load()\n",
    "\n",
    "    # 5.5 å¼‚æ­¥ç½‘é¡µåŠ è½½\n",
    "    async def async_web_loading():\n",
    "        urls = [\"https://example.com/1\", \"https://example.com/2\"]\n",
    "        async_loader = AsyncHtmlLoader(urls)\n",
    "        html_docs = async_loader.load()\n",
    "\n",
    "        # HTMLè½¬æ–‡æœ¬\n",
    "        html2text = Html2TextTransformer()\n",
    "        text_docs = html2text.transform_documents(html_docs)\n",
    "        return text_docs\n",
    "\n",
    "    # 5.6 å¤„ç†JavaScriptæ¸²æŸ“é¡µé¢\n",
    "    from langchain_community.document_loaders import SeleniumURLLoader\n",
    "\n",
    "    selenium_loader = SeleniumURLLoader(\n",
    "        urls=[\"https://spa-example.com\"],\n",
    "        browser=\"chrome\",\n",
    "        headless=True\n",
    "    )\n",
    "    docs = selenium_loader.load()"
   ],
   "id": "be4ec69a05ebcd5c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 6. ç›®å½•æ‰¹é‡åŠ è½½å™¨",
   "id": "9f794fef8c49c6a6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "from langchain_community.document_loaders import TextLoader, PyPDFLoader, CSVLoader\n",
    "\n",
    "def directory_loader_examples():\n",
    "    \"\"\"ç›®å½•åŠ è½½å™¨ç¤ºä¾‹\"\"\"\n",
    "\n",
    "    # åˆ›å»ºæµ‹è¯•ç›®å½•ç»“æ„\n",
    "    os.makedirs(\"documents/texts\", exist_ok=True)\n",
    "    os.makedirs(\"documents/pdfs\", exist_ok=True)\n",
    "    os.makedirs(\"documents/data\", exist_ok=True)\n",
    "\n",
    "    # åˆ›å»ºæµ‹è¯•æ–‡ä»¶\n",
    "    for i in range(3):\n",
    "        with open(f\"documents/texts/doc_{i}.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(f\"è¿™æ˜¯æ–‡æ¡£{i}çš„å†…å®¹ï¼ŒåŒ…å«é‡è¦ä¿¡æ¯ã€‚\")\n",
    "\n",
    "    # 6.1 åŠ è½½ç‰¹å®šç±»å‹æ–‡ä»¶\n",
    "    txt_loader = DirectoryLoader(\n",
    "        \"documents/texts\",\n",
    "        glob=\"*.txt\",\n",
    "        loader_cls=TextLoader,\n",
    "        loader_kwargs={\"encoding\": \"utf-8\"}\n",
    "    )\n",
    "    txt_docs = txt_loader.load()\n",
    "    print(f\"æ–‡æœ¬æ–‡æ¡£æ•°é‡: {len(txt_docs)}\")\n",
    "\n",
    "    # 6.2 å¤šç§æ–‡ä»¶ç±»å‹æ··åˆåŠ è½½\n",
    "    from langchain_community.document_loaders import UnstructuredFileLoader\n",
    "\n",
    "    mixed_loader = DirectoryLoader(\n",
    "        \"documents\",\n",
    "        glob=\"**/*\",  # é€’å½’æœç´¢\n",
    "        loader_cls=UnstructuredFileLoader,\n",
    "        recursive=True,\n",
    "        show_progress=True\n",
    "    )\n",
    "    mixed_docs = mixed_loader.load()\n",
    "\n",
    "    # 6.3 è‡ªå®šä¹‰æ–‡ä»¶ç±»å‹æ˜ å°„\n",
    "    def get_loader_for_file(file_path: str):\n",
    "        if file_path.endswith('.txt'):\n",
    "            return TextLoader(file_path, encoding=\"utf-8\")\n",
    "        elif file_path.endswith('.pdf'):\n",
    "            return PyPDFLoader(file_path)\n",
    "        elif file_path.endswith('.csv'):\n",
    "            return CSVLoader(file_path, encoding=\"utf-8\")\n",
    "        else:\n",
    "            return UnstructuredFileLoader(file_path)\n",
    "\n",
    "    # 6.4 è¿‡æ»¤å’Œæ’é™¤æ–‡ä»¶\n",
    "    filtered_loader = DirectoryLoader(\n",
    "        \"documents\",\n",
    "        glob=\"*.txt\",\n",
    "        exclude=[\"temp_*\", \"*.tmp\"],\n",
    "        loader_cls=TextLoader,\n",
    "        loader_kwargs={\"encoding\": \"utf-8\"}\n",
    "    )\n",
    "    filtered_docs = filtered_loader.load()\n",
    "\n",
    "    # 6.5 å¹¶è¡ŒåŠ è½½\n",
    "    parallel_loader = DirectoryLoader(\n",
    "        \"documents\",\n",
    "        glob=\"**/*\",\n",
    "        loader_cls=UnstructuredFileLoader,\n",
    "        use_multithreading=True,\n",
    "        max_concurrency=4\n",
    "    )\n",
    "    parallel_docs = parallel_loader.load()"
   ],
   "id": "d450f59175737713"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 7. æ•°æ®åº“åŠ è½½å™¨",
   "id": "dc1720b2b32a1d88"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from langchain_community.document_loaders import SQLDatabaseLoader\n",
    "from sqlalchemy import create_engine, text\n",
    "\n",
    "def database_loader_examples():\n",
    "    \"\"\"æ•°æ®åº“åŠ è½½å™¨ç¤ºä¾‹\"\"\"\n",
    "\n",
    "    # 7.1 SQLiteæ•°æ®åº“åŠ è½½\n",
    "    engine = create_engine(\"sqlite:///example.db\")\n",
    "\n",
    "    # åˆ›å»ºç¤ºä¾‹è¡¨å’Œæ•°æ®\n",
    "    with engine.connect() as conn:\n",
    "        conn.execute(text(\"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS articles (\n",
    "                id INTEGER PRIMARY KEY,\n",
    "                title TEXT,\n",
    "                content TEXT,\n",
    "                author TEXT,\n",
    "                created_at TIMESTAMP\n",
    "            )\n",
    "        \"\"\"))\n",
    "\n",
    "        conn.execute(text(\"\"\"\n",
    "            INSERT OR REPLACE INTO articles VALUES\n",
    "            (1, 'Pythonæ•™ç¨‹', 'Pythonæ˜¯ä¸€ç§ç¼–ç¨‹è¯­è¨€', 'å¼ ä¸‰', '2024-01-01'),\n",
    "            (2, 'AIå‘å±•', 'äººå·¥æ™ºèƒ½å¿«é€Ÿå‘å±•', 'æå››', '2024-01-02')\n",
    "        \"\"\"))\n",
    "        conn.commit()\n",
    "\n",
    "    # åŠ è½½æ•°æ®åº“å†…å®¹\n",
    "    db_loader = SQLDatabaseLoader(\n",
    "        query=\"SELECT title, content, author FROM articles\",\n",
    "        db=engine,\n",
    "        page_content_columns=[\"title\", \"content\"],\n",
    "        metadata_columns=[\"author\"]\n",
    "    )\n",
    "    docs = db_loader.load()\n",
    "    print(f\"æ•°æ®åº“æ–‡æ¡£æ•°é‡: {len(docs)}\")\n",
    "\n",
    "    # 7.2 PostgreSQLç¤ºä¾‹\n",
    "    # pg_engine = create_engine(\"postgresql://user:password@localhost/dbname\")\n",
    "    # pg_loader = SQLDatabaseLoader(\n",
    "    #     query=\"SELECT * FROM documents WHERE category = 'tech'\",\n",
    "    #     db=pg_engine\n",
    "    # )\n",
    "    # pg_docs = pg_loader.load()\n",
    "\n",
    "    # 7.3 MongoDBåŠ è½½å™¨\n",
    "    from langchain_community.document_loaders import MongodbLoader\n",
    "\n",
    "    # mongodb_loader = MongodbLoader(\n",
    "    #     connection_string=\"mongodb://localhost:27017/\",\n",
    "    #     db_name=\"mydb\",\n",
    "    #     collection_name=\"documents\",\n",
    "    #     filter_criteria={\"status\": \"published\"}\n",
    "    # )\n",
    "    # mongo_docs = mongodb_loader.load()"
   ],
   "id": "b58217d2dad099d5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 8. äº‘å­˜å‚¨åŠ è½½å™¨",
   "id": "16cfd1e55688358"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def cloud_storage_examples():\n",
    "    \"\"\"äº‘å­˜å‚¨åŠ è½½å™¨ç¤ºä¾‹\"\"\"\n",
    "\n",
    "    # 8.1 AWS S3åŠ è½½å™¨\n",
    "    from langchain_community.document_loaders import S3DirectoryLoader, S3FileLoader\n",
    "\n",
    "    # å•ä¸ªS3æ–‡ä»¶\n",
    "    s3_file_loader = S3FileLoader(\n",
    "        bucket=\"my-bucket\",\n",
    "        key=\"documents/report.pdf\"\n",
    "    )\n",
    "    s3_docs = s3_file_loader.load()\n",
    "\n",
    "    # S3ç›®å½•\n",
    "    s3_dir_loader = S3DirectoryLoader(\n",
    "        bucket=\"my-bucket\",\n",
    "        prefix=\"documents/\",\n",
    "        aws_access_key_id=\"your-access-key\",\n",
    "        aws_secret_access_key=\"your-secret-key\"\n",
    "    )\n",
    "    s3_dir_docs = s3_dir_loader.load()\n",
    "\n",
    "    # 8.2 Google DriveåŠ è½½å™¨\n",
    "    from langchain_community.document_loaders import GoogleDriveLoader\n",
    "\n",
    "    # gdrive_loader = GoogleDriveLoader(\n",
    "    #     folder_id=\"your-folder-id\",\n",
    "    #     credentials_path=\"path/to/credentials.json\",\n",
    "    #     token_path=\"path/to/token.json\"\n",
    "    # )\n",
    "    # gdrive_docs = gdrive_loader.load()\n",
    "\n",
    "    # 8.3 Azure Blob Storage\n",
    "    from langchain_community.document_loaders import AzureBlobStorageContainerLoader\n",
    "\n",
    "    # azure_loader = AzureBlobStorageContainerLoader(\n",
    "    #     conn_str=\"your-connection-string\",\n",
    "    #     container=\"documents\"\n",
    "    # )\n",
    "    # azure_docs = azure_loader.load()"
   ],
   "id": "cc9720e9a8356866"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 9. è‡ªå®šä¹‰æ–‡æ¡£åŠ è½½å™¨",
   "id": "9da531146d384b92"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from langchain_core.document_loaders import BaseLoader\n",
    "from langchain_core.documents import Document\n",
    "from typing import List, Iterator\n",
    "import requests\n",
    "\n",
    "class CustomAPILoader(BaseLoader):\n",
    "    \"\"\"è‡ªå®šä¹‰APIåŠ è½½å™¨\"\"\"\n",
    "\n",
    "    def __init__(self, api_url: str, headers: dict = None):\n",
    "        self.api_url = api_url\n",
    "        self.headers = headers or {}\n",
    "\n",
    "    def load(self) -> List[Document]:\n",
    "        \"\"\"åŠ è½½æ–‡æ¡£\"\"\"\n",
    "        response = requests.get(self.api_url, headers=self.headers)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        data = response.json()\n",
    "        documents = []\n",
    "\n",
    "        for item in data.get('items', []):\n",
    "            doc = Document(\n",
    "                page_content=item.get('content', ''),\n",
    "                metadata={\n",
    "                    'source': self.api_url,\n",
    "                    'id': item.get('id'),\n",
    "                    'title': item.get('title'),\n",
    "                    'timestamp': item.get('created_at')\n",
    "                }\n",
    "            )\n",
    "            documents.append(doc)\n",
    "\n",
    "        return documents\n",
    "\n",
    "    def lazy_load(self) -> Iterator[Document]:\n",
    "        \"\"\"æ‡’åŠ è½½æ–‡æ¡£\"\"\"\n",
    "        response = requests.get(self.api_url, headers=self.headers)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        data = response.json()\n",
    "        for item in data.get('items', []):\n",
    "            yield Document(\n",
    "                page_content=item.get('content', ''),\n",
    "                metadata={\n",
    "                    'source': self.api_url,\n",
    "                    'id': item.get('id'),\n",
    "                    'title': item.get('title')\n",
    "                }\n",
    "            )\n",
    "\n",
    "class DatabaseStreamLoader(BaseLoader):\n",
    "    \"\"\"æµå¼æ•°æ®åº“åŠ è½½å™¨\"\"\"\n",
    "\n",
    "    def __init__(self, connection_string: str, query: str, batch_size: int = 1000):\n",
    "        self.connection_string = connection_string\n",
    "        self.query = query\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def lazy_load(self) -> Iterator[Document]:\n",
    "        \"\"\"åˆ†æ‰¹åŠ è½½å¤§é‡æ•°æ®\"\"\"\n",
    "        from sqlalchemy import create_engine, text\n",
    "\n",
    "        engine = create_engine(self.connection_string)\n",
    "        offset = 0\n",
    "\n",
    "        while True:\n",
    "            paginated_query = f\"{self.query} LIMIT {self.batch_size} OFFSET {offset}\"\n",
    "\n",
    "            with engine.connect() as conn:\n",
    "                result = conn.execute(text(paginated_query))\n",
    "                rows = result.fetchall()\n",
    "\n",
    "                if not rows:\n",
    "                    break\n",
    "\n",
    "                for row in rows:\n",
    "                    yield Document(\n",
    "                        page_content=str(row[1]),  # å‡è®¾ç¬¬äºŒåˆ—æ˜¯å†…å®¹\n",
    "                        metadata={\n",
    "                            'id': row[0],  # å‡è®¾ç¬¬ä¸€åˆ—æ˜¯ID\n",
    "                            'source': 'database',\n",
    "                            'batch': offset // self.batch_size\n",
    "                        }\n",
    "                    )\n",
    "\n",
    "                offset += self.batch_size\n",
    "\n",
    "def custom_loader_examples():\n",
    "    \"\"\"è‡ªå®šä¹‰åŠ è½½å™¨ä½¿ç”¨ç¤ºä¾‹\"\"\"\n",
    "\n",
    "    # ä½¿ç”¨è‡ªå®šä¹‰APIåŠ è½½å™¨\n",
    "    api_loader = CustomAPILoader(\n",
    "        api_url=\"https://api.example.com/articles\",\n",
    "        headers={\"Authorization\": \"Bearer your-token\"}\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        api_docs = api_loader.load()\n",
    "        print(f\"APIæ–‡æ¡£æ•°é‡: {len(api_docs)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"APIåŠ è½½å¤±è´¥: {e}\")\n",
    "\n",
    "    # ä½¿ç”¨æµå¼æ•°æ®åº“åŠ è½½å™¨\n",
    "    db_stream_loader = DatabaseStreamLoader(\n",
    "        connection_string=\"sqlite:///large_db.db\",\n",
    "        query=\"SELECT id, content FROM large_table\",\n",
    "        batch_size=500\n",
    "    )\n",
    "\n",
    "    # æ‡’åŠ è½½å¤„ç†å¤§é‡æ•°æ®\n",
    "    for i, doc in enumerate(db_stream_loader.lazy_load()):\n",
    "        if i >= 10:  # åªå¤„ç†å‰10ä¸ªæ–‡æ¡£ä½œä¸ºç¤ºä¾‹\n",
    "            break\n",
    "        print(f\"æ–‡æ¡£ {i}: {doc.page_content[:50]}...\")"
   ],
   "id": "6705f6f795ac930b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 10. å®Œæ•´ä½¿ç”¨ç¤ºä¾‹",
   "id": "2ff9e60626b983a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def complete_document_loader_example():\n",
    "    \"\"\"å®Œæ•´çš„æ–‡æ¡£åŠ è½½å™¨ä½¿ç”¨ç¤ºä¾‹\"\"\"\n",
    "\n",
    "    print(\"ğŸš€ LangChain 0.3 Document Loaders å®Œæ•´ç¤ºä¾‹\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    all_documents = []\n",
    "\n",
    "    # 1. æ–‡æœ¬æ–‡ä»¶\n",
    "    print(\"\\nğŸ“„ åŠ è½½æ–‡æœ¬æ–‡ä»¶...\")\n",
    "    text_docs = text_loader_examples()\n",
    "    all_documents.extend(text_docs)\n",
    "\n",
    "    # 2. CSVæ•°æ®\n",
    "    print(\"\\nğŸ“Š åŠ è½½CSVæ•°æ®...\")\n",
    "    csv_docs = csv_loader_examples()\n",
    "    all_documents.extend(csv_docs)\n",
    "\n",
    "    # 3. JSONæ•°æ®\n",
    "    print(\"\\nğŸ”§ åŠ è½½JSONæ•°æ®...\")\n",
    "    json_docs = json_loader_examples()\n",
    "    all_documents.extend(json_docs)\n",
    "\n",
    "    # 4. ç›®å½•æ‰¹é‡åŠ è½½\n",
    "    print(\"\\nğŸ“ æ‰¹é‡åŠ è½½ç›®å½•...\")\n",
    "    dir_docs = directory_loader_examples()\n",
    "    all_documents.extend(dir_docs)\n",
    "\n",
    "    # 5. æ•°æ®åº“åŠ è½½\n",
    "    print(\"\\nğŸ—„ï¸ åŠ è½½æ•°æ®åº“...\")\n",
    "    db_docs = database_loader_examples()\n",
    "    all_documents.extend(db_docs)\n",
    "\n",
    "    # 6. è‡ªå®šä¹‰åŠ è½½å™¨\n",
    "    print(\"\\nâš™ï¸ è‡ªå®šä¹‰åŠ è½½å™¨...\")\n",
    "    custom_docs = custom_loader_examples()\n",
    "\n",
    "    # ç»Ÿè®¡ä¿¡æ¯\n",
    "    print(f\"\\nğŸ“ˆ åŠ è½½ç»Ÿè®¡:\")\n",
    "    print(f\"æ€»æ–‡æ¡£æ•°é‡: {len(all_documents)}\")\n",
    "\n",
    "    # æŒ‰æ¥æºåˆ†ç»„\n",
    "    sources = {}\n",
    "    for doc in all_documents:\n",
    "        source = doc.metadata.get('source', 'unknown')\n",
    "        sources[source] = sources.get(source, 0) + 1\n",
    "\n",
    "    print(\"æŒ‰æ¥æºåˆ†å¸ƒ:\")\n",
    "    for source, count in sources.items():\n",
    "        print(f\"  {source}: {count} ä¸ªæ–‡æ¡£\")\n",
    "\n",
    "    # å†…å®¹é•¿åº¦ç»Ÿè®¡\n",
    "    lengths = [len(doc.page_content) for doc in all_documents]\n",
    "    if lengths:\n",
    "        print(f\"å†…å®¹é•¿åº¦ç»Ÿè®¡:\")\n",
    "        print(f\"  å¹³å‡é•¿åº¦: {sum(lengths) / len(lengths):.0f} å­—ç¬¦\")\n",
    "        print(f\"  æœ€çŸ­: {min(lengths)} å­—ç¬¦\")\n",
    "        print(f\"  æœ€é•¿: {max(lengths)} å­—ç¬¦\")\n",
    "\n",
    "    return all_documents\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    documents = complete_document_loader_example()\n",
    "\n",
    "    # æ¸…ç†ä¸´æ—¶æ–‡ä»¶\n",
    "    import shutil\n",
    "    for path in [\"sample.txt\", \"employees.csv\", \"articles.json\", \"documents\"]:\n",
    "        if os.path.exists(path):\n",
    "            if os.path.isdir(path):\n",
    "                shutil.rmtree(path)\n",
    "            else:\n",
    "                os.remove(path)\n",
    "\n",
    "    print(\"\\nğŸ§¹ ä¸´æ—¶æ–‡ä»¶å·²æ¸…ç†\")"
   ],
   "id": "555928003009f244"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### æ€»ç»“\n",
    "1. LangChain 0.3 çš„ Document Loaders æä¾›äº†ä¸°å¯Œçš„æ•°æ®æºæ”¯æŒï¼š\n",
    "#### ä¸»è¦ç‰¹ç‚¹ï¼š\n",
    "3. ç»Ÿä¸€çš„ Document æ¥å£\n",
    "4. ä¸°å¯Œçš„æ–‡ä»¶æ ¼å¼æ”¯æŒ\n",
    "5. äº‘å­˜å‚¨é›†æˆ\n",
    "6. è‡ªå®šä¹‰åŠ è½½å™¨æ‰©å±•\n",
    "7. æ‰¹é‡å’Œæµå¼å¤„ç†\n",
    "8. å…ƒæ•°æ®ä¿ç•™\n",
    "\n",
    "#### é€‰æ‹©å»ºè®®ï¼š\n",
    "10. ç®€å•æ–‡æœ¬ï¼šä½¿ç”¨ TextLoader\n",
    "11. PDFæ–‡æ¡£ï¼šæ¨è PyPDFLoader\n",
    "12. ç»“æ„åŒ–æ•°æ®ï¼šä½¿ç”¨ CSVLoader æˆ– JSONLoader\n",
    "13. ç½‘é¡µå†…å®¹ï¼šä½¿ç”¨ WebBaseLoader\n",
    "14. å¤§é‡æ–‡ä»¶ï¼šä½¿ç”¨ DirectoryLoader\n",
    "15. äº‘å­˜å‚¨ï¼šä½¿ç”¨å¯¹åº”çš„äº‘å­˜å‚¨åŠ è½½å™¨\n",
    "16. ç‰¹æ®Šéœ€æ±‚ï¼šå®ç°è‡ªå®šä¹‰åŠ è½½å™¨"
   ],
   "id": "bd2a088707c0f236"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T03:31:32.029515Z",
     "start_time": "2025-07-23T03:31:32.012522Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# 1. Document Loaders ç¤ºä¾‹\n",
    "def document_loaders_example():\n",
    "    \"\"\"æ–‡æ¡£åŠ è½½å™¨ç¤ºä¾‹\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"1. Document Loaders æ–‡æ¡£åŠ è½½å™¨ç¤ºä¾‹\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # 1.1 æ–‡æœ¬æ–‡ä»¶åŠ è½½\n",
    "    print(\"\\n1.1 æ–‡æœ¬æ–‡ä»¶åŠ è½½\")\n",
    "    # åˆ›å»ºç¤ºä¾‹æ–‡æœ¬æ–‡ä»¶\n",
    "    with open(\"sample.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"\"\"\n",
    "        äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œè‡´åŠ›äºåˆ›å»ºèƒ½å¤Ÿæ‰§è¡Œé€šå¸¸éœ€è¦äººç±»æ™ºèƒ½çš„ä»»åŠ¡çš„ç³»ç»Ÿã€‚\n",
    "        æœºå™¨å­¦ä¹ æ˜¯AIçš„ä¸€ä¸ªå­é›†ï¼Œå®ƒä½¿è®¡ç®—æœºèƒ½å¤Ÿä»æ•°æ®ä¸­å­¦ä¹ è€Œæ— éœ€æ˜ç¡®ç¼–ç¨‹ã€‚\n",
    "        æ·±åº¦å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„ä¸€ä¸ªå­é›†ï¼Œä½¿ç”¨ç¥ç»ç½‘ç»œæ¥æ¨¡æ‹Ÿäººè„‘çš„å·¥ä½œæ–¹å¼ã€‚\n",
    "        \"\"\")\n",
    "\n",
    "    loader = TextLoader(\"sample.txt\", encoding=\"utf-8\")\n",
    "    documents = loader.load()\n",
    "    print(f\"åŠ è½½çš„æ–‡æ¡£æ•°é‡: {len(documents)}\")\n",
    "    print(f\"æ–‡æ¡£å†…å®¹é¢„è§ˆ: {documents[0].page_content[:100]}...\")\n",
    "\n",
    "    # 1.2 CSVæ–‡ä»¶åŠ è½½\n",
    "    print(\"\\n1.2 CSVæ–‡ä»¶åŠ è½½\")\n",
    "    import pandas as pd\n",
    "\n",
    "    # åˆ›å»ºç¤ºä¾‹CSV\n",
    "    df = pd.DataFrame({\n",
    "        'name': ['å¼ ä¸‰', 'æå››', 'ç‹äº”'],\n",
    "        'age': [25, 30, 35],\n",
    "        'city': ['åŒ—äº¬', 'ä¸Šæµ·', 'æ·±åœ³'],\n",
    "        'description': ['è½¯ä»¶å·¥ç¨‹å¸ˆ', 'æ•°æ®ç§‘å­¦å®¶', 'äº§å“ç»ç†']\n",
    "    })\n",
    "    df.to_csv(\"sample.csv\", index=False, encoding=\"utf-8\")\n",
    "\n",
    "    csv_loader = CSVLoader(\"sample.csv\", encoding=\"utf-8\")\n",
    "    csv_docs = csv_loader.load()\n",
    "    print(f\"CSVæ–‡æ¡£æ•°é‡: {len(csv_docs)}\")\n",
    "    print(f\"CSVæ–‡æ¡£ç¤ºä¾‹: {csv_docs[0].page_content}\")\n",
    "\n",
    "    # 1.3 JSONæ–‡ä»¶åŠ è½½\n",
    "    print(\"\\n1.3 JSONæ–‡ä»¶åŠ è½½\")\n",
    "    import json\n",
    "\n",
    "    sample_data = [\n",
    "        {\"title\": \"Pythonç¼–ç¨‹\", \"content\": \"Pythonæ˜¯ä¸€ç§é«˜çº§ç¼–ç¨‹è¯­è¨€\", \"category\": \"æŠ€æœ¯\"},\n",
    "        {\"title\": \"æ•°æ®åˆ†æ\", \"content\": \"æ•°æ®åˆ†ææ˜¯ä»æ•°æ®ä¸­æå–æ´å¯Ÿçš„è¿‡ç¨‹\", \"category\": \"æ•°æ®ç§‘å­¦\"}\n",
    "    ]\n",
    "\n",
    "    with open(\"sample.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(sample_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    json_loader = JSONLoader(\"sample.json\", jq_schema=\".[].content\")\n",
    "    json_docs = json_loader.load()\n",
    "    print(f\"JSONæ–‡æ¡£æ•°é‡: {len(json_docs)}\")\n",
    "    print(f\"JSONæ–‡æ¡£ç¤ºä¾‹: {json_docs[0].page_content}\")\n",
    "\n",
    "    # 1.4 ç›®å½•æ‰¹é‡åŠ è½½\n",
    "    print(\"\\n1.4 ç›®å½•æ‰¹é‡åŠ è½½\")\n",
    "    os.makedirs(\"docs\", exist_ok=True)\n",
    "\n",
    "    # åˆ›å»ºå¤šä¸ªæ–‡æ¡£\n",
    "    for i in range(3):\n",
    "        with open(f\"docs/doc_{i}.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(f\"è¿™æ˜¯ç¬¬{i + 1}ä¸ªæ–‡æ¡£çš„å†…å®¹ã€‚åŒ…å«å…³äºæŠ€æœ¯{i + 1}çš„è¯¦ç»†ä¿¡æ¯ã€‚\")\n",
    "\n",
    "    dir_loader = DirectoryLoader(\"docs\", glob=\"*.txt\",\n",
    "                                 loader_cls=TextLoader,\n",
    "                                 loader_kwargs={\"encoding\": \"utf-8\"})\n",
    "    dir_docs = dir_loader.load()\n",
    "    print(f\"ç›®å½•æ–‡æ¡£æ•°é‡: {len(dir_docs)}\")\n",
    "\n",
    "    return documents + csv_docs + json_docs + dir_docs\n",
    "\n",
    "documents = document_loaders_example()"
   ],
   "id": "ba3eef49f7f1be9e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "1. Document Loaders æ–‡æ¡£åŠ è½½å™¨ç¤ºä¾‹\n",
      "============================================================\n",
      "\n",
      "1.1 æ–‡æœ¬æ–‡ä»¶åŠ è½½\n",
      "åŠ è½½çš„æ–‡æ¡£æ•°é‡: 1\n",
      "æ–‡æ¡£å†…å®¹é¢„è§ˆ: \n",
      "        äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œè‡´åŠ›äºåˆ›å»ºèƒ½å¤Ÿæ‰§è¡Œé€šå¸¸éœ€è¦äººç±»æ™ºèƒ½çš„ä»»åŠ¡çš„ç³»ç»Ÿã€‚\n",
      "        æœºå™¨å­¦ä¹ æ˜¯AIçš„ä¸€ä¸ªå­é›†ï¼Œå®ƒä½¿è®¡ç®—æœºèƒ½å¤Ÿä»æ•°æ®ä¸­å­¦ä¹ è€Œæ— éœ€æ˜ç¡®ç¼–ç¨‹ã€‚\n",
      "   ...\n",
      "\n",
      "1.2 CSVæ–‡ä»¶åŠ è½½\n",
      "CSVæ–‡æ¡£æ•°é‡: 3\n",
      "CSVæ–‡æ¡£ç¤ºä¾‹: name: å¼ ä¸‰\n",
      "age: 25\n",
      "city: åŒ—äº¬\n",
      "description: è½¯ä»¶å·¥ç¨‹å¸ˆ\n",
      "\n",
      "1.3 JSONæ–‡ä»¶åŠ è½½\n",
      "JSONæ–‡æ¡£æ•°é‡: 2\n",
      "JSONæ–‡æ¡£ç¤ºä¾‹: Pythonæ˜¯ä¸€ç§é«˜çº§ç¼–ç¨‹è¯­è¨€\n",
      "\n",
      "1.4 ç›®å½•æ‰¹é‡åŠ è½½\n",
      "ç›®å½•æ–‡æ¡£æ•°é‡: 3\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 2. Text Splitters ç¤ºä¾‹",
   "id": "5fd3ca1111ba6401"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "LangChain 0.3 Text Splitters å®Œæ•´ç¤ºä¾‹\n",
    "åŒ…å«æ‰€æœ‰ä¸»è¦åˆ†å‰²å™¨ç±»å‹å’Œé«˜çº§ç”¨æ³•\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "from typing import List, Dict, Any\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain.text_splitter import (\n",
    "    RecursiveCharacterTextSplitter,\n",
    "    CharacterTextSplitter,\n",
    "    TokenTextSplitter,\n",
    "    MarkdownHeaderTextSplitter,\n",
    "    HTMLHeaderTextSplitter,\n",
    "    PythonCodeTextSplitter,\n",
    "    LatexTextSplitter\n",
    ")\n",
    "from langchain_core.documents import Document"
   ],
   "id": "ce7e6218653160d3",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### åˆ›å»ºç¤ºä¾‹æ–‡æ¡£ç”¨äºæµ‹è¯•",
   "id": "9892fb6b1e4a7dae"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T04:37:15.626366Z",
     "start_time": "2025-07-23T04:37:15.620363Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "def create_sample_documents():\n",
    "    \"\"\"åˆ›å»ºç¤ºä¾‹æ–‡æ¡£ç”¨äºæµ‹è¯•\"\"\"\n",
    "\n",
    "    # é•¿æ–‡æœ¬ç¤ºä¾‹\n",
    "    long_text = \"\"\"\n",
    "äººå·¥æ™ºèƒ½çš„å‘å±•å†ç¨‹å¯ä»¥è¿½æº¯åˆ°20ä¸–çºª50å¹´ä»£ã€‚å½“æ—¶ï¼Œè®¡ç®—æœºç§‘å­¦å®¶å¼€å§‹æ¢ç´¢è®©æœºå™¨æ¨¡æ‹Ÿäººç±»æ™ºèƒ½çš„å¯èƒ½æ€§ã€‚\n",
    "\n",
    "åœ¨1956å¹´çš„è¾¾ç‰¹èŒ…æ–¯ä¼šè®®ä¸Šï¼Œäººå·¥æ™ºèƒ½è¿™ä¸ªæœ¯è¯­é¦–æ¬¡è¢«æ­£å¼æå‡ºã€‚è¿™æ ‡å¿—ç€AIä½œä¸ºä¸€ä¸ªç‹¬ç«‹å­¦ç§‘çš„è¯ç”Ÿã€‚\n",
    "\n",
    "éšåçš„å‡ åå¹´é‡Œï¼ŒAIç»å†äº†å¤šæ¬¡èµ·ä¼ã€‚60-70å¹´ä»£æ˜¯ç¬¬ä¸€ä¸ªAIæ˜¥å¤©ï¼Œä¸“å®¶ç³»ç»Ÿå¾—åˆ°äº†å¹¿æ³›åº”ç”¨ã€‚\n",
    "\n",
    "80å¹´ä»£æœ«åˆ°90å¹´ä»£åˆï¼Œç”±äºæŠ€æœ¯é™åˆ¶å’Œè¿‡é«˜æœŸæœ›ï¼ŒAIè¿›å…¥äº†æ‰€è°“çš„\"AIå†¬å¤©\"ã€‚\n",
    "\n",
    "21ä¸–çºªä»¥æ¥ï¼Œéšç€å¤§æ•°æ®ã€äº‘è®¡ç®—å’Œæ·±åº¦å­¦ä¹ çš„å‘å±•ï¼ŒAIè¿æ¥äº†æ–°çš„æ˜¥å¤©ã€‚\n",
    "\n",
    "ä»Šå¤©ï¼ŒAIå·²ç»åœ¨å›¾åƒè¯†åˆ«ã€è‡ªç„¶è¯­è¨€å¤„ç†ã€æ¨èç³»ç»Ÿç­‰é¢†åŸŸå–å¾—äº†çªç ´æ€§è¿›å±•ã€‚\n",
    "\n",
    "æœºå™¨å­¦ä¹ ä½œä¸ºAIçš„æ ¸å¿ƒæŠ€æœ¯ï¼ŒåŒ…æ‹¬ç›‘ç£å­¦ä¹ ã€æ— ç›‘ç£å­¦ä¹ å’Œå¼ºåŒ–å­¦ä¹ ä¸‰å¤§ç±»ã€‚æ·±åº¦å­¦ä¹ åˆ™æ˜¯æœºå™¨å­¦ä¹ çš„ä¸€ä¸ªé‡è¦åˆ†æ”¯ã€‚\n",
    "\n",
    "è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰è®©è®¡ç®—æœºèƒ½å¤Ÿç†è§£å’Œç”Ÿæˆäººç±»è¯­è¨€ã€‚è®¡ç®—æœºè§†è§‰è®©æœºå™¨èƒ½å¤Ÿ\"çœ‹è§\"å’Œç†è§£å›¾åƒã€‚\n",
    "\n",
    "æœªæ¥ï¼ŒAIå°†åœ¨æ›´å¤šé¢†åŸŸå‘æŒ¥é‡è¦ä½œç”¨ï¼ŒåŒ…æ‹¬åŒ»ç–—ã€æ•™è‚²ã€äº¤é€šã€é‡‘èç­‰ã€‚åŒæ—¶ï¼ŒAIçš„ä¼¦ç†å’Œå®‰å…¨é—®é¢˜ä¹Ÿéœ€è¦å¾—åˆ°é‡è§†ã€‚\n",
    "    \"\"\"\n",
    "\n",
    "    return Document(page_content=long_text.strip(), metadata={\"source\": \"ai_history\"})\n",
    "create_sample_documents()"
   ],
   "id": "88469d08a6c00c3",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': 'ai_history'}, page_content='äººå·¥æ™ºèƒ½çš„å‘å±•å†ç¨‹å¯ä»¥è¿½æº¯åˆ°20ä¸–çºª50å¹´ä»£ã€‚å½“æ—¶ï¼Œè®¡ç®—æœºç§‘å­¦å®¶å¼€å§‹æ¢ç´¢è®©æœºå™¨æ¨¡æ‹Ÿäººç±»æ™ºèƒ½çš„å¯èƒ½æ€§ã€‚\\n\\nåœ¨1956å¹´çš„è¾¾ç‰¹èŒ…æ–¯ä¼šè®®ä¸Šï¼Œäººå·¥æ™ºèƒ½è¿™ä¸ªæœ¯è¯­é¦–æ¬¡è¢«æ­£å¼æå‡ºã€‚è¿™æ ‡å¿—ç€AIä½œä¸ºä¸€ä¸ªç‹¬ç«‹å­¦ç§‘çš„è¯ç”Ÿã€‚\\n\\néšåçš„å‡ åå¹´é‡Œï¼ŒAIç»å†äº†å¤šæ¬¡èµ·ä¼ã€‚60-70å¹´ä»£æ˜¯ç¬¬ä¸€ä¸ªAIæ˜¥å¤©ï¼Œä¸“å®¶ç³»ç»Ÿå¾—åˆ°äº†å¹¿æ³›åº”ç”¨ã€‚\\n\\n80å¹´ä»£æœ«åˆ°90å¹´ä»£åˆï¼Œç”±äºæŠ€æœ¯é™åˆ¶å’Œè¿‡é«˜æœŸæœ›ï¼ŒAIè¿›å…¥äº†æ‰€è°“çš„\"AIå†¬å¤©\"ã€‚\\n\\n21ä¸–çºªä»¥æ¥ï¼Œéšç€å¤§æ•°æ®ã€äº‘è®¡ç®—å’Œæ·±åº¦å­¦ä¹ çš„å‘å±•ï¼ŒAIè¿æ¥äº†æ–°çš„æ˜¥å¤©ã€‚\\n\\nä»Šå¤©ï¼ŒAIå·²ç»åœ¨å›¾åƒè¯†åˆ«ã€è‡ªç„¶è¯­è¨€å¤„ç†ã€æ¨èç³»ç»Ÿç­‰é¢†åŸŸå–å¾—äº†çªç ´æ€§è¿›å±•ã€‚\\n\\næœºå™¨å­¦ä¹ ä½œä¸ºAIçš„æ ¸å¿ƒæŠ€æœ¯ï¼ŒåŒ…æ‹¬ç›‘ç£å­¦ä¹ ã€æ— ç›‘ç£å­¦ä¹ å’Œå¼ºåŒ–å­¦ä¹ ä¸‰å¤§ç±»ã€‚æ·±åº¦å­¦ä¹ åˆ™æ˜¯æœºå™¨å­¦ä¹ çš„ä¸€ä¸ªé‡è¦åˆ†æ”¯ã€‚\\n\\nè‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰è®©è®¡ç®—æœºèƒ½å¤Ÿç†è§£å’Œç”Ÿæˆäººç±»è¯­è¨€ã€‚è®¡ç®—æœºè§†è§‰è®©æœºå™¨èƒ½å¤Ÿ\"çœ‹è§\"å’Œç†è§£å›¾åƒã€‚\\n\\næœªæ¥ï¼ŒAIå°†åœ¨æ›´å¤šé¢†åŸŸå‘æŒ¥é‡è¦ä½œç”¨ï¼ŒåŒ…æ‹¬åŒ»ç–—ã€æ•™è‚²ã€äº¤é€šã€é‡‘èç­‰ã€‚åŒæ—¶ï¼ŒAIçš„ä¼¦ç†å’Œå®‰å…¨é—®é¢˜ä¹Ÿéœ€è¦å¾—åˆ°é‡è§†ã€‚')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### é€’å½’å­—ç¬¦åˆ†å‰²å™¨ç¤ºä¾‹ - æ¨èä½¿ç”¨",
   "id": "9d86747bd818921f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T04:38:06.056307Z",
     "start_time": "2025-07-23T04:38:06.048370Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "def recursive_character_splitter_example():\n",
    "    \"\"\"é€’å½’å­—ç¬¦åˆ†å‰²å™¨ç¤ºä¾‹ - æ¨èä½¿ç”¨\"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"1. RecursiveCharacterTextSplitterï¼ˆæ¨èï¼‰\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    doc = create_sample_documents()\n",
    "\n",
    "    # 1.1 åŸºç¡€ç”¨æ³•\n",
    "    print(\"\\n1.1 åŸºç¡€é€’å½’åˆ†å‰²\")\n",
    "    basic_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=200,           # å—å¤§å°\n",
    "        chunk_overlap=50,         # é‡å å¤§å°\n",
    "        length_function=len,      # é•¿åº¦è®¡ç®—å‡½æ•°\n",
    "        is_separator_regex=False  # åˆ†éš”ç¬¦æ˜¯å¦ä¸ºæ­£åˆ™è¡¨è¾¾å¼\n",
    "    )\n",
    "\n",
    "    basic_chunks = basic_splitter.split_documents([doc])\n",
    "    print(f\"åŸºç¡€åˆ†å‰²å—æ•°: {len(basic_chunks)}\")\n",
    "    for i, chunk in enumerate(basic_chunks[:3]):\n",
    "        print(f\"å— {i+1} (é•¿åº¦: {len(chunk.page_content)}): {chunk.page_content[:80]}...\")\n",
    "\n",
    "    # 1.2 è‡ªå®šä¹‰åˆ†éš”ç¬¦\n",
    "    print(\"\\n1.2 è‡ªå®šä¹‰åˆ†éš”ç¬¦ä¼˜å…ˆçº§\")\n",
    "    custom_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=150,\n",
    "        chunk_overlap=30,\n",
    "        separators=[\n",
    "            \"\\n\\n\",    # æ®µè½åˆ†éš”ç¬¦ï¼ˆæœ€é«˜ä¼˜å…ˆçº§ï¼‰\n",
    "            \"\\n\",      # è¡Œåˆ†éš”ç¬¦\n",
    "            \"ã€‚\",      # ä¸­æ–‡å¥å·\n",
    "            \"ï¼\",      # ä¸­æ–‡æ„Ÿå¹å·\n",
    "            \"ï¼Ÿ\",      # ä¸­æ–‡é—®å·\n",
    "            \"ï¼Œ\",      # ä¸­æ–‡é€—å·\n",
    "            \" \",       # ç©ºæ ¼\n",
    "            \"\"         # å­—ç¬¦çº§åˆ†å‰²ï¼ˆæœ€åæ‰‹æ®µï¼‰\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    custom_chunks = custom_splitter.split_documents([doc])\n",
    "    print(f\"è‡ªå®šä¹‰åˆ†å‰²å—æ•°: {len(custom_chunks)}\")\n",
    "\n",
    "    # 1.3 ä¿æŒæ®µè½å®Œæ•´æ€§\n",
    "    print(\"\\n1.3 æ®µè½ä¼˜å…ˆåˆ†å‰²\")\n",
    "    paragraph_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=300,\n",
    "        chunk_overlap=50,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \"ã€‚\", \" \", \"\"]\n",
    "    )\n",
    "\n",
    "    para_chunks = paragraph_splitter.split_documents([doc])\n",
    "    print(f\"æ®µè½åˆ†å‰²å—æ•°: {len(para_chunks)}\")\n",
    "\n",
    "    return basic_chunks\n",
    "recursive_character_splitter_example()"
   ],
   "id": "ce030a9b3e7cb3b3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "1. RecursiveCharacterTextSplitterï¼ˆæ¨èï¼‰\n",
      "============================================================\n",
      "\n",
      "1.1 åŸºç¡€é€’å½’åˆ†å‰²\n",
      "åŸºç¡€åˆ†å‰²å—æ•°: 3\n",
      "å— 1 (é•¿åº¦: 190): äººå·¥æ™ºèƒ½çš„å‘å±•å†ç¨‹å¯ä»¥è¿½æº¯åˆ°20ä¸–çºª50å¹´ä»£ã€‚å½“æ—¶ï¼Œè®¡ç®—æœºç§‘å­¦å®¶å¼€å§‹æ¢ç´¢è®©æœºå™¨æ¨¡æ‹Ÿäººç±»æ™ºèƒ½çš„å¯èƒ½æ€§ã€‚\n",
      "\n",
      "åœ¨1956å¹´çš„è¾¾ç‰¹èŒ…æ–¯ä¼šè®®ä¸Šï¼Œäººå·¥æ™ºèƒ½è¿™ä¸ªæœ¯è¯­é¦–æ¬¡è¢«æ­£å¼...\n",
      "å— 2 (é•¿åº¦: 169): 80å¹´ä»£æœ«åˆ°90å¹´ä»£åˆï¼Œç”±äºæŠ€æœ¯é™åˆ¶å’Œè¿‡é«˜æœŸæœ›ï¼ŒAIè¿›å…¥äº†æ‰€è°“çš„\"AIå†¬å¤©\"ã€‚\n",
      "\n",
      "21ä¸–çºªä»¥æ¥ï¼Œéšç€å¤§æ•°æ®ã€äº‘è®¡ç®—å’Œæ·±åº¦å­¦ä¹ çš„å‘å±•ï¼ŒAIè¿æ¥äº†æ–°çš„æ˜¥å¤©ã€‚\n",
      "\n",
      "ä»Šå¤©...\n",
      "å— 3 (é•¿åº¦: 103): è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰è®©è®¡ç®—æœºèƒ½å¤Ÿç†è§£å’Œç”Ÿæˆäººç±»è¯­è¨€ã€‚è®¡ç®—æœºè§†è§‰è®©æœºå™¨èƒ½å¤Ÿ\"çœ‹è§\"å’Œç†è§£å›¾åƒã€‚\n",
      "\n",
      "æœªæ¥ï¼ŒAIå°†åœ¨æ›´å¤šé¢†åŸŸå‘æŒ¥é‡è¦ä½œç”¨ï¼ŒåŒ…æ‹¬åŒ»ç–—ã€æ•™è‚²ã€äº¤é€šã€é‡‘è...\n",
      "\n",
      "1.2 è‡ªå®šä¹‰åˆ†éš”ç¬¦ä¼˜å…ˆçº§\n",
      "è‡ªå®šä¹‰åˆ†å‰²å—æ•°: 4\n",
      "\n",
      "1.3 æ®µè½ä¼˜å…ˆåˆ†å‰²\n",
      "æ®µè½åˆ†å‰²å—æ•°: 2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'ai_history'}, page_content='äººå·¥æ™ºèƒ½çš„å‘å±•å†ç¨‹å¯ä»¥è¿½æº¯åˆ°20ä¸–çºª50å¹´ä»£ã€‚å½“æ—¶ï¼Œè®¡ç®—æœºç§‘å­¦å®¶å¼€å§‹æ¢ç´¢è®©æœºå™¨æ¨¡æ‹Ÿäººç±»æ™ºèƒ½çš„å¯èƒ½æ€§ã€‚\\n\\nåœ¨1956å¹´çš„è¾¾ç‰¹èŒ…æ–¯ä¼šè®®ä¸Šï¼Œäººå·¥æ™ºèƒ½è¿™ä¸ªæœ¯è¯­é¦–æ¬¡è¢«æ­£å¼æå‡ºã€‚è¿™æ ‡å¿—ç€AIä½œä¸ºä¸€ä¸ªç‹¬ç«‹å­¦ç§‘çš„è¯ç”Ÿã€‚\\n\\néšåçš„å‡ åå¹´é‡Œï¼ŒAIç»å†äº†å¤šæ¬¡èµ·ä¼ã€‚60-70å¹´ä»£æ˜¯ç¬¬ä¸€ä¸ªAIæ˜¥å¤©ï¼Œä¸“å®¶ç³»ç»Ÿå¾—åˆ°äº†å¹¿æ³›åº”ç”¨ã€‚\\n\\n80å¹´ä»£æœ«åˆ°90å¹´ä»£åˆï¼Œç”±äºæŠ€æœ¯é™åˆ¶å’Œè¿‡é«˜æœŸæœ›ï¼ŒAIè¿›å…¥äº†æ‰€è°“çš„\"AIå†¬å¤©\"ã€‚'),\n",
       " Document(metadata={'source': 'ai_history'}, page_content='80å¹´ä»£æœ«åˆ°90å¹´ä»£åˆï¼Œç”±äºæŠ€æœ¯é™åˆ¶å’Œè¿‡é«˜æœŸæœ›ï¼ŒAIè¿›å…¥äº†æ‰€è°“çš„\"AIå†¬å¤©\"ã€‚\\n\\n21ä¸–çºªä»¥æ¥ï¼Œéšç€å¤§æ•°æ®ã€äº‘è®¡ç®—å’Œæ·±åº¦å­¦ä¹ çš„å‘å±•ï¼ŒAIè¿æ¥äº†æ–°çš„æ˜¥å¤©ã€‚\\n\\nä»Šå¤©ï¼ŒAIå·²ç»åœ¨å›¾åƒè¯†åˆ«ã€è‡ªç„¶è¯­è¨€å¤„ç†ã€æ¨èç³»ç»Ÿç­‰é¢†åŸŸå–å¾—äº†çªç ´æ€§è¿›å±•ã€‚\\n\\næœºå™¨å­¦ä¹ ä½œä¸ºAIçš„æ ¸å¿ƒæŠ€æœ¯ï¼ŒåŒ…æ‹¬ç›‘ç£å­¦ä¹ ã€æ— ç›‘ç£å­¦ä¹ å’Œå¼ºåŒ–å­¦ä¹ ä¸‰å¤§ç±»ã€‚æ·±åº¦å­¦ä¹ åˆ™æ˜¯æœºå™¨å­¦ä¹ çš„ä¸€ä¸ªé‡è¦åˆ†æ”¯ã€‚'),\n",
       " Document(metadata={'source': 'ai_history'}, page_content='è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰è®©è®¡ç®—æœºèƒ½å¤Ÿç†è§£å’Œç”Ÿæˆäººç±»è¯­è¨€ã€‚è®¡ç®—æœºè§†è§‰è®©æœºå™¨èƒ½å¤Ÿ\"çœ‹è§\"å’Œç†è§£å›¾åƒã€‚\\n\\næœªæ¥ï¼ŒAIå°†åœ¨æ›´å¤šé¢†åŸŸå‘æŒ¥é‡è¦ä½œç”¨ï¼ŒåŒ…æ‹¬åŒ»ç–—ã€æ•™è‚²ã€äº¤é€šã€é‡‘èç­‰ã€‚åŒæ—¶ï¼ŒAIçš„ä¼¦ç†å’Œå®‰å…¨é—®é¢˜ä¹Ÿéœ€è¦å¾—åˆ°é‡è§†ã€‚')]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### å­—ç¬¦æ–‡æœ¬åˆ†å‰²å™¨ç¤ºä¾‹",
   "id": "9842d0500ccd9820"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "def character_text_splitter_example():\n",
    "    \"\"\"å­—ç¬¦æ–‡æœ¬åˆ†å‰²å™¨ç¤ºä¾‹\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"2. CharacterTextSplitter\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    doc = create_sample_documents()\n",
    "\n",
    "    # 2.1 æŒ‰æ®µè½åˆ†å‰²\n",
    "    print(\"\\n2.1 æŒ‰æ®µè½åˆ†å‰²\")\n",
    "    para_splitter = CharacterTextSplitter(\n",
    "        chunk_size=300,\n",
    "        chunk_overlap=50,\n",
    "        separator=\"\\n\\n\"  # åªä½¿ç”¨æ®µè½åˆ†éš”ç¬¦\n",
    "    )\n",
    "\n",
    "    para_chunks = para_splitter.split_documents([doc])\n",
    "    print(f\"æ®µè½åˆ†å‰²å—æ•°: {len(para_chunks)}\")\n",
    "\n",
    "    # 2.2 æŒ‰å¥å­åˆ†å‰²\n",
    "    print(\"\\n2.2 æŒ‰å¥å­åˆ†å‰²\")\n",
    "    sentence_splitter = CharacterTextSplitter(\n",
    "        chunk_size=100,\n",
    "        chunk_overlap=20,\n",
    "        separator=\"ã€‚\"  # æŒ‰ä¸­æ–‡å¥å·åˆ†å‰²\n",
    "    )\n",
    "\n",
    "    sentence_chunks = sentence_splitter.split_documents([doc])\n",
    "    print(f\"å¥å­åˆ†å‰²å—æ•°: {len(sentence_chunks)}\")\n",
    "\n",
    "    # 2.3 è‡ªå®šä¹‰åˆ†éš”ç¬¦\n",
    "    print(\"\\n2.3 è‡ªå®šä¹‰åˆ†éš”ç¬¦\")\n",
    "    custom_text = \"é¡¹ç›®A|é¡¹ç›®B|é¡¹ç›®C|é¡¹ç›®D|é¡¹ç›®Eçš„è¯¦ç»†æè¿°å’Œåˆ†ææŠ¥å‘Š\"\n",
    "    custom_doc = Document(page_content=custom_text)\n",
    "\n",
    "    custom_splitter = CharacterTextSplitter(\n",
    "        chunk_size=20,\n",
    "        chunk_overlap=0,\n",
    "        separator=\"|\"\n",
    "    )\n",
    "\n",
    "    custom_chunks = custom_splitter.split_documents([custom_doc])\n",
    "    print(f\"è‡ªå®šä¹‰åˆ†å‰²å—æ•°: {len(custom_chunks)}\")\n",
    "    for chunk in custom_chunks:\n",
    "        print(f\"  - {chunk.page_content}\")\n",
    "\n",
    "    return para_chunks\n",
    "character_text_splitter_example()"
   ],
   "id": "e757e63d91a4e4fc",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Tokenæ–‡æœ¬åˆ†å‰²å™¨ç¤ºä¾‹",
   "id": "6041fe05918b6c54"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T04:39:44.020099Z",
     "start_time": "2025-07-23T04:39:44.010096Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "def token_text_splitter_example():\n",
    "    \"\"\"Tokenæ–‡æœ¬åˆ†å‰²å™¨ç¤ºä¾‹\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"3. TokenTextSplitter\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    doc = create_sample_documents()\n",
    "\n",
    "    # 3.1 åŸºç¡€Tokenåˆ†å‰²\n",
    "    print(\"\\n3.1 åŸºç¡€Tokenåˆ†å‰²\")\n",
    "    token_splitter = TokenTextSplitter(\n",
    "        chunk_size=100,      # Tokenæ•°é‡\n",
    "        chunk_overlap=20,    # é‡å Tokenæ•°\n",
    "        model_name=\"gpt-3.5-turbo\"  # æŒ‡å®štokenizeræ¨¡å‹\n",
    "    )\n",
    "\n",
    "    token_chunks = token_splitter.split_documents([doc])\n",
    "    print(f\"Tokenåˆ†å‰²å—æ•°: {len(token_chunks)}\")\n",
    "\n",
    "    # æ˜¾ç¤ºæ¯å—çš„å®é™…tokenæ•°\n",
    "    for i, chunk in enumerate(token_chunks[:3]):\n",
    "        token_count = token_splitter._tokenizer.encode(chunk.page_content)\n",
    "        print(f\"å— {i+1} Tokenæ•°: {len(token_count)}, å†…å®¹: {chunk.page_content[:60]}...\")\n",
    "\n",
    "    # 3.2 ä¸åŒæ¨¡å‹çš„Tokenåˆ†å‰²\n",
    "    print(\"\\n3.2 ä¸åŒæ¨¡å‹Tokenåˆ†å‰²å¯¹æ¯”\")\n",
    "    models = [\"gpt-3.5-turbo\", \"text-davinci-003\", \"gpt-4\"]\n",
    "\n",
    "    for model in models:\n",
    "        try:\n",
    "            model_splitter = TokenTextSplitter(\n",
    "                chunk_size=50,\n",
    "                chunk_overlap=10,\n",
    "                model_name=model\n",
    "            )\n",
    "            model_chunks = model_splitter.split_documents([doc])\n",
    "            print(f\"{model}: {len(model_chunks)} å—\")\n",
    "        except Exception as e:\n",
    "            print(f\"{model}: ä¸æ”¯æŒ ({e})\")\n",
    "\n",
    "    return token_chunks\n",
    "token_text_splitter_example()"
   ],
   "id": "6f3d4d1dc5ea686e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "3. TokenTextSplitter\n",
      "============================================================\n",
      "\n",
      "3.1 åŸºç¡€Tokenåˆ†å‰²\n",
      "Tokenåˆ†å‰²å—æ•°: 6\n",
      "å— 1 Tokenæ•°: 100, å†…å®¹: äººå·¥æ™ºèƒ½çš„å‘å±•å†ç¨‹å¯ä»¥è¿½æº¯åˆ°20ä¸–çºª50å¹´ä»£ã€‚å½“æ—¶ï¼Œè®¡ç®—æœºç§‘å­¦å®¶å¼€å§‹æ¢ç´¢è®©æœºå™¨æ¨¡æ‹Ÿäººç±»æ™ºèƒ½çš„å¯èƒ½æ€§ã€‚\n",
      "\n",
      "åœ¨1956å¹´çš„è¾¾...\n",
      "å— 2 Tokenæ•°: 100, å†…å®¹: æ¬¡è¢«æ­£å¼æå‡ºã€‚è¿™æ ‡å¿—ç€AIä½œä¸ºä¸€ä¸ªç‹¬ç«‹å­¦ç§‘çš„è¯ç”Ÿã€‚\n",
      "\n",
      "éšåçš„å‡ åå¹´é‡Œï¼ŒAIç»å†äº†å¤šæ¬¡èµ·ä¼ã€‚60-70å¹´ä»£æ˜¯ç¬¬ä¸€ä¸ªAIæ˜¥å¤©...\n",
      "å— 3 Tokenæ•°: 100, å†…å®¹: å¹´ä»£åˆï¼Œç”±äºæŠ€æœ¯é™åˆ¶å’Œè¿‡é«˜æœŸæœ›ï¼ŒAIè¿›å…¥äº†æ‰€è°“çš„\"AIå†¬å¤©\"ã€‚\n",
      "\n",
      "21ä¸–çºªä»¥æ¥ï¼Œéšç€å¤§æ•°æ®ã€äº‘è®¡ç®—å’Œæ·±åº¦å­¦ä¹ çš„å‘å±•ï¼ŒAI...\n",
      "\n",
      "3.2 ä¸åŒæ¨¡å‹Tokenåˆ†å‰²å¯¹æ¯”\n",
      "gpt-3.5-turbo: 11 å—\n",
      "text-davinci-003: 21 å—\n",
      "gpt-4: 11 å—\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'ai_history'}, page_content='äººå·¥æ™ºèƒ½çš„å‘å±•å†ç¨‹å¯ä»¥è¿½æº¯åˆ°20ä¸–çºª50å¹´ä»£ã€‚å½“æ—¶ï¼Œè®¡ç®—æœºç§‘å­¦å®¶å¼€å§‹æ¢ç´¢è®©æœºå™¨æ¨¡æ‹Ÿäººç±»æ™ºèƒ½çš„å¯èƒ½æ€§ã€‚\\n\\nåœ¨1956å¹´çš„è¾¾ç‰¹èŒ…æ–¯ä¼šè®®ä¸Šï¼Œäººå·¥æ™ºèƒ½è¿™ä¸ªæœ¯è¯­é¦–æ¬¡è¢«æ­£å¼æå‡ºã€‚è¿™æ ‡å¿—ç€AIä½œä¸ºä¸€ä¸ªç‹¬'),\n",
       " Document(metadata={'source': 'ai_history'}, page_content='æ¬¡è¢«æ­£å¼æå‡ºã€‚è¿™æ ‡å¿—ç€AIä½œä¸ºä¸€ä¸ªç‹¬ç«‹å­¦ç§‘çš„è¯ç”Ÿã€‚\\n\\néšåçš„å‡ åå¹´é‡Œï¼ŒAIç»å†äº†å¤šæ¬¡èµ·ä¼ã€‚60-70å¹´ä»£æ˜¯ç¬¬ä¸€ä¸ªAIæ˜¥å¤©ï¼Œä¸“å®¶ç³»ç»Ÿå¾—åˆ°äº†å¹¿æ³›åº”ç”¨ã€‚\\n\\n80å¹´ä»£æœ«åˆ°90å¹´ä»£åˆï¼Œç”±äºæŠ€æœ¯é™åˆ¶å’Œè¿‡é«˜æœŸæœ›ï¼ŒAIè¿›'),\n",
       " Document(metadata={'source': 'ai_history'}, page_content='å¹´ä»£åˆï¼Œç”±äºæŠ€æœ¯é™åˆ¶å’Œè¿‡é«˜æœŸæœ›ï¼ŒAIè¿›å…¥äº†æ‰€è°“çš„\"AIå†¬å¤©\"ã€‚\\n\\n21ä¸–çºªä»¥æ¥ï¼Œéšç€å¤§æ•°æ®ã€äº‘è®¡ç®—å’Œæ·±åº¦å­¦ä¹ çš„å‘å±•ï¼ŒAIè¿æ¥äº†æ–°çš„æ˜¥å¤©ã€‚\\n\\nä»Šå¤©ï¼ŒAIå·²ç»åœ¨å›¾åƒè¯†åˆ«ã€è‡ªç„¶è¯­è¨€å¤„ç†ã€æ¨èç³»ç»Ÿç­‰é¢†'),\n",
       " Document(metadata={'source': 'ai_history'}, page_content='åœ¨å›¾åƒè¯†åˆ«ã€è‡ªç„¶è¯­è¨€å¤„ç†ã€æ¨èç³»ç»Ÿç­‰é¢†åŸŸå–å¾—äº†çªç ´æ€§è¿›å±•ã€‚\\n\\næœºå™¨å­¦ä¹ ä½œä¸ºAIçš„æ ¸å¿ƒæŠ€æœ¯ï¼ŒåŒ…æ‹¬ç›‘ç£å­¦ä¹ ã€æ— ç›‘ç£å­¦ä¹ å’Œå¼ºåŒ–å­¦ä¹ ä¸‰å¤§ç±»ã€‚æ·±åº¦å­¦ä¹ åˆ™æ˜¯æœºå™¨å­¦ä¹ çš„ä¸€ä¸ªé‡è¦åˆ†æ”¯ã€‚\\n\\nè‡ªç„¶è¯­'),\n",
       " Document(metadata={'source': 'ai_history'}, page_content='å­¦ä¹ åˆ™æ˜¯æœºå™¨å­¦ä¹ çš„ä¸€ä¸ªé‡è¦åˆ†æ”¯ã€‚\\n\\nè‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰è®©è®¡ç®—æœºèƒ½å¤Ÿç†è§£å’Œç”Ÿæˆäººç±»è¯­è¨€ã€‚è®¡ç®—æœºè§†è§‰è®©æœºå™¨èƒ½å¤Ÿ\"çœ‹è§\"å’Œç†è§£å›¾åƒã€‚\\n\\næœªæ¥ï¼ŒAIå°†åœ¨æ›´å¤šé¢†åŸŸå‘æŒ¥é‡è¦ä½œç”¨ï¼ŒåŒ…æ‹¬åŒ»ç–—ã€æ•™è‚²ã€'),\n",
       " Document(metadata={'source': 'ai_history'}, page_content='ï¿½é‡è¦ä½œç”¨ï¼ŒåŒ…æ‹¬åŒ»ç–—ã€æ•™è‚²ã€äº¤é€šã€é‡‘èç­‰ã€‚åŒæ—¶ï¼ŒAIçš„ä¼¦ç†å’Œå®‰å…¨é—®é¢˜ä¹Ÿéœ€è¦å¾—åˆ°é‡è§†ã€‚')]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Markdownæ ‡é¢˜åˆ†å‰²å™¨ç¤ºä¾‹",
   "id": "aad363dd25cc64f7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T04:39:47.529630Z",
     "start_time": "2025-07-23T04:39:47.521482Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "def markdown_header_splitter_example():\n",
    "    \"\"\"Markdownæ ‡é¢˜åˆ†å‰²å™¨ç¤ºä¾‹\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"4. MarkdownHeaderTextSplitter\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # åˆ›å»ºMarkdownæ–‡æ¡£\n",
    "    markdown_text = \"\"\"\n",
    "# äººå·¥æ™ºèƒ½æŠ€æœ¯æŒ‡å—\n",
    "\n",
    "## 1. æœºå™¨å­¦ä¹ åŸºç¡€\n",
    "\n",
    "### 1.1 ç›‘ç£å­¦ä¹ \n",
    "ç›‘ç£å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„ä¸€ä¸ªé‡è¦åˆ†æ”¯ï¼Œä½¿ç”¨æ ‡è®°çš„è®­ç»ƒæ•°æ®æ¥å­¦ä¹ è¾“å…¥åˆ°è¾“å‡ºçš„æ˜ å°„ã€‚\n",
    "\n",
    "å¸¸è§ç®—æ³•åŒ…æ‹¬ï¼š\n",
    "- çº¿æ€§å›å½’\n",
    "- é€»è¾‘å›å½’\n",
    "- å†³ç­–æ ‘\n",
    "- éšæœºæ£®æ—\n",
    "\n",
    "### 1.2 æ— ç›‘ç£å­¦ä¹ \n",
    "æ— ç›‘ç£å­¦ä¹ ä»æœªæ ‡è®°çš„æ•°æ®ä¸­å‘ç°éšè—çš„æ¨¡å¼ã€‚\n",
    "\n",
    "ä¸»è¦æ–¹æ³•ï¼š\n",
    "- èšç±»åˆ†æ\n",
    "- é™ç»´æŠ€æœ¯\n",
    "- å…³è”è§„åˆ™æŒ–æ˜\n",
    "\n",
    "## 2. æ·±åº¦å­¦ä¹ \n",
    "\n",
    "### 2.1 ç¥ç»ç½‘ç»œåŸºç¡€\n",
    "ç¥ç»ç½‘ç»œæ˜¯æ·±åº¦å­¦ä¹ çš„åŸºç¡€ï¼Œæ¨¡æ‹Ÿäººè„‘ç¥ç»å…ƒçš„å·¥ä½œæ–¹å¼ã€‚\n",
    "\n",
    "### 2.2 å·ç§¯ç¥ç»ç½‘ç»œ\n",
    "CNNä¸»è¦ç”¨äºå›¾åƒå¤„ç†å’Œè®¡ç®—æœºè§†è§‰ä»»åŠ¡ã€‚\n",
    "\n",
    "### 2.3 å¾ªç¯ç¥ç»ç½‘ç»œ\n",
    "RNNé€‚åˆå¤„ç†åºåˆ—æ•°æ®ï¼Œå¦‚æ–‡æœ¬å’Œæ—¶é—´åºåˆ—ã€‚\n",
    "\n",
    "## 3. è‡ªç„¶è¯­è¨€å¤„ç†\n",
    "\n",
    "### 3.1 æ–‡æœ¬é¢„å¤„ç†\n",
    "åŒ…æ‹¬åˆ†è¯ã€è¯æ€§æ ‡æ³¨ã€å‘½åå®ä½“è¯†åˆ«ç­‰æ­¥éª¤ã€‚\n",
    "\n",
    "### 3.2 è¯­è¨€æ¨¡å‹\n",
    "ä»ç»Ÿè®¡è¯­è¨€æ¨¡å‹åˆ°ç°ä»£çš„Transformeræ¨¡å‹ã€‚\n",
    "\n",
    "# æ€»ç»“\n",
    "\n",
    "äººå·¥æ™ºèƒ½æŠ€æœ¯æ­£åœ¨å¿«é€Ÿå‘å±•ï¼Œå„ä¸ªé¢†åŸŸéƒ½æœ‰é‡è¦çªç ´ã€‚\n",
    "\"\"\"\n",
    "\n",
    "    # 4.1 åŸºç¡€æ ‡é¢˜åˆ†å‰²\n",
    "    print(\"\\n4.1 åŸºç¡€æ ‡é¢˜åˆ†å‰²\")\n",
    "    md_splitter = MarkdownHeaderTextSplitter(\n",
    "        headers_to_split_on=[\n",
    "            (\"#\", \"Header 1\"),\n",
    "            (\"##\", \"Header 2\"),\n",
    "            (\"###\", \"Header 3\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    md_chunks = md_splitter.split_text(markdown_text)\n",
    "    print(f\"Markdownåˆ†å‰²å—æ•°: {len(md_chunks)}\")\n",
    "\n",
    "    for i, chunk in enumerate(md_chunks[:5]):\n",
    "        print(f\"\\nå— {i+1}:\")\n",
    "        print(f\"å†…å®¹: {chunk.page_content[:100]}...\")\n",
    "        print(f\"å…ƒæ•°æ®: {chunk.metadata}\")\n",
    "\n",
    "    # 4.2 ç»“åˆé€’å½’åˆ†å‰²å™¨\n",
    "    print(\"\\n4.2 ç»“åˆé€’å½’åˆ†å‰²å™¨è¿›è¡ŒäºŒæ¬¡åˆ†å‰²\")\n",
    "\n",
    "    # å…ˆæŒ‰æ ‡é¢˜åˆ†å‰²\n",
    "    md_docs = md_splitter.split_text(markdown_text)\n",
    "\n",
    "    # å†å¯¹é•¿å—è¿›è¡Œé€’å½’åˆ†å‰²\n",
    "    recursive_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=200,\n",
    "        chunk_overlap=50\n",
    "    )\n",
    "\n",
    "    final_chunks = []\n",
    "    for doc in md_docs:\n",
    "        if len(doc.page_content) > 200:\n",
    "            sub_chunks = recursive_splitter.split_documents([doc])\n",
    "            final_chunks.extend(sub_chunks)\n",
    "        else:\n",
    "            final_chunks.append(doc)\n",
    "\n",
    "    print(f\"äºŒæ¬¡åˆ†å‰²åå—æ•°: {len(final_chunks)}\")\n",
    "\n",
    "    return md_chunks\n",
    "markdown_header_splitter_example()"
   ],
   "id": "16cc12ea512e6025",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "4. MarkdownHeaderTextSplitter\n",
      "============================================================\n",
      "\n",
      "4.1 åŸºç¡€æ ‡é¢˜åˆ†å‰²\n",
      "Markdownåˆ†å‰²å—æ•°: 8\n",
      "\n",
      "å— 1:\n",
      "å†…å®¹: ç›‘ç£å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„ä¸€ä¸ªé‡è¦åˆ†æ”¯ï¼Œä½¿ç”¨æ ‡è®°çš„è®­ç»ƒæ•°æ®æ¥å­¦ä¹ è¾“å…¥åˆ°è¾“å‡ºçš„æ˜ å°„ã€‚  \n",
      "å¸¸è§ç®—æ³•åŒ…æ‹¬ï¼š\n",
      "- çº¿æ€§å›å½’\n",
      "- é€»è¾‘å›å½’\n",
      "- å†³ç­–æ ‘\n",
      "- éšæœºæ£®æ—...\n",
      "å…ƒæ•°æ®: {'Header 1': 'äººå·¥æ™ºèƒ½æŠ€æœ¯æŒ‡å—', 'Header 2': '1. æœºå™¨å­¦ä¹ åŸºç¡€', 'Header 3': '1.1 ç›‘ç£å­¦ä¹ '}\n",
      "\n",
      "å— 2:\n",
      "å†…å®¹: æ— ç›‘ç£å­¦ä¹ ä»æœªæ ‡è®°çš„æ•°æ®ä¸­å‘ç°éšè—çš„æ¨¡å¼ã€‚  \n",
      "ä¸»è¦æ–¹æ³•ï¼š\n",
      "- èšç±»åˆ†æ\n",
      "- é™ç»´æŠ€æœ¯\n",
      "- å…³è”è§„åˆ™æŒ–æ˜...\n",
      "å…ƒæ•°æ®: {'Header 1': 'äººå·¥æ™ºèƒ½æŠ€æœ¯æŒ‡å—', 'Header 2': '1. æœºå™¨å­¦ä¹ åŸºç¡€', 'Header 3': '1.2 æ— ç›‘ç£å­¦ä¹ '}\n",
      "\n",
      "å— 3:\n",
      "å†…å®¹: ç¥ç»ç½‘ç»œæ˜¯æ·±åº¦å­¦ä¹ çš„åŸºç¡€ï¼Œæ¨¡æ‹Ÿäººè„‘ç¥ç»å…ƒçš„å·¥ä½œæ–¹å¼ã€‚...\n",
      "å…ƒæ•°æ®: {'Header 1': 'äººå·¥æ™ºèƒ½æŠ€æœ¯æŒ‡å—', 'Header 2': '2. æ·±åº¦å­¦ä¹ ', 'Header 3': '2.1 ç¥ç»ç½‘ç»œåŸºç¡€'}\n",
      "\n",
      "å— 4:\n",
      "å†…å®¹: CNNä¸»è¦ç”¨äºå›¾åƒå¤„ç†å’Œè®¡ç®—æœºè§†è§‰ä»»åŠ¡ã€‚...\n",
      "å…ƒæ•°æ®: {'Header 1': 'äººå·¥æ™ºèƒ½æŠ€æœ¯æŒ‡å—', 'Header 2': '2. æ·±åº¦å­¦ä¹ ', 'Header 3': '2.2 å·ç§¯ç¥ç»ç½‘ç»œ'}\n",
      "\n",
      "å— 5:\n",
      "å†…å®¹: RNNé€‚åˆå¤„ç†åºåˆ—æ•°æ®ï¼Œå¦‚æ–‡æœ¬å’Œæ—¶é—´åºåˆ—ã€‚...\n",
      "å…ƒæ•°æ®: {'Header 1': 'äººå·¥æ™ºèƒ½æŠ€æœ¯æŒ‡å—', 'Header 2': '2. æ·±åº¦å­¦ä¹ ', 'Header 3': '2.3 å¾ªç¯ç¥ç»ç½‘ç»œ'}\n",
      "\n",
      "4.2 ç»“åˆé€’å½’åˆ†å‰²å™¨è¿›è¡ŒäºŒæ¬¡åˆ†å‰²\n",
      "äºŒæ¬¡åˆ†å‰²åå—æ•°: 8\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'Header 1': 'äººå·¥æ™ºèƒ½æŠ€æœ¯æŒ‡å—', 'Header 2': '1. æœºå™¨å­¦ä¹ åŸºç¡€', 'Header 3': '1.1 ç›‘ç£å­¦ä¹ '}, page_content='ç›‘ç£å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„ä¸€ä¸ªé‡è¦åˆ†æ”¯ï¼Œä½¿ç”¨æ ‡è®°çš„è®­ç»ƒæ•°æ®æ¥å­¦ä¹ è¾“å…¥åˆ°è¾“å‡ºçš„æ˜ å°„ã€‚  \\nå¸¸è§ç®—æ³•åŒ…æ‹¬ï¼š\\n- çº¿æ€§å›å½’\\n- é€»è¾‘å›å½’\\n- å†³ç­–æ ‘\\n- éšæœºæ£®æ—'),\n",
       " Document(metadata={'Header 1': 'äººå·¥æ™ºèƒ½æŠ€æœ¯æŒ‡å—', 'Header 2': '1. æœºå™¨å­¦ä¹ åŸºç¡€', 'Header 3': '1.2 æ— ç›‘ç£å­¦ä¹ '}, page_content='æ— ç›‘ç£å­¦ä¹ ä»æœªæ ‡è®°çš„æ•°æ®ä¸­å‘ç°éšè—çš„æ¨¡å¼ã€‚  \\nä¸»è¦æ–¹æ³•ï¼š\\n- èšç±»åˆ†æ\\n- é™ç»´æŠ€æœ¯\\n- å…³è”è§„åˆ™æŒ–æ˜'),\n",
       " Document(metadata={'Header 1': 'äººå·¥æ™ºèƒ½æŠ€æœ¯æŒ‡å—', 'Header 2': '2. æ·±åº¦å­¦ä¹ ', 'Header 3': '2.1 ç¥ç»ç½‘ç»œåŸºç¡€'}, page_content='ç¥ç»ç½‘ç»œæ˜¯æ·±åº¦å­¦ä¹ çš„åŸºç¡€ï¼Œæ¨¡æ‹Ÿäººè„‘ç¥ç»å…ƒçš„å·¥ä½œæ–¹å¼ã€‚'),\n",
       " Document(metadata={'Header 1': 'äººå·¥æ™ºèƒ½æŠ€æœ¯æŒ‡å—', 'Header 2': '2. æ·±åº¦å­¦ä¹ ', 'Header 3': '2.2 å·ç§¯ç¥ç»ç½‘ç»œ'}, page_content='CNNä¸»è¦ç”¨äºå›¾åƒå¤„ç†å’Œè®¡ç®—æœºè§†è§‰ä»»åŠ¡ã€‚'),\n",
       " Document(metadata={'Header 1': 'äººå·¥æ™ºèƒ½æŠ€æœ¯æŒ‡å—', 'Header 2': '2. æ·±åº¦å­¦ä¹ ', 'Header 3': '2.3 å¾ªç¯ç¥ç»ç½‘ç»œ'}, page_content='RNNé€‚åˆå¤„ç†åºåˆ—æ•°æ®ï¼Œå¦‚æ–‡æœ¬å’Œæ—¶é—´åºåˆ—ã€‚'),\n",
       " Document(metadata={'Header 1': 'äººå·¥æ™ºèƒ½æŠ€æœ¯æŒ‡å—', 'Header 2': '3. è‡ªç„¶è¯­è¨€å¤„ç†', 'Header 3': '3.1 æ–‡æœ¬é¢„å¤„ç†'}, page_content='åŒ…æ‹¬åˆ†è¯ã€è¯æ€§æ ‡æ³¨ã€å‘½åå®ä½“è¯†åˆ«ç­‰æ­¥éª¤ã€‚'),\n",
       " Document(metadata={'Header 1': 'äººå·¥æ™ºèƒ½æŠ€æœ¯æŒ‡å—', 'Header 2': '3. è‡ªç„¶è¯­è¨€å¤„ç†', 'Header 3': '3.2 è¯­è¨€æ¨¡å‹'}, page_content='ä»ç»Ÿè®¡è¯­è¨€æ¨¡å‹åˆ°ç°ä»£çš„Transformeræ¨¡å‹ã€‚'),\n",
       " Document(metadata={'Header 1': 'æ€»ç»“'}, page_content='äººå·¥æ™ºèƒ½æŠ€æœ¯æ­£åœ¨å¿«é€Ÿå‘å±•ï¼Œå„ä¸ªé¢†åŸŸéƒ½æœ‰é‡è¦çªç ´ã€‚')]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### HTMLæ ‡é¢˜åˆ†å‰²å™¨ç¤ºä¾‹",
   "id": "40d65927c234df09"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T04:40:33.308628Z",
     "start_time": "2025-07-23T04:40:33.300629Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "def html_header_splitter_example():\n",
    "    \"\"\"HTMLæ ‡é¢˜åˆ†å‰²å™¨ç¤ºä¾‹\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"5. HTMLHeaderTextSplitter\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    html_text = \"\"\"\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "<head>\n",
    "    <title>AIæŠ€æœ¯æ–‡æ¡£</title>\n",
    "</head>\n",
    "<body>\n",
    "    <h1>äººå·¥æ™ºèƒ½æ¦‚è¿°</h1>\n",
    "    <p>äººå·¥æ™ºèƒ½æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªåˆ†æ”¯ã€‚</p>\n",
    "\n",
    "    <h2>æœºå™¨å­¦ä¹ </h2>\n",
    "    <p>æœºå™¨å­¦ä¹ æ˜¯AIçš„é‡è¦ç»„æˆéƒ¨åˆ†ã€‚</p>\n",
    "\n",
    "    <h3>ç›‘ç£å­¦ä¹ </h3>\n",
    "    <p>ä½¿ç”¨æ ‡è®°æ•°æ®è¿›è¡Œè®­ç»ƒã€‚</p>\n",
    "\n",
    "    <h3>æ— ç›‘ç£å­¦ä¹ </h3>\n",
    "    <p>ä»æœªæ ‡è®°æ•°æ®ä¸­å‘ç°æ¨¡å¼ã€‚</p>\n",
    "\n",
    "    <h2>æ·±åº¦å­¦ä¹ </h2>\n",
    "    <p>åŸºäºç¥ç»ç½‘ç»œçš„å­¦ä¹ æ–¹æ³•ã€‚</p>\n",
    "\n",
    "    <h3>CNN</h3>\n",
    "    <p>å·ç§¯ç¥ç»ç½‘ç»œç”¨äºå›¾åƒå¤„ç†ã€‚</p>\n",
    "\n",
    "    <h3>RNN</h3>\n",
    "    <p>å¾ªç¯ç¥ç»ç½‘ç»œå¤„ç†åºåˆ—æ•°æ®ã€‚</p>\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "    html_splitter = HTMLHeaderTextSplitter(\n",
    "        headers_to_split_on=[\n",
    "            (\"h1\", \"Header 1\"),\n",
    "            (\"h2\", \"Header 2\"),\n",
    "            (\"h3\", \"Header 3\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    html_chunks = html_splitter.split_text(html_text)\n",
    "    print(f\"HTMLåˆ†å‰²å—æ•°: {len(html_chunks)}\")\n",
    "\n",
    "    for i, chunk in enumerate(html_chunks[:3]):\n",
    "        print(f\"\\nå— {i+1}:\")\n",
    "        print(f\"å†…å®¹: {chunk.page_content[:80]}...\")\n",
    "        print(f\"å…ƒæ•°æ®: {chunk.metadata}\")\n",
    "\n",
    "    return html_chunks\n",
    "html_header_splitter_example()"
   ],
   "id": "4aa23e32db7c52d0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "5. HTMLHeaderTextSplitter\n",
      "============================================================\n",
      "HTMLåˆ†å‰²å—æ•°: 14\n",
      "\n",
      "å— 1:\n",
      "å†…å®¹: äººå·¥æ™ºèƒ½æ¦‚è¿°...\n",
      "å…ƒæ•°æ®: {'Header 1': 'äººå·¥æ™ºèƒ½æ¦‚è¿°'}\n",
      "\n",
      "å— 2:\n",
      "å†…å®¹: äººå·¥æ™ºèƒ½æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªåˆ†æ”¯ã€‚...\n",
      "å…ƒæ•°æ®: {'Header 1': 'äººå·¥æ™ºèƒ½æ¦‚è¿°'}\n",
      "\n",
      "å— 3:\n",
      "å†…å®¹: æœºå™¨å­¦ä¹ ...\n",
      "å…ƒæ•°æ®: {'Header 1': 'äººå·¥æ™ºèƒ½æ¦‚è¿°', 'Header 2': 'æœºå™¨å­¦ä¹ '}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'Header 1': 'äººå·¥æ™ºèƒ½æ¦‚è¿°'}, page_content='äººå·¥æ™ºèƒ½æ¦‚è¿°'),\n",
       " Document(metadata={'Header 1': 'äººå·¥æ™ºèƒ½æ¦‚è¿°'}, page_content='äººå·¥æ™ºèƒ½æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªåˆ†æ”¯ã€‚'),\n",
       " Document(metadata={'Header 1': 'äººå·¥æ™ºèƒ½æ¦‚è¿°', 'Header 2': 'æœºå™¨å­¦ä¹ '}, page_content='æœºå™¨å­¦ä¹ '),\n",
       " Document(metadata={'Header 1': 'äººå·¥æ™ºèƒ½æ¦‚è¿°', 'Header 2': 'æœºå™¨å­¦ä¹ '}, page_content='æœºå™¨å­¦ä¹ æ˜¯AIçš„é‡è¦ç»„æˆéƒ¨åˆ†ã€‚'),\n",
       " Document(metadata={'Header 1': 'äººå·¥æ™ºèƒ½æ¦‚è¿°', 'Header 2': 'æœºå™¨å­¦ä¹ ', 'Header 3': 'ç›‘ç£å­¦ä¹ '}, page_content='ç›‘ç£å­¦ä¹ '),\n",
       " Document(metadata={'Header 1': 'äººå·¥æ™ºèƒ½æ¦‚è¿°', 'Header 2': 'æœºå™¨å­¦ä¹ ', 'Header 3': 'ç›‘ç£å­¦ä¹ '}, page_content='ä½¿ç”¨æ ‡è®°æ•°æ®è¿›è¡Œè®­ç»ƒã€‚'),\n",
       " Document(metadata={'Header 1': 'äººå·¥æ™ºèƒ½æ¦‚è¿°', 'Header 2': 'æœºå™¨å­¦ä¹ ', 'Header 3': 'æ— ç›‘ç£å­¦ä¹ '}, page_content='æ— ç›‘ç£å­¦ä¹ '),\n",
       " Document(metadata={'Header 1': 'äººå·¥æ™ºèƒ½æ¦‚è¿°', 'Header 2': 'æœºå™¨å­¦ä¹ ', 'Header 3': 'æ— ç›‘ç£å­¦ä¹ '}, page_content='ä»æœªæ ‡è®°æ•°æ®ä¸­å‘ç°æ¨¡å¼ã€‚'),\n",
       " Document(metadata={'Header 1': 'äººå·¥æ™ºèƒ½æ¦‚è¿°', 'Header 2': 'æ·±åº¦å­¦ä¹ '}, page_content='æ·±åº¦å­¦ä¹ '),\n",
       " Document(metadata={'Header 1': 'äººå·¥æ™ºèƒ½æ¦‚è¿°', 'Header 2': 'æ·±åº¦å­¦ä¹ '}, page_content='åŸºäºç¥ç»ç½‘ç»œçš„å­¦ä¹ æ–¹æ³•ã€‚'),\n",
       " Document(metadata={'Header 1': 'äººå·¥æ™ºèƒ½æ¦‚è¿°', 'Header 2': 'æ·±åº¦å­¦ä¹ ', 'Header 3': 'CNN'}, page_content='CNN'),\n",
       " Document(metadata={'Header 1': 'äººå·¥æ™ºèƒ½æ¦‚è¿°', 'Header 2': 'æ·±åº¦å­¦ä¹ ', 'Header 3': 'CNN'}, page_content='å·ç§¯ç¥ç»ç½‘ç»œç”¨äºå›¾åƒå¤„ç†ã€‚'),\n",
       " Document(metadata={'Header 1': 'äººå·¥æ™ºèƒ½æ¦‚è¿°', 'Header 2': 'æ·±åº¦å­¦ä¹ ', 'Header 3': 'RNN'}, page_content='RNN'),\n",
       " Document(metadata={'Header 1': 'äººå·¥æ™ºèƒ½æ¦‚è¿°', 'Header 2': 'æ·±åº¦å­¦ä¹ ', 'Header 3': 'RNN'}, page_content='å¾ªç¯ç¥ç»ç½‘ç»œå¤„ç†åºåˆ—æ•°æ®ã€‚')]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### ä»£ç åˆ†å‰²å™¨ç¤ºä¾‹",
   "id": "9f6db3d961b5606a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T04:40:54.517430Z",
     "start_time": "2025-07-23T04:40:54.508407Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "def code_splitter_example():\n",
    "    \"\"\"ä»£ç åˆ†å‰²å™¨ç¤ºä¾‹\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"6. PythonCodeTextSplitter\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    python_code = '''\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "class DataProcessor:\n",
    "    \"\"\"æ•°æ®å¤„ç†ç±»\"\"\"\n",
    "\n",
    "    def __init__(self, data_path):\n",
    "        self.data_path = data_path\n",
    "        self.data = None\n",
    "\n",
    "    def load_data(self):\n",
    "        \"\"\"åŠ è½½æ•°æ®\"\"\"\n",
    "        self.data = pd.read_csv(self.data_path)\n",
    "        return self.data\n",
    "\n",
    "    def preprocess(self):\n",
    "        \"\"\"æ•°æ®é¢„å¤„ç†\"\"\"\n",
    "        # å¤„ç†ç¼ºå¤±å€¼\n",
    "        self.data = self.data.dropna()\n",
    "\n",
    "        # ç‰¹å¾ç¼©æ”¾\n",
    "        from sklearn.preprocessing import StandardScaler\n",
    "        scaler = StandardScaler()\n",
    "        numeric_columns = self.data.select_dtypes(include=[np.number]).columns\n",
    "        self.data[numeric_columns] = scaler.fit_transform(self.data[numeric_columns])\n",
    "\n",
    "        return self.data\n",
    "\n",
    "def train_model(X, y):\n",
    "    \"\"\"è®­ç»ƒæ¨¡å‹\"\"\"\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "    print(f\"Mean Squared Error: {mse}\")\n",
    "    return model\n",
    "\n",
    "def main():\n",
    "    \"\"\"ä¸»å‡½æ•°\"\"\"\n",
    "    processor = DataProcessor(\"data.csv\")\n",
    "    data = processor.load_data()\n",
    "    processed_data = processor.preprocess()\n",
    "\n",
    "    X = processed_data.drop('target', axis=1)\n",
    "    y = processed_data['target']\n",
    "\n",
    "    model = train_model(X, y)\n",
    "    print(\"æ¨¡å‹è®­ç»ƒå®Œæˆ\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "'''\n",
    "\n",
    "    # Pythonä»£ç åˆ†å‰²\n",
    "    python_splitter = PythonCodeTextSplitter(\n",
    "        chunk_size=500,\n",
    "        chunk_overlap=50\n",
    "    )\n",
    "\n",
    "    code_chunks = python_splitter.split_text(python_code)\n",
    "    print(f\"Pythonä»£ç åˆ†å‰²å—æ•°: {len(code_chunks)}\")\n",
    "\n",
    "    for i, chunk in enumerate(code_chunks[:3]):\n",
    "        print(f\"\\nä»£ç å— {i+1} (é•¿åº¦: {len(chunk)}):\")\n",
    "        print(chunk[:200] + \"...\" if len(chunk) > 200 else chunk)\n",
    "\n",
    "    return code_chunks\n",
    "code_splitter_example()"
   ],
   "id": "16e9cd00aaf06ba0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "6. PythonCodeTextSplitter\n",
      "============================================================\n",
      "Pythonä»£ç åˆ†å‰²å—æ•°: 5\n",
      "\n",
      "ä»£ç å— 1 (é•¿åº¦: 188):\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.linear_model import LinearRegression\n",
      "from sklearn.metrics import mean_squared_error\n",
      "\n",
      "ä»£ç å— 2 (é•¿åº¦: 352):\n",
      "class DataProcessor:\n",
      "    \"\"\"æ•°æ®å¤„ç†ç±»\"\"\"\n",
      "\n",
      "    def __init__(self, data_path):\n",
      "        self.data_path = data_path\n",
      "        self.data = None\n",
      "\n",
      "    def load_data(self):\n",
      "        \"\"\"åŠ è½½æ•°æ®\"\"\"\n",
      "        self.data = pd...\n",
      "\n",
      "ä»£ç å— 3 (é•¿åº¦: 288):\n",
      "# ç‰¹å¾ç¼©æ”¾\n",
      "        from sklearn.preprocessing import StandardScaler\n",
      "        scaler = StandardScaler()\n",
      "        numeric_columns = self.data.select_dtypes(include=[np.number]).columns\n",
      "        self.data[numer...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['import numpy as np\\nimport pandas as pd\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.linear_model import LinearRegression\\nfrom sklearn.metrics import mean_squared_error',\n",
       " 'class DataProcessor:\\n    \"\"\"æ•°æ®å¤„ç†ç±»\"\"\"\\n\\n    def __init__(self, data_path):\\n        self.data_path = data_path\\n        self.data = None\\n\\n    def load_data(self):\\n        \"\"\"åŠ è½½æ•°æ®\"\"\"\\n        self.data = pd.read_csv(self.data_path)\\n        return self.data\\n\\n    def preprocess(self):\\n        \"\"\"æ•°æ®é¢„å¤„ç†\"\"\"\\n        # å¤„ç†ç¼ºå¤±å€¼\\n        self.data = self.data.dropna()',\n",
       " '# ç‰¹å¾ç¼©æ”¾\\n        from sklearn.preprocessing import StandardScaler\\n        scaler = StandardScaler()\\n        numeric_columns = self.data.select_dtypes(include=[np.number]).columns\\n        self.data[numeric_columns] = scaler.fit_transform(self.data[numeric_columns])\\n\\n        return self.data',\n",
       " 'def train_model(X, y):\\n    \"\"\"è®­ç»ƒæ¨¡å‹\"\"\"\\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\\n\\n    model = LinearRegression()\\n    model.fit(X_train, y_train)\\n\\n    y_pred = model.predict(X_test)\\n    mse = mean_squared_error(y_test, y_pred)\\n\\n    print(f\"Mean Squared Error: {mse}\")\\n    return model',\n",
       " 'def main():\\n    \"\"\"ä¸»å‡½æ•°\"\"\"\\n    processor = DataProcessor(\"data.csv\")\\n    data = processor.load_data()\\n    processed_data = processor.preprocess()\\n\\n    X = processed_data.drop(\\'target\\', axis=1)\\n    y = processed_data[\\'target\\']\\n\\n    model = train_model(X, y)\\n    print(\"æ¨¡å‹è®­ç»ƒå®Œæˆ\")\\n\\nif __name__ == \"__main__\":\\n    main()']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### LaTeXåˆ†å‰²å™¨ç¤ºä¾‹",
   "id": "1c2a67cb0547d28"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T04:41:10.518491Z",
     "start_time": "2025-07-23T04:41:10.512148Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "def latex_splitter_example():\n",
    "    \"\"\"LaTeXåˆ†å‰²å™¨ç¤ºä¾‹\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"7. LatexTextSplitter\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    latex_text = r\"\"\"\n",
    "\\documentclass{article}\n",
    "\\usepackage{amsmath}\n",
    "\\title{æœºå™¨å­¦ä¹ æ•°å­¦åŸºç¡€}\n",
    "\\author{AIç ”ç©¶å›¢é˜Ÿ}\n",
    "\n",
    "\\begin{document}\n",
    "\\maketitle\n",
    "\n",
    "\\section{çº¿æ€§ä»£æ•°}\n",
    "çº¿æ€§ä»£æ•°æ˜¯æœºå™¨å­¦ä¹ çš„æ•°å­¦åŸºç¡€ä¹‹ä¸€ã€‚\n",
    "\n",
    "\\subsection{å‘é‡}\n",
    "å‘é‡æ˜¯å…·æœ‰å¤§å°å’Œæ–¹å‘çš„é‡ã€‚åœ¨æœºå™¨å­¦ä¹ ä¸­ï¼Œç‰¹å¾é€šå¸¸è¡¨ç¤ºä¸ºå‘é‡ã€‚\n",
    "\n",
    "è®¾å‘é‡ $\\mathbf{v} = [v_1, v_2, \\ldots, v_n]^T$ï¼Œå…¶ä¸­ $v_i$ æ˜¯ç¬¬ $i$ ä¸ªåˆ†é‡ã€‚\n",
    "\n",
    "\\subsection{çŸ©é˜µ}\n",
    "çŸ©é˜µæ˜¯äºŒç»´æ•°ç»„ï¼Œç”¨äºè¡¨ç¤ºçº¿æ€§å˜æ¢ã€‚\n",
    "\n",
    "çŸ©é˜µä¹˜æ³•å®šä¹‰ä¸ºï¼š\n",
    "\\begin{equation}\n",
    "(\\mathbf{AB})_{ij} = \\sum_{k=1}^{n} a_{ik}b_{kj}\n",
    "\\end{equation}\n",
    "\n",
    "\\section{æ¦‚ç‡è®º}\n",
    "æ¦‚ç‡è®ºä¸ºæœºå™¨å­¦ä¹ æä¾›äº†ä¸ç¡®å®šæ€§å»ºæ¨¡çš„å·¥å…·ã€‚\n",
    "\n",
    "\\subsection{è´å¶æ–¯å®šç†}\n",
    "è´å¶æ–¯å®šç†æ˜¯æ¦‚ç‡è®ºçš„æ ¸å¿ƒï¼š\n",
    "\\begin{equation}\n",
    "P(A|B) = \\frac{P(B|A)P(A)}{P(B)}\n",
    "\\end{equation}\n",
    "\n",
    "\\section{ä¼˜åŒ–ç†è®º}\n",
    "ä¼˜åŒ–ç†è®ºç”¨äºå¯»æ‰¾æ¨¡å‹çš„æœ€ä¼˜å‚æ•°ã€‚\n",
    "\n",
    "\\subsection{æ¢¯åº¦ä¸‹é™}\n",
    "æ¢¯åº¦ä¸‹é™æ˜¯æœ€å¸¸ç”¨çš„ä¼˜åŒ–ç®—æ³•ï¼š\n",
    "\\begin{equation}\n",
    "\\theta_{t+1} = \\theta_t - \\alpha \\nabla_\\theta J(\\theta)\n",
    "\\end{equation}\n",
    "\n",
    "å…¶ä¸­ $\\alpha$ æ˜¯å­¦ä¹ ç‡ï¼Œ$J(\\theta)$ æ˜¯æŸå¤±å‡½æ•°ã€‚\n",
    "\n",
    "\\end{document}\n",
    "\"\"\"\n",
    "\n",
    "    latex_splitter = LatexTextSplitter(\n",
    "        chunk_size=300,\n",
    "        chunk_overlap=50\n",
    "    )\n",
    "\n",
    "    latex_chunks = latex_splitter.split_text(latex_text)\n",
    "    print(f\"LaTeXåˆ†å‰²å—æ•°: {len(latex_chunks)}\")\n",
    "\n",
    "    for i, chunk in enumerate(latex_chunks[:3]):\n",
    "        print(f\"\\nLaTeXå— {i+1}:\")\n",
    "        print(chunk[:150] + \"...\" if len(chunk) > 150 else chunk)\n",
    "\n",
    "    return latex_chunks\n",
    "latex_splitter_example()"
   ],
   "id": "4b7e76a91110b69b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "7. LatexTextSplitter\n",
      "============================================================\n",
      "LaTeXåˆ†å‰²å—æ•°: 4\n",
      "\n",
      "LaTeXå— 1:\n",
      "\\documentclass{article}\n",
      "\\usepackage{amsmath}\n",
      "\\title{æœºå™¨å­¦ä¹ æ•°å­¦åŸºç¡€}\n",
      "\\author{AIç ”ç©¶å›¢é˜Ÿ}\n",
      "\n",
      "\\begin{document}\n",
      "\\maketitle\n",
      "\n",
      "\\section{çº¿æ€§ä»£æ•°}\n",
      "çº¿æ€§ä»£æ•°æ˜¯æœºå™¨å­¦ä¹ çš„æ•°å­¦åŸºç¡€ä¹‹ä¸€ã€‚\n",
      "\n",
      "\\subsect...\n",
      "\n",
      "LaTeXå— 2:\n",
      "$ ä¸ªåˆ†é‡ã€‚\n",
      "\n",
      "\\subsection{çŸ©é˜µ}\n",
      "çŸ©é˜µæ˜¯äºŒç»´æ•°ç»„ï¼Œç”¨äºè¡¨ç¤ºçº¿æ€§å˜æ¢ã€‚\n",
      "\n",
      "çŸ©é˜µä¹˜æ³•å®šä¹‰ä¸ºï¼š\n",
      "\\begin{equation}\n",
      "(\\mathbf{AB})_{ij} = \\sum_{k=1}^{n} a_{ik}b_{kj}\n",
      "\\end{equation}\n",
      "\n",
      "\\section{æ¦‚ç‡è®º}\n",
      "æ¦‚ç‡...\n",
      "\n",
      "LaTeXå— 3:\n",
      "= \\frac{P(B|A)P(A)}{P(B)}\n",
      "\\end{equation}\n",
      "\n",
      "\\section{ä¼˜åŒ–ç†è®º}\n",
      "ä¼˜åŒ–ç†è®ºç”¨äºå¯»æ‰¾æ¨¡å‹çš„æœ€ä¼˜å‚æ•°ã€‚\n",
      "\n",
      "\\subsection{æ¢¯åº¦ä¸‹é™}\n",
      "æ¢¯åº¦ä¸‹é™æ˜¯æœ€å¸¸ç”¨çš„ä¼˜åŒ–ç®—æ³•ï¼š\n",
      "\\begin{equation}\n",
      "\\theta_{t+1} = \\theta_t -...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['\\\\documentclass{article}\\n\\\\usepackage{amsmath}\\n\\\\title{æœºå™¨å­¦ä¹ æ•°å­¦åŸºç¡€}\\n\\\\author{AIç ”ç©¶å›¢é˜Ÿ}\\n\\n\\\\begin{document}\\n\\\\maketitle\\n\\n\\\\section{çº¿æ€§ä»£æ•°}\\nçº¿æ€§ä»£æ•°æ˜¯æœºå™¨å­¦ä¹ çš„æ•°å­¦åŸºç¡€ä¹‹ä¸€ã€‚\\n\\n\\\\subsection{å‘é‡}\\nå‘é‡æ˜¯å…·æœ‰å¤§å°å’Œæ–¹å‘çš„é‡ã€‚åœ¨æœºå™¨å­¦ä¹ ä¸­ï¼Œç‰¹å¾é€šå¸¸è¡¨ç¤ºä¸ºå‘é‡ã€‚\\n\\nè®¾å‘é‡ $\\\\mathbf{v} = [v_1, v_2, \\\\ldots, v_n]^T$ï¼Œå…¶ä¸­ $v_i$ æ˜¯ç¬¬ $i',\n",
       " '$ ä¸ªåˆ†é‡ã€‚\\n\\n\\\\subsection{çŸ©é˜µ}\\nçŸ©é˜µæ˜¯äºŒç»´æ•°ç»„ï¼Œç”¨äºè¡¨ç¤ºçº¿æ€§å˜æ¢ã€‚\\n\\nçŸ©é˜µä¹˜æ³•å®šä¹‰ä¸ºï¼š\\n\\\\begin{equation}\\n(\\\\mathbf{AB})_{ij} = \\\\sum_{k=1}^{n} a_{ik}b_{kj}\\n\\\\end{equation}\\n\\n\\\\section{æ¦‚ç‡è®º}\\næ¦‚ç‡è®ºä¸ºæœºå™¨å­¦ä¹ æä¾›äº†ä¸ç¡®å®šæ€§å»ºæ¨¡çš„å·¥å…·ã€‚\\n\\n\\\\subsection{è´å¶æ–¯å®šç†}\\nè´å¶æ–¯å®šç†æ˜¯æ¦‚ç‡è®ºçš„æ ¸å¿ƒï¼š\\n\\\\begin{equation}\\nP(A|B) =',\n",
       " '= \\\\frac{P(B|A)P(A)}{P(B)}\\n\\\\end{equation}\\n\\n\\\\section{ä¼˜åŒ–ç†è®º}\\nä¼˜åŒ–ç†è®ºç”¨äºå¯»æ‰¾æ¨¡å‹çš„æœ€ä¼˜å‚æ•°ã€‚\\n\\n\\\\subsection{æ¢¯åº¦ä¸‹é™}\\næ¢¯åº¦ä¸‹é™æ˜¯æœ€å¸¸ç”¨çš„ä¼˜åŒ–ç®—æ³•ï¼š\\n\\\\begin{equation}\\n\\\\theta_{t+1} = \\\\theta_t - \\\\alpha \\\\nabla_\\\\theta J(\\\\theta)\\n\\\\end{equation}\\n\\nå…¶ä¸­',\n",
       " '$\\\\alpha$ æ˜¯å­¦ä¹ ç‡ï¼Œ$J(\\\\theta)$ æ˜¯æŸå¤±å‡½æ•°ã€‚\\n\\n\\\\end{document}']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### é«˜çº§åˆ†å‰²æŠ€æœ¯",
   "id": "529c08d21ed984fb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T04:41:51.146275Z",
     "start_time": "2025-07-23T04:41:51.136142Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "def advanced_splitting_techniques():\n",
    "    \"\"\"é«˜çº§åˆ†å‰²æŠ€æœ¯\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"8. é«˜çº§åˆ†å‰²æŠ€æœ¯\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    doc = create_sample_documents()\n",
    "\n",
    "    # 8.1 åŠ¨æ€å—å¤§å°\n",
    "    print(\"\\n8.1 åŠ¨æ€å—å¤§å°è°ƒæ•´\")\n",
    "\n",
    "    def adaptive_chunk_size(text: str) -> int:\n",
    "        \"\"\"æ ¹æ®æ–‡æœ¬å¤æ‚åº¦åŠ¨æ€è°ƒæ•´å—å¤§å°\"\"\"\n",
    "        sentences = text.split('ã€‚')\n",
    "        avg_sentence_length = sum(len(s) for s in sentences) / len(sentences) if sentences else 100\n",
    "\n",
    "        if avg_sentence_length > 50:\n",
    "            return 300  # é•¿å¥å­ç”¨å¤§å—\n",
    "        elif avg_sentence_length > 30:\n",
    "            return 200  # ä¸­ç­‰å¥å­ç”¨ä¸­å—\n",
    "        else:\n",
    "            return 150  # çŸ­å¥å­ç”¨å°å—\n",
    "\n",
    "    adaptive_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=adaptive_chunk_size(doc.page_content),\n",
    "        chunk_overlap=50\n",
    "    )\n",
    "\n",
    "    adaptive_chunks = adaptive_splitter.split_documents([doc])\n",
    "    print(f\"è‡ªé€‚åº”åˆ†å‰²å—æ•°: {len(adaptive_chunks)}\")\n",
    "\n",
    "    # 8.2 è¯­ä¹‰æ„ŸçŸ¥åˆ†å‰²\n",
    "    print(\"\\n8.2 è¯­ä¹‰æ„ŸçŸ¥åˆ†å‰²\")\n",
    "\n",
    "    def semantic_splitter(text: str, max_chunk_size: int = 200) -> List[str]:\n",
    "        \"\"\"åŸºäºè¯­ä¹‰çš„åˆ†å‰²ï¼ˆç®€åŒ–ç‰ˆï¼‰\"\"\"\n",
    "        sentences = text.split('ã€‚')\n",
    "        chunks = []\n",
    "        current_chunk = \"\"\n",
    "\n",
    "        for sentence in sentences:\n",
    "            sentence = sentence.strip()\n",
    "            if not sentence:\n",
    "                continue\n",
    "\n",
    "            # æ£€æŸ¥æ˜¯å¦åŒ…å«å…³é”®è¯ï¼ˆæ–°ä¸»é¢˜å¼€å§‹ï¼‰\n",
    "            topic_keywords = ['ç„¶è€Œ', 'å¦å¤–', 'æ­¤å¤–', 'åŒæ—¶', 'å› æ­¤', 'æ€»ä¹‹']\n",
    "            is_new_topic = any(keyword in sentence for keyword in topic_keywords)\n",
    "\n",
    "            if (len(current_chunk) + len(sentence) > max_chunk_size) or is_new_topic:\n",
    "                if current_chunk:\n",
    "                    chunks.append(current_chunk.strip())\n",
    "                current_chunk = sentence + 'ã€‚'\n",
    "            else:\n",
    "                current_chunk += sentence + 'ã€‚'\n",
    "\n",
    "        if current_chunk:\n",
    "            chunks.append(current_chunk.strip())\n",
    "\n",
    "        return chunks\n",
    "\n",
    "    semantic_chunks = semantic_splitter(doc.page_content)\n",
    "    print(f\"è¯­ä¹‰åˆ†å‰²å—æ•°: {len(semantic_chunks)}\")\n",
    "\n",
    "    # 8.3 é‡å ç­–ç•¥ä¼˜åŒ–\n",
    "    print(\"\\n8.3 æ™ºèƒ½é‡å ç­–ç•¥\")\n",
    "\n",
    "    def smart_overlap_splitter(text: str, chunk_size: int = 200) -> List[Document]:\n",
    "        \"\"\"æ™ºèƒ½é‡å åˆ†å‰²\"\"\"\n",
    "        sentences = text.split('ã€‚')\n",
    "        chunks = []\n",
    "\n",
    "        for i in range(0, len(sentences), 3):  # æ¯3å¥ä¸ºä¸€å—\n",
    "            chunk_sentences = sentences[i:i+4]  # å–4å¥ï¼ˆåŒ…å«1å¥é‡å ï¼‰\n",
    "            chunk_text = 'ã€‚'.join(chunk_sentences).strip()\n",
    "\n",
    "            if chunk_text:\n",
    "                chunks.append(Document(\n",
    "                    page_content=chunk_text,\n",
    "                    metadata={\"chunk_id\": len(chunks), \"overlap_strategy\": \"sentence_based\"}\n",
    "                ))\n",
    "\n",
    "        return chunks\n",
    "\n",
    "    smart_chunks = smart_overlap_splitter(doc.page_content)\n",
    "    print(f\"æ™ºèƒ½é‡å åˆ†å‰²å—æ•°: {len(smart_chunks)}\")\n",
    "\n",
    "    return adaptive_chunks\n",
    "advanced_splitting_techniques()"
   ],
   "id": "eb577b3c88979ea0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "8. é«˜çº§åˆ†å‰²æŠ€æœ¯\n",
      "============================================================\n",
      "\n",
      "8.1 åŠ¨æ€å—å¤§å°è°ƒæ•´\n",
      "è‡ªé€‚åº”åˆ†å‰²å—æ•°: 4\n",
      "\n",
      "8.2 è¯­ä¹‰æ„ŸçŸ¥åˆ†å‰²\n",
      "è¯­ä¹‰åˆ†å‰²å—æ•°: 4\n",
      "\n",
      "8.3 æ™ºèƒ½é‡å ç­–ç•¥\n",
      "æ™ºèƒ½é‡å åˆ†å‰²å—æ•°: 5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'ai_history'}, page_content='äººå·¥æ™ºèƒ½çš„å‘å±•å†ç¨‹å¯ä»¥è¿½æº¯åˆ°20ä¸–çºª50å¹´ä»£ã€‚å½“æ—¶ï¼Œè®¡ç®—æœºç§‘å­¦å®¶å¼€å§‹æ¢ç´¢è®©æœºå™¨æ¨¡æ‹Ÿäººç±»æ™ºèƒ½çš„å¯èƒ½æ€§ã€‚\\n\\nåœ¨1956å¹´çš„è¾¾ç‰¹èŒ…æ–¯ä¼šè®®ä¸Šï¼Œäººå·¥æ™ºèƒ½è¿™ä¸ªæœ¯è¯­é¦–æ¬¡è¢«æ­£å¼æå‡ºã€‚è¿™æ ‡å¿—ç€AIä½œä¸ºä¸€ä¸ªç‹¬ç«‹å­¦ç§‘çš„è¯ç”Ÿã€‚\\n\\néšåçš„å‡ åå¹´é‡Œï¼ŒAIç»å†äº†å¤šæ¬¡èµ·ä¼ã€‚60-70å¹´ä»£æ˜¯ç¬¬ä¸€ä¸ªAIæ˜¥å¤©ï¼Œä¸“å®¶ç³»ç»Ÿå¾—åˆ°äº†å¹¿æ³›åº”ç”¨ã€‚'),\n",
       " Document(metadata={'source': 'ai_history'}, page_content='éšåçš„å‡ åå¹´é‡Œï¼ŒAIç»å†äº†å¤šæ¬¡èµ·ä¼ã€‚60-70å¹´ä»£æ˜¯ç¬¬ä¸€ä¸ªAIæ˜¥å¤©ï¼Œä¸“å®¶ç³»ç»Ÿå¾—åˆ°äº†å¹¿æ³›åº”ç”¨ã€‚\\n\\n80å¹´ä»£æœ«åˆ°90å¹´ä»£åˆï¼Œç”±äºæŠ€æœ¯é™åˆ¶å’Œè¿‡é«˜æœŸæœ›ï¼ŒAIè¿›å…¥äº†æ‰€è°“çš„\"AIå†¬å¤©\"ã€‚\\n\\n21ä¸–çºªä»¥æ¥ï¼Œéšç€å¤§æ•°æ®ã€äº‘è®¡ç®—å’Œæ·±åº¦å­¦ä¹ çš„å‘å±•ï¼ŒAIè¿æ¥äº†æ–°çš„æ˜¥å¤©ã€‚'),\n",
       " Document(metadata={'source': 'ai_history'}, page_content='21ä¸–çºªä»¥æ¥ï¼Œéšç€å¤§æ•°æ®ã€äº‘è®¡ç®—å’Œæ·±åº¦å­¦ä¹ çš„å‘å±•ï¼ŒAIè¿æ¥äº†æ–°çš„æ˜¥å¤©ã€‚\\n\\nä»Šå¤©ï¼ŒAIå·²ç»åœ¨å›¾åƒè¯†åˆ«ã€è‡ªç„¶è¯­è¨€å¤„ç†ã€æ¨èç³»ç»Ÿç­‰é¢†åŸŸå–å¾—äº†çªç ´æ€§è¿›å±•ã€‚\\n\\næœºå™¨å­¦ä¹ ä½œä¸ºAIçš„æ ¸å¿ƒæŠ€æœ¯ï¼ŒåŒ…æ‹¬ç›‘ç£å­¦ä¹ ã€æ— ç›‘ç£å­¦ä¹ å’Œå¼ºåŒ–å­¦ä¹ ä¸‰å¤§ç±»ã€‚æ·±åº¦å­¦ä¹ åˆ™æ˜¯æœºå™¨å­¦ä¹ çš„ä¸€ä¸ªé‡è¦åˆ†æ”¯ã€‚'),\n",
       " Document(metadata={'source': 'ai_history'}, page_content='è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰è®©è®¡ç®—æœºèƒ½å¤Ÿç†è§£å’Œç”Ÿæˆäººç±»è¯­è¨€ã€‚è®¡ç®—æœºè§†è§‰è®©æœºå™¨èƒ½å¤Ÿ\"çœ‹è§\"å’Œç†è§£å›¾åƒã€‚\\n\\næœªæ¥ï¼ŒAIå°†åœ¨æ›´å¤šé¢†åŸŸå‘æŒ¥é‡è¦ä½œç”¨ï¼ŒåŒ…æ‹¬åŒ»ç–—ã€æ•™è‚²ã€äº¤é€šã€é‡‘èç­‰ã€‚åŒæ—¶ï¼ŒAIçš„ä¼¦ç†å’Œå®‰å…¨é—®é¢˜ä¹Ÿéœ€è¦å¾—åˆ°é‡è§†ã€‚')]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### åˆ†å‰²å™¨æ€§èƒ½å¯¹æ¯”",
   "id": "d496c39856552c96"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T04:42:17.512714Z",
     "start_time": "2025-07-23T04:42:17.504253Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "def splitting_performance_comparison():\n",
    "    \"\"\"åˆ†å‰²å™¨æ€§èƒ½å¯¹æ¯”\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"9. åˆ†å‰²å™¨æ€§èƒ½å¯¹æ¯”\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    doc = create_sample_documents()\n",
    "\n",
    "    splitters = {\n",
    "        \"RecursiveCharacter\": RecursiveCharacterTextSplitter(chunk_size=200, chunk_overlap=50),\n",
    "        \"Character\": CharacterTextSplitter(chunk_size=200, chunk_overlap=50, separator=\"\\n\\n\"),\n",
    "        \"Token\": TokenTextSplitter(chunk_size=50, chunk_overlap=10)\n",
    "    }\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    for name, splitter in splitters.items():\n",
    "        try:\n",
    "            import time\n",
    "            start_time = time.time()\n",
    "            chunks = splitter.split_documents([doc])\n",
    "            end_time = time.time()\n",
    "\n",
    "            results[name] = {\n",
    "                \"chunks\": len(chunks),\n",
    "                \"time\": end_time - start_time,\n",
    "                \"avg_length\": sum(len(c.page_content) for c in chunks) / len(chunks),\n",
    "                \"min_length\": min(len(c.page_content) for c in chunks),\n",
    "                \"max_length\": max(len(c.page_content) for c in chunks)\n",
    "            }\n",
    "        except Exception as e:\n",
    "            results[name] = {\"error\": str(e)}\n",
    "\n",
    "    print(\"\\næ€§èƒ½å¯¹æ¯”ç»“æœ:\")\n",
    "    print(f\"{'åˆ†å‰²å™¨':<20} {'å—æ•°':<8} {'æ—¶é—´(s)':<10} {'å¹³å‡é•¿åº¦':<10} {'æœ€å°é•¿åº¦':<10} {'æœ€å¤§é•¿åº¦':<10}\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "    for name, result in results.items():\n",
    "        if \"error\" not in result:\n",
    "            print(f\"{name:<20} {result['chunks']:<8} {result['time']:<10.4f} \"\n",
    "                  f\"{result['avg_length']:<10.1f} {result['min_length']:<10} {result['max_length']:<10}\")\n",
    "        else:\n",
    "            print(f\"{name:<20} é”™è¯¯: {result['error']}\")\n",
    "splitting_performance_comparison()"
   ],
   "id": "f956334c9f412a6b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "9. åˆ†å‰²å™¨æ€§èƒ½å¯¹æ¯”\n",
      "============================================================\n",
      "\n",
      "æ€§èƒ½å¯¹æ¯”ç»“æœ:\n",
      "åˆ†å‰²å™¨                  å—æ•°       æ—¶é—´(s)      å¹³å‡é•¿åº¦       æœ€å°é•¿åº¦       æœ€å¤§é•¿åº¦      \n",
      "--------------------------------------------------------------------------------\n",
      "RecursiveCharacter   3        0.0000     154.0      103        190       \n",
      "Character            3        0.0000     154.0      103        190       \n",
      "Token                21       0.0010     25.9       10         32        \n"
     ]
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### è¿è¡Œæ‰€æœ‰æ–‡æœ¬åˆ†å‰²å™¨ç¤ºä¾‹",
   "id": "1f989884bffaaeb8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T04:42:52.546641Z",
     "start_time": "2025-07-23T04:42:52.537224Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "def main():\n",
    "    \"\"\"è¿è¡Œæ‰€æœ‰æ–‡æœ¬åˆ†å‰²å™¨ç¤ºä¾‹\"\"\"\n",
    "    print(\"ğŸš€ LangChain 0.3 Text Splitters å®Œæ•´ç¤ºä¾‹\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    # è¿è¡Œæ‰€æœ‰ç¤ºä¾‹\n",
    "    recursive_character_splitter_example()\n",
    "    character_text_splitter_example()\n",
    "    token_text_splitter_example()\n",
    "    markdown_header_splitter_example()\n",
    "    html_header_splitter_example()\n",
    "    code_splitter_example()\n",
    "    latex_splitter_example()\n",
    "    advanced_splitting_techniques()\n",
    "    splitting_performance_comparison()\n",
    "\n",
    "    print(\"\\nğŸ‰ æ‰€æœ‰æ–‡æœ¬åˆ†å‰²å™¨ç¤ºä¾‹è¿è¡Œå®Œæˆï¼\")\n",
    "\n",
    "    # æœ€ä½³å®è·µå»ºè®®\n",
    "    print(\"\\nğŸ“‹ æœ€ä½³å®è·µå»ºè®®:\")\n",
    "    print(\"1. é€šç”¨æ–‡æœ¬ï¼šä½¿ç”¨ RecursiveCharacterTextSplitter\")\n",
    "    print(\"2. ç»“æ„åŒ–æ–‡æ¡£ï¼šä½¿ç”¨å¯¹åº”çš„Headeråˆ†å‰²å™¨\")\n",
    "    print(\"3. ä»£ç æ–‡æ¡£ï¼šä½¿ç”¨ PythonCodeTextSplitter\")\n",
    "    print(\"4. Tokené™åˆ¶ï¼šä½¿ç”¨ TokenTextSplitter\")\n",
    "    print(\"5. ç®€å•éœ€æ±‚ï¼šä½¿ç”¨ CharacterTextSplitter\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "id": "9194c102d056b703",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ LangChain 0.3 Text Splitters å®Œæ•´ç¤ºä¾‹\n",
      "================================================================================\n",
      "============================================================\n",
      "1. RecursiveCharacterTextSplitterï¼ˆæ¨èï¼‰\n",
      "============================================================\n",
      "\n",
      "1.1 åŸºç¡€é€’å½’åˆ†å‰²\n",
      "åŸºç¡€åˆ†å‰²å—æ•°: 3\n",
      "å— 1 (é•¿åº¦: 190): äººå·¥æ™ºèƒ½çš„å‘å±•å†ç¨‹å¯ä»¥è¿½æº¯åˆ°20ä¸–çºª50å¹´ä»£ã€‚å½“æ—¶ï¼Œè®¡ç®—æœºç§‘å­¦å®¶å¼€å§‹æ¢ç´¢è®©æœºå™¨æ¨¡æ‹Ÿäººç±»æ™ºèƒ½çš„å¯èƒ½æ€§ã€‚\n",
      "\n",
      "åœ¨1956å¹´çš„è¾¾ç‰¹èŒ…æ–¯ä¼šè®®ä¸Šï¼Œäººå·¥æ™ºèƒ½è¿™ä¸ªæœ¯è¯­é¦–æ¬¡è¢«æ­£å¼...\n",
      "å— 2 (é•¿åº¦: 169): 80å¹´ä»£æœ«åˆ°90å¹´ä»£åˆï¼Œç”±äºæŠ€æœ¯é™åˆ¶å’Œè¿‡é«˜æœŸæœ›ï¼ŒAIè¿›å…¥äº†æ‰€è°“çš„\"AIå†¬å¤©\"ã€‚\n",
      "\n",
      "21ä¸–çºªä»¥æ¥ï¼Œéšç€å¤§æ•°æ®ã€äº‘è®¡ç®—å’Œæ·±åº¦å­¦ä¹ çš„å‘å±•ï¼ŒAIè¿æ¥äº†æ–°çš„æ˜¥å¤©ã€‚\n",
      "\n",
      "ä»Šå¤©...\n",
      "å— 3 (é•¿åº¦: 103): è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰è®©è®¡ç®—æœºèƒ½å¤Ÿç†è§£å’Œç”Ÿæˆäººç±»è¯­è¨€ã€‚è®¡ç®—æœºè§†è§‰è®©æœºå™¨èƒ½å¤Ÿ\"çœ‹è§\"å’Œç†è§£å›¾åƒã€‚\n",
      "\n",
      "æœªæ¥ï¼ŒAIå°†åœ¨æ›´å¤šé¢†åŸŸå‘æŒ¥é‡è¦ä½œç”¨ï¼ŒåŒ…æ‹¬åŒ»ç–—ã€æ•™è‚²ã€äº¤é€šã€é‡‘è...\n",
      "\n",
      "1.2 è‡ªå®šä¹‰åˆ†éš”ç¬¦ä¼˜å…ˆçº§\n",
      "è‡ªå®šä¹‰åˆ†å‰²å—æ•°: 4\n",
      "\n",
      "1.3 æ®µè½ä¼˜å…ˆåˆ†å‰²\n",
      "æ®µè½åˆ†å‰²å—æ•°: 2\n",
      "\n",
      "============================================================\n",
      "2. CharacterTextSplitter\n",
      "============================================================\n",
      "\n",
      "2.1 æŒ‰æ®µè½åˆ†å‰²\n",
      "æ®µè½åˆ†å‰²å—æ•°: 2\n",
      "\n",
      "2.2 æŒ‰å¥å­åˆ†å‰²\n",
      "å¥å­åˆ†å‰²å—æ•°: 6\n",
      "\n",
      "2.3 è‡ªå®šä¹‰åˆ†éš”ç¬¦\n",
      "è‡ªå®šä¹‰åˆ†å‰²å—æ•°: 2\n",
      "  - é¡¹ç›®A|é¡¹ç›®B|é¡¹ç›®C|é¡¹ç›®D\n",
      "  - é¡¹ç›®Eçš„è¯¦ç»†æè¿°å’Œåˆ†ææŠ¥å‘Š\n",
      "\n",
      "============================================================\n",
      "3. TokenTextSplitter\n",
      "============================================================\n",
      "\n",
      "3.1 åŸºç¡€Tokenåˆ†å‰²\n",
      "Tokenåˆ†å‰²å—æ•°: 6\n",
      "å— 1 Tokenæ•°: 100, å†…å®¹: äººå·¥æ™ºèƒ½çš„å‘å±•å†ç¨‹å¯ä»¥è¿½æº¯åˆ°20ä¸–çºª50å¹´ä»£ã€‚å½“æ—¶ï¼Œè®¡ç®—æœºç§‘å­¦å®¶å¼€å§‹æ¢ç´¢è®©æœºå™¨æ¨¡æ‹Ÿäººç±»æ™ºèƒ½çš„å¯èƒ½æ€§ã€‚\n",
      "\n",
      "åœ¨1956å¹´çš„è¾¾...\n",
      "å— 2 Tokenæ•°: 100, å†…å®¹: æ¬¡è¢«æ­£å¼æå‡ºã€‚è¿™æ ‡å¿—ç€AIä½œä¸ºä¸€ä¸ªç‹¬ç«‹å­¦ç§‘çš„è¯ç”Ÿã€‚\n",
      "\n",
      "éšåçš„å‡ åå¹´é‡Œï¼ŒAIç»å†äº†å¤šæ¬¡èµ·ä¼ã€‚60-70å¹´ä»£æ˜¯ç¬¬ä¸€ä¸ªAIæ˜¥å¤©...\n",
      "å— 3 Tokenæ•°: 100, å†…å®¹: å¹´ä»£åˆï¼Œç”±äºæŠ€æœ¯é™åˆ¶å’Œè¿‡é«˜æœŸæœ›ï¼ŒAIè¿›å…¥äº†æ‰€è°“çš„\"AIå†¬å¤©\"ã€‚\n",
      "\n",
      "21ä¸–çºªä»¥æ¥ï¼Œéšç€å¤§æ•°æ®ã€äº‘è®¡ç®—å’Œæ·±åº¦å­¦ä¹ çš„å‘å±•ï¼ŒAI...\n",
      "\n",
      "3.2 ä¸åŒæ¨¡å‹Tokenåˆ†å‰²å¯¹æ¯”\n",
      "gpt-3.5-turbo: 11 å—\n",
      "text-davinci-003: 21 å—\n",
      "gpt-4: 11 å—\n",
      "\n",
      "============================================================\n",
      "4. MarkdownHeaderTextSplitter\n",
      "============================================================\n",
      "\n",
      "4.1 åŸºç¡€æ ‡é¢˜åˆ†å‰²\n",
      "Markdownåˆ†å‰²å—æ•°: 8\n",
      "\n",
      "å— 1:\n",
      "å†…å®¹: ç›‘ç£å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„ä¸€ä¸ªé‡è¦åˆ†æ”¯ï¼Œä½¿ç”¨æ ‡è®°çš„è®­ç»ƒæ•°æ®æ¥å­¦ä¹ è¾“å…¥åˆ°è¾“å‡ºçš„æ˜ å°„ã€‚  \n",
      "å¸¸è§ç®—æ³•åŒ…æ‹¬ï¼š\n",
      "- çº¿æ€§å›å½’\n",
      "- é€»è¾‘å›å½’\n",
      "- å†³ç­–æ ‘\n",
      "- éšæœºæ£®æ—...\n",
      "å…ƒæ•°æ®: {'Header 1': 'äººå·¥æ™ºèƒ½æŠ€æœ¯æŒ‡å—', 'Header 2': '1. æœºå™¨å­¦ä¹ åŸºç¡€', 'Header 3': '1.1 ç›‘ç£å­¦ä¹ '}\n",
      "\n",
      "å— 2:\n",
      "å†…å®¹: æ— ç›‘ç£å­¦ä¹ ä»æœªæ ‡è®°çš„æ•°æ®ä¸­å‘ç°éšè—çš„æ¨¡å¼ã€‚  \n",
      "ä¸»è¦æ–¹æ³•ï¼š\n",
      "- èšç±»åˆ†æ\n",
      "- é™ç»´æŠ€æœ¯\n",
      "- å…³è”è§„åˆ™æŒ–æ˜...\n",
      "å…ƒæ•°æ®: {'Header 1': 'äººå·¥æ™ºèƒ½æŠ€æœ¯æŒ‡å—', 'Header 2': '1. æœºå™¨å­¦ä¹ åŸºç¡€', 'Header 3': '1.2 æ— ç›‘ç£å­¦ä¹ '}\n",
      "\n",
      "å— 3:\n",
      "å†…å®¹: ç¥ç»ç½‘ç»œæ˜¯æ·±åº¦å­¦ä¹ çš„åŸºç¡€ï¼Œæ¨¡æ‹Ÿäººè„‘ç¥ç»å…ƒçš„å·¥ä½œæ–¹å¼ã€‚...\n",
      "å…ƒæ•°æ®: {'Header 1': 'äººå·¥æ™ºèƒ½æŠ€æœ¯æŒ‡å—', 'Header 2': '2. æ·±åº¦å­¦ä¹ ', 'Header 3': '2.1 ç¥ç»ç½‘ç»œåŸºç¡€'}\n",
      "\n",
      "å— 4:\n",
      "å†…å®¹: CNNä¸»è¦ç”¨äºå›¾åƒå¤„ç†å’Œè®¡ç®—æœºè§†è§‰ä»»åŠ¡ã€‚...\n",
      "å…ƒæ•°æ®: {'Header 1': 'äººå·¥æ™ºèƒ½æŠ€æœ¯æŒ‡å—', 'Header 2': '2. æ·±åº¦å­¦ä¹ ', 'Header 3': '2.2 å·ç§¯ç¥ç»ç½‘ç»œ'}\n",
      "\n",
      "å— 5:\n",
      "å†…å®¹: RNNé€‚åˆå¤„ç†åºåˆ—æ•°æ®ï¼Œå¦‚æ–‡æœ¬å’Œæ—¶é—´åºåˆ—ã€‚...\n",
      "å…ƒæ•°æ®: {'Header 1': 'äººå·¥æ™ºèƒ½æŠ€æœ¯æŒ‡å—', 'Header 2': '2. æ·±åº¦å­¦ä¹ ', 'Header 3': '2.3 å¾ªç¯ç¥ç»ç½‘ç»œ'}\n",
      "\n",
      "4.2 ç»“åˆé€’å½’åˆ†å‰²å™¨è¿›è¡ŒäºŒæ¬¡åˆ†å‰²\n",
      "äºŒæ¬¡åˆ†å‰²åå—æ•°: 8\n",
      "\n",
      "============================================================\n",
      "5. HTMLHeaderTextSplitter\n",
      "============================================================\n",
      "HTMLåˆ†å‰²å—æ•°: 14\n",
      "\n",
      "å— 1:\n",
      "å†…å®¹: äººå·¥æ™ºèƒ½æ¦‚è¿°...\n",
      "å…ƒæ•°æ®: {'Header 1': 'äººå·¥æ™ºèƒ½æ¦‚è¿°'}\n",
      "\n",
      "å— 2:\n",
      "å†…å®¹: äººå·¥æ™ºèƒ½æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªåˆ†æ”¯ã€‚...\n",
      "å…ƒæ•°æ®: {'Header 1': 'äººå·¥æ™ºèƒ½æ¦‚è¿°'}\n",
      "\n",
      "å— 3:\n",
      "å†…å®¹: æœºå™¨å­¦ä¹ ...\n",
      "å…ƒæ•°æ®: {'Header 1': 'äººå·¥æ™ºèƒ½æ¦‚è¿°', 'Header 2': 'æœºå™¨å­¦ä¹ '}\n",
      "\n",
      "============================================================\n",
      "6. PythonCodeTextSplitter\n",
      "============================================================\n",
      "Pythonä»£ç åˆ†å‰²å—æ•°: 5\n",
      "\n",
      "ä»£ç å— 1 (é•¿åº¦: 188):\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.linear_model import LinearRegression\n",
      "from sklearn.metrics import mean_squared_error\n",
      "\n",
      "ä»£ç å— 2 (é•¿åº¦: 352):\n",
      "class DataProcessor:\n",
      "    \"\"\"æ•°æ®å¤„ç†ç±»\"\"\"\n",
      "\n",
      "    def __init__(self, data_path):\n",
      "        self.data_path = data_path\n",
      "        self.data = None\n",
      "\n",
      "    def load_data(self):\n",
      "        \"\"\"åŠ è½½æ•°æ®\"\"\"\n",
      "        self.data = pd...\n",
      "\n",
      "ä»£ç å— 3 (é•¿åº¦: 288):\n",
      "# ç‰¹å¾ç¼©æ”¾\n",
      "        from sklearn.preprocessing import StandardScaler\n",
      "        scaler = StandardScaler()\n",
      "        numeric_columns = self.data.select_dtypes(include=[np.number]).columns\n",
      "        self.data[numer...\n",
      "\n",
      "============================================================\n",
      "7. LatexTextSplitter\n",
      "============================================================\n",
      "LaTeXåˆ†å‰²å—æ•°: 4\n",
      "\n",
      "LaTeXå— 1:\n",
      "\\documentclass{article}\n",
      "\\usepackage{amsmath}\n",
      "\\title{æœºå™¨å­¦ä¹ æ•°å­¦åŸºç¡€}\n",
      "\\author{AIç ”ç©¶å›¢é˜Ÿ}\n",
      "\n",
      "\\begin{document}\n",
      "\\maketitle\n",
      "\n",
      "\\section{çº¿æ€§ä»£æ•°}\n",
      "çº¿æ€§ä»£æ•°æ˜¯æœºå™¨å­¦ä¹ çš„æ•°å­¦åŸºç¡€ä¹‹ä¸€ã€‚\n",
      "\n",
      "\\subsect...\n",
      "\n",
      "LaTeXå— 2:\n",
      "$ ä¸ªåˆ†é‡ã€‚\n",
      "\n",
      "\\subsection{çŸ©é˜µ}\n",
      "çŸ©é˜µæ˜¯äºŒç»´æ•°ç»„ï¼Œç”¨äºè¡¨ç¤ºçº¿æ€§å˜æ¢ã€‚\n",
      "\n",
      "çŸ©é˜µä¹˜æ³•å®šä¹‰ä¸ºï¼š\n",
      "\\begin{equation}\n",
      "(\\mathbf{AB})_{ij} = \\sum_{k=1}^{n} a_{ik}b_{kj}\n",
      "\\end{equation}\n",
      "\n",
      "\\section{æ¦‚ç‡è®º}\n",
      "æ¦‚ç‡...\n",
      "\n",
      "LaTeXå— 3:\n",
      "= \\frac{P(B|A)P(A)}{P(B)}\n",
      "\\end{equation}\n",
      "\n",
      "\\section{ä¼˜åŒ–ç†è®º}\n",
      "ä¼˜åŒ–ç†è®ºç”¨äºå¯»æ‰¾æ¨¡å‹çš„æœ€ä¼˜å‚æ•°ã€‚\n",
      "\n",
      "\\subsection{æ¢¯åº¦ä¸‹é™}\n",
      "æ¢¯åº¦ä¸‹é™æ˜¯æœ€å¸¸ç”¨çš„ä¼˜åŒ–ç®—æ³•ï¼š\n",
      "\\begin{equation}\n",
      "\\theta_{t+1} = \\theta_t -...\n",
      "\n",
      "============================================================\n",
      "8. é«˜çº§åˆ†å‰²æŠ€æœ¯\n",
      "============================================================\n",
      "\n",
      "8.1 åŠ¨æ€å—å¤§å°è°ƒæ•´\n",
      "è‡ªé€‚åº”åˆ†å‰²å—æ•°: 4\n",
      "\n",
      "8.2 è¯­ä¹‰æ„ŸçŸ¥åˆ†å‰²\n",
      "è¯­ä¹‰åˆ†å‰²å—æ•°: 4\n",
      "\n",
      "8.3 æ™ºèƒ½é‡å ç­–ç•¥\n",
      "æ™ºèƒ½é‡å åˆ†å‰²å—æ•°: 5\n",
      "\n",
      "============================================================\n",
      "9. åˆ†å‰²å™¨æ€§èƒ½å¯¹æ¯”\n",
      "============================================================\n",
      "\n",
      "æ€§èƒ½å¯¹æ¯”ç»“æœ:\n",
      "åˆ†å‰²å™¨                  å—æ•°       æ—¶é—´(s)      å¹³å‡é•¿åº¦       æœ€å°é•¿åº¦       æœ€å¤§é•¿åº¦      \n",
      "--------------------------------------------------------------------------------\n",
      "RecursiveCharacter   3        0.0000     154.0      103        190       \n",
      "Character            3        0.0000     154.0      103        190       \n",
      "Token                21       0.0000     25.9       10         32        \n",
      "\n",
      "ğŸ‰ æ‰€æœ‰æ–‡æœ¬åˆ†å‰²å™¨ç¤ºä¾‹è¿è¡Œå®Œæˆï¼\n",
      "\n",
      "ğŸ“‹ æœ€ä½³å®è·µå»ºè®®:\n",
      "1. é€šç”¨æ–‡æœ¬ï¼šä½¿ç”¨ RecursiveCharacterTextSplitter\n",
      "2. ç»“æ„åŒ–æ–‡æ¡£ï¼šä½¿ç”¨å¯¹åº”çš„Headeråˆ†å‰²å™¨\n",
      "3. ä»£ç æ–‡æ¡£ï¼šä½¿ç”¨ PythonCodeTextSplitter\n",
      "4. Tokené™åˆ¶ï¼šä½¿ç”¨ TokenTextSplitter\n",
      "5. ç®€å•éœ€æ±‚ï¼šä½¿ç”¨ CharacterTextSplitter\n"
     ]
    }
   ],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T03:08:17.593085Z",
     "start_time": "2025-07-23T03:08:17.414855Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 2. Text Splitters ç¤ºä¾‹\n",
    "def text_splitters_example(documents: List[Document]):\n",
    "    \"\"\"æ–‡æœ¬åˆ†å‰²å™¨ç¤ºä¾‹\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"2. Text Splitters æ–‡æœ¬åˆ†å‰²å™¨ç¤ºä¾‹\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # åˆ›å»ºé•¿æ–‡æœ¬ç”¨äºåˆ†å‰²\n",
    "    long_text = \"\"\"\n",
    "    äººå·¥æ™ºèƒ½çš„å‘å±•å†ç¨‹å¯ä»¥è¿½æº¯åˆ°20ä¸–çºª50å¹´ä»£ã€‚å½“æ—¶ï¼Œè®¡ç®—æœºç§‘å­¦å®¶å¼€å§‹æ¢ç´¢è®©æœºå™¨æ¨¡æ‹Ÿäººç±»æ™ºèƒ½çš„å¯èƒ½æ€§ã€‚\n",
    "\n",
    "    åœ¨1956å¹´çš„è¾¾ç‰¹èŒ…æ–¯ä¼šè®®ä¸Šï¼Œäººå·¥æ™ºèƒ½è¿™ä¸ªæœ¯è¯­é¦–æ¬¡è¢«æ­£å¼æå‡ºã€‚è¿™æ ‡å¿—ç€AIä½œä¸ºä¸€ä¸ªç‹¬ç«‹å­¦ç§‘çš„è¯ç”Ÿã€‚\n",
    "\n",
    "    éšåçš„å‡ åå¹´é‡Œï¼ŒAIç»å†äº†å¤šæ¬¡èµ·ä¼ã€‚60-70å¹´ä»£æ˜¯ç¬¬ä¸€ä¸ªAIæ˜¥å¤©ï¼Œä¸“å®¶ç³»ç»Ÿå¾—åˆ°äº†å¹¿æ³›åº”ç”¨ã€‚\n",
    "\n",
    "    80å¹´ä»£æœ«åˆ°90å¹´ä»£åˆï¼Œç”±äºæŠ€æœ¯é™åˆ¶å’Œè¿‡é«˜æœŸæœ›ï¼ŒAIè¿›å…¥äº†æ‰€è°“çš„\"AIå†¬å¤©\"ã€‚\n",
    "\n",
    "    21ä¸–çºªä»¥æ¥ï¼Œéšç€å¤§æ•°æ®ã€äº‘è®¡ç®—å’Œæ·±åº¦å­¦ä¹ çš„å‘å±•ï¼ŒAIè¿æ¥äº†æ–°çš„æ˜¥å¤©ã€‚\n",
    "\n",
    "    ä»Šå¤©ï¼ŒAIå·²ç»åœ¨å›¾åƒè¯†åˆ«ã€è‡ªç„¶è¯­è¨€å¤„ç†ã€æ¨èç³»ç»Ÿç­‰é¢†åŸŸå–å¾—äº†çªç ´æ€§è¿›å±•ã€‚\n",
    "    \"\"\"\n",
    "\n",
    "    long_doc = Document(page_content=long_text, metadata={\"source\": \"ai_history\"})\n",
    "\n",
    "    # 2.1 é€’å½’å­—ç¬¦åˆ†å‰²å™¨ï¼ˆæ¨èï¼‰\n",
    "    print(\"\\n2.1 RecursiveCharacterTextSplitter\")\n",
    "    recursive_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=200,\n",
    "        chunk_overlap=50,\n",
    "        length_function=len,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \"ã€‚\", \"ï¼Œ\", \" \", \"\"]\n",
    "    )\n",
    "\n",
    "    recursive_chunks = recursive_splitter.split_documents([long_doc])\n",
    "    print(f\"é€’å½’åˆ†å‰²å—æ•°: {len(recursive_chunks)}\")\n",
    "    for i, chunk in enumerate(recursive_chunks[:2]):\n",
    "        print(f\"å— {i + 1}: {chunk.page_content[:100]}...\")\n",
    "\n",
    "    # 2.2 å­—ç¬¦åˆ†å‰²å™¨\n",
    "    print(\"\\n2.2 CharacterTextSplitter\")\n",
    "    char_splitter = CharacterTextSplitter(\n",
    "        chunk_size=300,\n",
    "        chunk_overlap=50,\n",
    "        separator=\"\\n\\n\"\n",
    "    )\n",
    "\n",
    "    char_chunks = char_splitter.split_documents([long_doc])\n",
    "    print(f\"å­—ç¬¦åˆ†å‰²å—æ•°: {len(char_chunks)}\")\n",
    "\n",
    "    # 2.3 Tokenåˆ†å‰²å™¨\n",
    "    print(\"\\n2.3 TokenTextSplitter\")\n",
    "    token_splitter = TokenTextSplitter(\n",
    "        chunk_size=100,\n",
    "        chunk_overlap=20\n",
    "    )\n",
    "\n",
    "    token_chunks = token_splitter.split_documents([long_doc])\n",
    "    print(f\"Tokenåˆ†å‰²å—æ•°: {len(token_chunks)}\")\n",
    "\n",
    "    # 2.4 Markdownåˆ†å‰²å™¨\n",
    "    print(\"\\n2.4 MarkdownHeaderTextSplitter\")\n",
    "    markdown_text = \"\"\"\n",
    "# äººå·¥æ™ºèƒ½æ¦‚è¿°\n",
    "\n",
    "## ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½\n",
    "äººå·¥æ™ºèƒ½æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªåˆ†æ”¯ã€‚\n",
    "\n",
    "## AIçš„åº”ç”¨é¢†åŸŸ\n",
    "\n",
    "### è‡ªç„¶è¯­è¨€å¤„ç†\n",
    "NLPæ˜¯AIçš„é‡è¦åˆ†æ”¯ã€‚\n",
    "\n",
    "### è®¡ç®—æœºè§†è§‰\n",
    "è®¡ç®—æœºè§†è§‰è®©æœºå™¨èƒ½å¤Ÿ\"çœ‹è§\"ã€‚\n",
    "\n",
    "## æœªæ¥å‘å±•\n",
    "AIå°†ç»§ç»­å¿«é€Ÿå‘å±•ã€‚\n",
    "\"\"\"\n",
    "\n",
    "    markdown_splitter = MarkdownHeaderTextSplitter(\n",
    "        headers_to_split_on=[\n",
    "            (\"#\", \"Header 1\"),\n",
    "            (\"##\", \"Header 2\"),\n",
    "            (\"###\", \"Header 3\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    md_doc = Document(page_content=markdown_text)\n",
    "    md_chunks = markdown_splitter.split_text(markdown_text)\n",
    "    print(f\"Markdownåˆ†å‰²å—æ•°: {len(md_chunks)}\")\n",
    "\n",
    "    return recursive_chunks\n",
    "# 2. æ–‡æœ¬åˆ†å‰²\n",
    "chunks = text_splitters_example(documents)"
   ],
   "id": "1279205a511e600e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "2. Text Splitters æ–‡æœ¬åˆ†å‰²å™¨ç¤ºä¾‹\n",
      "============================================================\n",
      "\n",
      "2.1 RecursiveCharacterTextSplitter\n",
      "é€’å½’åˆ†å‰²å—æ•°: 2\n",
      "å— 1: äººå·¥æ™ºèƒ½çš„å‘å±•å†ç¨‹å¯ä»¥è¿½æº¯åˆ°20ä¸–çºª50å¹´ä»£ã€‚å½“æ—¶ï¼Œè®¡ç®—æœºç§‘å­¦å®¶å¼€å§‹æ¢ç´¢è®©æœºå™¨æ¨¡æ‹Ÿäººç±»æ™ºèƒ½çš„å¯èƒ½æ€§ã€‚\n",
      "\n",
      "    åœ¨1956å¹´çš„è¾¾ç‰¹èŒ…æ–¯ä¼šè®®ä¸Šï¼Œäººå·¥æ™ºèƒ½è¿™ä¸ªæœ¯è¯­é¦–æ¬¡è¢«æ­£å¼æå‡ºã€‚è¿™æ ‡å¿—ç€AIä½œä¸ºä¸€ä¸ªç‹¬ç«‹å­¦...\n",
      "å— 2: 80å¹´ä»£æœ«åˆ°90å¹´ä»£åˆï¼Œç”±äºæŠ€æœ¯é™åˆ¶å’Œè¿‡é«˜æœŸæœ›ï¼ŒAIè¿›å…¥äº†æ‰€è°“çš„\"AIå†¬å¤©\"ã€‚\n",
      "\n",
      "    21ä¸–çºªä»¥æ¥ï¼Œéšç€å¤§æ•°æ®ã€äº‘è®¡ç®—å’Œæ·±åº¦å­¦ä¹ çš„å‘å±•ï¼ŒAIè¿æ¥äº†æ–°çš„æ˜¥å¤©ã€‚\n",
      "\n",
      "    ä»Šå¤©ï¼ŒAIå·²ç»åœ¨å›¾åƒè¯†åˆ«ã€è‡ª...\n",
      "\n",
      "2.2 CharacterTextSplitter\n",
      "å­—ç¬¦åˆ†å‰²å—æ•°: 1\n",
      "\n",
      "2.3 TokenTextSplitter\n",
      "Tokenåˆ†å‰²å—æ•°: 7\n",
      "\n",
      "2.4 MarkdownHeaderTextSplitter\n",
      "Markdownåˆ†å‰²å—æ•°: 4\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 3. Embedding Models ç¤ºä¾‹",
   "id": "88b30c524eb356d8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T04:52:14.218680Z",
     "start_time": "2025-07-23T04:52:13.636641Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "LangChain 0.3 Embedding Models å®Œæ•´ç¤ºä¾‹\n",
    "åŒ…å«æ‰€æœ‰ä¸»è¦åµŒå…¥æ¨¡å‹å’Œé«˜çº§ç”¨æ³•\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import asyncio\n",
    "from typing import List, Dict, Any, Optional\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import time\n",
    "\n",
    "# æ ¸å¿ƒåµŒå…¥æ¨¡å‹å¯¼å…¥\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.embeddings import (\n",
    "    HuggingFaceEmbeddings,\n",
    "    HuggingFaceInstructEmbeddings,\n",
    "    SentenceTransformerEmbeddings,\n",
    "    CohereEmbeddings,\n",
    "    BedrockEmbeddings\n",
    ")\n",
    "from langchain_core.documents import Document"
   ],
   "id": "355d73806f13e63",
   "outputs": [],
   "execution_count": 34
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### OllamaåµŒå…¥æ¨¡å‹ç¤ºä¾‹",
   "id": "2cdaefec8bb544b2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T04:57:51.681456Z",
     "start_time": "2025-07-23T04:57:51.478789Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "def ollama_embeddings_example():\n",
    "    \"\"\"OllamaåµŒå…¥æ¨¡å‹ç¤ºä¾‹ - æœ¬åœ°éƒ¨ç½²\"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"1. OllamaåµŒå…¥æ¨¡å‹ç¤ºä¾‹\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    try:\n",
    "        # 1.1 åŸºç¡€OllamaåµŒå…¥\n",
    "        print(\"\\n1.1 åŸºç¡€OllamaåµŒå…¥æ¨¡å‹\")\n",
    "        embeddings = OllamaEmbeddings(\n",
    "            base_url=\"http://localhost:11434\",\n",
    "            model=\"nomic-embed-text:latest\",  # æ¨èçš„åµŒå…¥æ¨¡å‹\n",
    "        )\n",
    "\n",
    "        # æµ‹è¯•æ–‡æœ¬\n",
    "        texts = [\n",
    "            \"äººå·¥æ™ºèƒ½æ˜¯è®¡ç®—æœºç§‘å­¦çš„åˆ†æ”¯\",\n",
    "            \"æœºå™¨å­¦ä¹ æ˜¯AIçš„é‡è¦ç»„æˆéƒ¨åˆ†\",\n",
    "            \"æ·±åº¦å­¦ä¹ ä½¿ç”¨ç¥ç»ç½‘ç»œè¿›è¡Œå­¦ä¹ \",\n",
    "            \"è‡ªç„¶è¯­è¨€å¤„ç†è®©è®¡ç®—æœºç†è§£äººç±»è¯­è¨€\",\n",
    "            \"ä»Šå¤©å¤©æ°”å¾ˆå¥½ï¼Œé€‚åˆå‡ºé—¨æ•£æ­¥\"\n",
    "        ]\n",
    "\n",
    "        # ç”Ÿæˆæ–‡æ¡£åµŒå…¥\n",
    "        print(\"ç”Ÿæˆæ–‡æ¡£åµŒå…¥...\")\n",
    "        doc_embeddings = embeddings.embed_documents(texts)\n",
    "        print(f\"æ–‡æ¡£åµŒå…¥æ•°é‡: {len(doc_embeddings)}\")\n",
    "        print(f\"åµŒå…¥å‘é‡ç»´åº¦: {len(doc_embeddings[0])}\")\n",
    "\n",
    "        # ç”ŸæˆæŸ¥è¯¢åµŒå…¥\n",
    "        query = \"ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½æŠ€æœ¯ï¼Ÿ\"\n",
    "        query_embedding = embeddings.embed_query(query)\n",
    "        print(f\"æŸ¥è¯¢åµŒå…¥ç»´åº¦: {len(query_embedding)}\")\n",
    "\n",
    "        # 1.2 è®¡ç®—ç›¸ä¼¼åº¦\n",
    "        print(\"\\n1.2 è¯­ä¹‰ç›¸ä¼¼åº¦è®¡ç®—\")\n",
    "\n",
    "        def cosine_similarity(a: List[float], b: List[float]) -> float:\n",
    "            \"\"\"è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦\"\"\"\n",
    "            a_np = np.array(a)\n",
    "            b_np = np.array(b)\n",
    "            return np.dot(a_np, b_np) / (np.linalg.norm(a_np) * np.linalg.norm(b_np))\n",
    "\n",
    "        print(f\"æŸ¥è¯¢: '{query}'\")\n",
    "        print(\"ä¸å„æ–‡æ¡£çš„ç›¸ä¼¼åº¦:\")\n",
    "        similarities = []\n",
    "        for i, text in enumerate(texts):\n",
    "            similarity = cosine_similarity(query_embedding, doc_embeddings[i])\n",
    "            similarities.append((text, similarity))\n",
    "            print(f\"{i+1}. {similarity:.4f} - {text}\")\n",
    "\n",
    "        # æ’åºæ˜¾ç¤ºæœ€ç›¸ä¼¼çš„æ–‡æ¡£\n",
    "        similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "        print(f\"\\næœ€ç›¸ä¼¼æ–‡æ¡£: {similarities[0][0]} (ç›¸ä¼¼åº¦: {similarities[0][1]:.4f})\")\n",
    "\n",
    "        # # 1.3 ä¸åŒOllamaæ¨¡å‹å¯¹æ¯”\n",
    "        # print(\"\\n1.3 ä¸åŒOllamaåµŒå…¥æ¨¡å‹å¯¹æ¯”\")\n",
    "        # ollama_models = [\n",
    "        #     \"nomic-embed-text\",\n",
    "        #     \"mxbai-embed-large\",\n",
    "        #     \"all-minilm\"\n",
    "        # ]\n",
    "        #\n",
    "        # for model_name in ollama_models:\n",
    "        #     try:\n",
    "        #         model_embeddings = OllamaEmbeddings(\n",
    "        #             base_url=\"http://localhost:11434\",\n",
    "        #             model=model_name\n",
    "        #         )\n",
    "        #         test_embedding = model_embeddings.embed_query(\"æµ‹è¯•æ–‡æœ¬\")\n",
    "        #         print(f\"{model_name}: ç»´åº¦ {len(test_embedding)}\")\n",
    "        #     except Exception as e:\n",
    "        #         print(f\"{model_name}: ä¸å¯ç”¨ ({str(e)[:50]}...)\")\n",
    "\n",
    "        return embeddings\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"OllamaåµŒå…¥æ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {e}\")\n",
    "        print(\"è¯·ç¡®ä¿OllamaæœåŠ¡æ­£åœ¨è¿è¡Œå¹¶å®‰è£…äº†åµŒå…¥æ¨¡å‹\")\n",
    "        print(\"å®‰è£…å‘½ä»¤: ollama pull nomic-embed-text\")\n",
    "        return None\n",
    "ollama_embeddings_example()"
   ],
   "id": "f4693db3bc495453",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "1. OllamaåµŒå…¥æ¨¡å‹ç¤ºä¾‹\n",
      "============================================================\n",
      "\n",
      "1.1 åŸºç¡€OllamaåµŒå…¥æ¨¡å‹\n",
      "ç”Ÿæˆæ–‡æ¡£åµŒå…¥...\n",
      "æ–‡æ¡£åµŒå…¥æ•°é‡: 5\n",
      "åµŒå…¥å‘é‡ç»´åº¦: 768\n",
      "æŸ¥è¯¢åµŒå…¥ç»´åº¦: 768\n",
      "\n",
      "1.2 è¯­ä¹‰ç›¸ä¼¼åº¦è®¡ç®—\n",
      "æŸ¥è¯¢: 'ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½æŠ€æœ¯ï¼Ÿ'\n",
      "ä¸å„æ–‡æ¡£çš„ç›¸ä¼¼åº¦:\n",
      "1. 0.8601 - äººå·¥æ™ºèƒ½æ˜¯è®¡ç®—æœºç§‘å­¦çš„åˆ†æ”¯\n",
      "2. 0.5262 - æœºå™¨å­¦ä¹ æ˜¯AIçš„é‡è¦ç»„æˆéƒ¨åˆ†\n",
      "3. 0.5862 - æ·±åº¦å­¦ä¹ ä½¿ç”¨ç¥ç»ç½‘ç»œè¿›è¡Œå­¦ä¹ \n",
      "4. 0.7732 - è‡ªç„¶è¯­è¨€å¤„ç†è®©è®¡ç®—æœºç†è§£äººç±»è¯­è¨€\n",
      "5. 0.5400 - ä»Šå¤©å¤©æ°”å¾ˆå¥½ï¼Œé€‚åˆå‡ºé—¨æ•£æ­¥\n",
      "\n",
      "æœ€ç›¸ä¼¼æ–‡æ¡£: äººå·¥æ™ºèƒ½æ˜¯è®¡ç®—æœºç§‘å­¦çš„åˆ†æ”¯ (ç›¸ä¼¼åº¦: 0.8601)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "OllamaEmbeddings(model='nomic-embed-text:latest', validate_model_on_init=False, base_url='http://localhost:11434', client_kwargs={}, async_client_kwargs={}, sync_client_kwargs={}, mirostat=None, mirostat_eta=None, mirostat_tau=None, num_ctx=None, num_gpu=None, keep_alive=None, num_thread=None, repeat_last_n=None, repeat_penalty=None, temperature=None, stop=None, tfs_z=None, top_k=None, top_p=None)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 41
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### OpenAIåµŒå…¥æ¨¡å‹ç¤ºä¾‹",
   "id": "8681326b3e89e9d4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "def openai_embeddings_example():\n",
    "    \"\"\"OpenAIåµŒå…¥æ¨¡å‹ç¤ºä¾‹\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"2. OpenAIåµŒå…¥æ¨¡å‹ç¤ºä¾‹\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    try:\n",
    "        # 2.1 åŸºç¡€OpenAIåµŒå…¥\n",
    "        print(\"\\n2.1 åŸºç¡€OpenAIåµŒå…¥\")\n",
    "        embeddings = OpenAIEmbeddings(\n",
    "            model=\"text-embedding-3-small\",  # æ–°ç‰ˆæœ¬æ¨¡å‹\n",
    "            api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "            dimensions=1536  # å¯é€‰ï¼šæŒ‡å®šç»´åº¦\n",
    "        )\n",
    "\n",
    "        texts = [\n",
    "            \"Artificial intelligence is a branch of computer science\",\n",
    "            \"Machine learning is a subset of AI\",\n",
    "            \"Deep learning uses neural networks\",\n",
    "            \"Natural language processing enables computers to understand human language\"\n",
    "        ]\n",
    "\n",
    "        doc_embeddings = embeddings.embed_documents(texts)\n",
    "        query_embedding = embeddings.embed_query(\"What is artificial intelligence?\")\n",
    "\n",
    "        print(f\"OpenAIåµŒå…¥ç»´åº¦: {len(doc_embeddings[0])}\")\n",
    "\n",
    "        # 2.2 ä¸åŒOpenAIæ¨¡å‹å¯¹æ¯”\n",
    "        print(\"\\n2.2 OpenAIæ¨¡å‹å¯¹æ¯”\")\n",
    "        openai_models = [\n",
    "            (\"text-embedding-3-small\", 1536),\n",
    "            (\"text-embedding-3-large\", 3072),\n",
    "            (\"text-embedding-ada-002\", 1536)\n",
    "        ]\n",
    "\n",
    "        for model_name, default_dim in openai_models:\n",
    "            try:\n",
    "                model_embeddings = OpenAIEmbeddings(\n",
    "                    model=model_name,\n",
    "                    api_key=os.getenv(\"OPENAI_API_KEY\")\n",
    "                )\n",
    "                test_embedding = model_embeddings.embed_query(\"test\")\n",
    "                print(f\"{model_name}: ç»´åº¦ {len(test_embedding)}\")\n",
    "            except Exception as e:\n",
    "                print(f\"{model_name}: ä¸å¯ç”¨ ({str(e)[:50]}...)\")\n",
    "\n",
    "        # 2.3 è‡ªå®šä¹‰ç»´åº¦ï¼ˆä»…æ”¯æŒtext-embedding-3ç³»åˆ—ï¼‰\n",
    "        print(\"\\n2.3 è‡ªå®šä¹‰åµŒå…¥ç»´åº¦\")\n",
    "        try:\n",
    "            custom_embeddings = OpenAIEmbeddings(\n",
    "                model=\"text-embedding-3-large\",\n",
    "                dimensions=1024,  # è‡ªå®šä¹‰ç»´åº¦\n",
    "                api_key=os.getenv(\"OPENAI_API_KEY\")\n",
    "            )\n",
    "            custom_embedding = custom_embeddings.embed_query(\"è‡ªå®šä¹‰ç»´åº¦æµ‹è¯•\")\n",
    "            print(f\"è‡ªå®šä¹‰ç»´åº¦åµŒå…¥: {len(custom_embedding)}\")\n",
    "        except Exception as e:\n",
    "            print(f\"è‡ªå®šä¹‰ç»´åº¦å¤±è´¥: {e}\")\n",
    "\n",
    "        return embeddings\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"OpenAIåµŒå…¥æ¨¡å‹å¤±è´¥: {e}\")\n",
    "        print(\"è¯·è®¾ç½®OPENAI_API_KEYç¯å¢ƒå˜é‡\")\n",
    "        return None"
   ],
   "id": "9eea52b0b752046a",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### HuggingFaceåµŒå…¥æ¨¡å‹ç¤ºä¾‹",
   "id": "842aaf54fead0da4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "def huggingface_embeddings_example():\n",
    "    \"\"\"HuggingFaceåµŒå…¥æ¨¡å‹ç¤ºä¾‹\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"3. HuggingFaceåµŒå…¥æ¨¡å‹ç¤ºä¾‹\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # 3.1 åŸºç¡€HuggingFaceåµŒå…¥\n",
    "    print(\"\\n3.1 åŸºç¡€HuggingFaceåµŒå…¥\")\n",
    "    try:\n",
    "        # ä½¿ç”¨é¢„è®­ç»ƒçš„sentence-transformersæ¨¡å‹\n",
    "        embeddings = HuggingFaceEmbeddings(\n",
    "            model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "            model_kwargs={'device': 'cpu'},  # æˆ– 'cuda' å¦‚æœæœ‰GPU\n",
    "            encode_kwargs={'normalize_embeddings': True}  # æ ‡å‡†åŒ–åµŒå…¥\n",
    "        )\n",
    "\n",
    "        texts = [\n",
    "            \"è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•æ–‡æ¡£\",\n",
    "            \"äººå·¥æ™ºèƒ½æŠ€æœ¯å‘å±•è¿…é€Ÿ\",\n",
    "            \"æœºå™¨å­¦ä¹ ç®—æ³•å¾ˆé‡è¦\"\n",
    "        ]\n",
    "\n",
    "        doc_embeddings = embeddings.embed_documents(texts)\n",
    "        query_embedding = embeddings.embed_query(\"AIæŠ€æœ¯\")\n",
    "\n",
    "        print(f\"HuggingFaceåµŒå…¥ç»´åº¦: {len(doc_embeddings[0])}\")\n",
    "\n",
    "        # 3.2 ä¸­æ–‡ä¼˜åŒ–æ¨¡å‹\n",
    "        print(\"\\n3.2 ä¸­æ–‡ä¼˜åŒ–åµŒå…¥æ¨¡å‹\")\n",
    "        chinese_models = [\n",
    "            \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\",\n",
    "            \"sentence-transformers/distiluse-base-multilingual-cased\",\n",
    "            \"BAAI/bge-small-zh-v1.5\"  # ä¸­æ–‡ä¼˜åŒ–æ¨¡å‹\n",
    "        ]\n",
    "\n",
    "        for model_name in chinese_models:\n",
    "            try:\n",
    "                chinese_embeddings = HuggingFaceEmbeddings(\n",
    "                    model_name=model_name,\n",
    "                    model_kwargs={'device': 'cpu'}\n",
    "                )\n",
    "                test_embedding = chinese_embeddings.embed_query(\"ä¸­æ–‡æµ‹è¯•\")\n",
    "                print(f\"{model_name}: ç»´åº¦ {len(test_embedding)}\")\n",
    "            except Exception as e:\n",
    "                print(f\"{model_name}: åŠ è½½å¤±è´¥ ({str(e)[:50]}...)\")\n",
    "\n",
    "        # 3.3 æŒ‡ä»¤ä¼˜åŒ–åµŒå…¥\n",
    "        print(\"\\n3.3 æŒ‡ä»¤ä¼˜åŒ–åµŒå…¥æ¨¡å‹\")\n",
    "        try:\n",
    "            instruct_embeddings = HuggingFaceInstructEmbeddings(\n",
    "                model_name=\"hkunlp/instructor-xl\",\n",
    "                model_kwargs={'device': 'cpu'}\n",
    "            )\n",
    "\n",
    "            # ä½¿ç”¨æŒ‡ä»¤å‰ç¼€\n",
    "            query_instruction = \"ä¸ºè¿™ä¸ªæŸ¥è¯¢æ‰¾åˆ°æœ€ç›¸å…³çš„æ–‡æ¡£: \"\n",
    "            doc_instruction = \"è¿™æ˜¯ä¸€ä¸ªå…³äºæŠ€æœ¯çš„æ–‡æ¡£: \"\n",
    "\n",
    "            instruct_query = instruct_embeddings.embed_query(\n",
    "                query_instruction + \"äººå·¥æ™ºèƒ½åº”ç”¨\"\n",
    "            )\n",
    "            print(f\"æŒ‡ä»¤åµŒå…¥ç»´åº¦: {len(instruct_query)}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"æŒ‡ä»¤åµŒå…¥æ¨¡å‹åŠ è½½å¤±è´¥: {e}\")\n",
    "\n",
    "        return embeddings\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"HuggingFaceåµŒå…¥æ¨¡å‹å¤±è´¥: {e}\")\n",
    "        return None"
   ],
   "id": "25563679504a09f7",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### SentenceTransformersåµŒå…¥æ¨¡å‹ç¤ºä¾‹",
   "id": "fa655f44fae250e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "def sentence_transformers_example():\n",
    "    \"\"\"SentenceTransformersåµŒå…¥æ¨¡å‹ç¤ºä¾‹\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"4. SentenceTransformersåµŒå…¥æ¨¡å‹ç¤ºä¾‹\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    try:\n",
    "        # 4.1 å¤šè¯­è¨€æ¨¡å‹\n",
    "        print(\"\\n4.1 å¤šè¯­è¨€SentenceTransformers\")\n",
    "        multilingual_embeddings = SentenceTransformerEmbeddings(\n",
    "            model_name=\"sentence-transformers/paraphrase-multilingual-mpnet-base-v2\"\n",
    "        )\n",
    "\n",
    "        # å¤šè¯­è¨€æµ‹è¯•\n",
    "        multilingual_texts = [\n",
    "            \"Hello, how are you?\",\n",
    "            \"ä½ å¥½ï¼Œä½ å¥½å—ï¼Ÿ\",\n",
    "            \"Hola, Â¿cÃ³mo estÃ¡s?\",\n",
    "            \"Bonjour, comment allez-vous?\"\n",
    "        ]\n",
    "\n",
    "        multi_embeddings = multilingual_embeddings.embed_documents(multilingual_texts)\n",
    "        print(f\"å¤šè¯­è¨€åµŒå…¥ç»´åº¦: {len(multi_embeddings[0])}\")\n",
    "\n",
    "        # è®¡ç®—è·¨è¯­è¨€ç›¸ä¼¼åº¦\n",
    "        english_query = multilingual_embeddings.embed_query(\"greeting\")\n",
    "        chinese_query = multilingual_embeddings.embed_query(\"é—®å€™\")\n",
    "\n",
    "        def cosine_similarity(a, b):\n",
    "            return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "\n",
    "        cross_lang_similarity = cosine_similarity(english_query, chinese_query)\n",
    "        print(f\"è·¨è¯­è¨€ç›¸ä¼¼åº¦ (greeting vs é—®å€™): {cross_lang_similarity:.4f}\")\n",
    "\n",
    "        # 4.2 ä¸“ä¸šé¢†åŸŸæ¨¡å‹\n",
    "        print(\"\\n4.2 ä¸“ä¸šé¢†åŸŸåµŒå…¥æ¨¡å‹\")\n",
    "        domain_models = [\n",
    "            \"sentence-transformers/all-mpnet-base-v2\",  # é€šç”¨\n",
    "            \"sentence-transformers/msmarco-distilbert-base-v4\",  # æœç´¢ä¼˜åŒ–\n",
    "            \"sentence-transformers/nli-mpnet-base-v2\"  # è‡ªç„¶è¯­è¨€æ¨ç†\n",
    "        ]\n",
    "\n",
    "        for model_name in domain_models:\n",
    "            try:\n",
    "                domain_embeddings = SentenceTransformerEmbeddings(model_name=model_name)\n",
    "                test_embedding = domain_embeddings.embed_query(\"domain test\")\n",
    "                print(f\"{model_name.split('/')[-1]}: ç»´åº¦ {len(test_embedding)}\")\n",
    "            except Exception as e:\n",
    "                print(f\"{model_name}: ä¸å¯ç”¨\")\n",
    "\n",
    "        return multilingual_embeddings\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"SentenceTransformerså¤±è´¥: {e}\")\n",
    "        return None"
   ],
   "id": "adcd5cf10c88c30d",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### äº‘ç«¯åµŒå…¥æ¨¡å‹ç¤ºä¾‹",
   "id": "649ec6c4cb0d4a91"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "def cloud_embeddings_example():\n",
    "    \"\"\"äº‘ç«¯åµŒå…¥æ¨¡å‹ç¤ºä¾‹\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"5. äº‘ç«¯åµŒå…¥æ¨¡å‹ç¤ºä¾‹\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # 5.1 CohereåµŒå…¥\n",
    "    print(\"\\n5.1 CohereåµŒå…¥æ¨¡å‹\")\n",
    "    try:\n",
    "        cohere_embeddings = CohereEmbeddings(\n",
    "            cohere_api_key=os.getenv(\"COHERE_API_KEY\"),\n",
    "            model=\"embed-english-v3.0\"  # æˆ– embed-multilingual-v3.0\n",
    "        )\n",
    "\n",
    "        cohere_texts = [\"AI technology\", \"Machine learning algorithms\"]\n",
    "        cohere_embeds = cohere_embeddings.embed_documents(cohere_texts)\n",
    "        print(f\"CohereåµŒå…¥ç»´åº¦: {len(cohere_embeds[0])}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"CohereåµŒå…¥å¤±è´¥: {e}\")\n",
    "\n",
    "    # 5.2 AWS BedrockåµŒå…¥\n",
    "    print(\"\\n5.2 AWS BedrockåµŒå…¥æ¨¡å‹\")\n",
    "    try:\n",
    "        bedrock_embeddings = BedrockEmbeddings(\n",
    "            credentials_profile_name=\"default\",\n",
    "            region_name=\"us-east-1\",\n",
    "            model_id=\"amazon.titan-embed-text-v1\"\n",
    "        )\n",
    "\n",
    "        bedrock_texts = [\"Cloud computing\", \"Serverless architecture\"]\n",
    "        bedrock_embeds = bedrock_embeddings.embed_documents(bedrock_texts)\n",
    "        print(f\"BedrockåµŒå…¥ç»´åº¦: {len(bedrock_embeds[0])}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"BedrockåµŒå…¥å¤±è´¥: {e}\")"
   ],
   "id": "c659a74a35c242d7",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### åµŒå…¥æ¨¡å‹æ€§èƒ½å¯¹æ¯”",
   "id": "cf847864f2143d27"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "def embedding_performance_comparison():\n",
    "    \"\"\"åµŒå…¥æ¨¡å‹æ€§èƒ½å¯¹æ¯”\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"6. åµŒå…¥æ¨¡å‹æ€§èƒ½å¯¹æ¯”\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # æµ‹è¯•æ–‡æœ¬\n",
    "    test_texts = [\n",
    "        \"äººå·¥æ™ºèƒ½æŠ€æœ¯æ­£åœ¨å¿«é€Ÿå‘å±•\",\n",
    "        \"æœºå™¨å­¦ä¹ ç®—æ³•åœ¨å„ä¸ªé¢†åŸŸéƒ½æœ‰åº”ç”¨\",\n",
    "        \"æ·±åº¦å­¦ä¹ æ¨¡å‹éœ€è¦å¤§é‡çš„è®­ç»ƒæ•°æ®\",\n",
    "        \"è‡ªç„¶è¯­è¨€å¤„ç†è®©è®¡ç®—æœºç†è§£äººç±»è¯­è¨€\",\n",
    "        \"è®¡ç®—æœºè§†è§‰æŠ€æœ¯å¯ä»¥è¯†åˆ«å›¾åƒä¸­çš„ç‰©ä½“\"\n",
    "    ]\n",
    "\n",
    "    test_query = \"AIæŠ€æœ¯çš„åº”ç”¨é¢†åŸŸ\"\n",
    "\n",
    "    # å®šä¹‰è¦æµ‹è¯•çš„æ¨¡å‹\n",
    "    models_to_test = []\n",
    "\n",
    "    # Ollamaæ¨¡å‹\n",
    "    try:\n",
    "        ollama_model = OllamaEmbeddings(\n",
    "            base_url=\"http://localhost:11434\",\n",
    "            model=\"nomic-embed-text\"\n",
    "        )\n",
    "        models_to_test.append((\"Ollama-nomic\", ollama_model))\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # HuggingFaceæ¨¡å‹\n",
    "    try:\n",
    "        hf_model = HuggingFaceEmbeddings(\n",
    "            model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "        )\n",
    "        models_to_test.append((\"HF-MiniLM\", hf_model))\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # æ€§èƒ½æµ‹è¯•\n",
    "    results = {}\n",
    "\n",
    "    for model_name, model in models_to_test:\n",
    "        try:\n",
    "            print(f\"\\næµ‹è¯• {model_name}...\")\n",
    "\n",
    "            # æµ‹è¯•æ–‡æ¡£åµŒå…¥æ—¶é—´\n",
    "            start_time = time.time()\n",
    "            doc_embeddings = model.embed_documents(test_texts)\n",
    "            doc_time = time.time() - start_time\n",
    "\n",
    "            # æµ‹è¯•æŸ¥è¯¢åµŒå…¥æ—¶é—´\n",
    "            start_time = time.time()\n",
    "            query_embedding = model.embed_query(test_query)\n",
    "            query_time = time.time() - start_time\n",
    "\n",
    "            # è®¡ç®—ç›¸ä¼¼åº¦\n",
    "            similarities = []\n",
    "            for doc_emb in doc_embeddings:\n",
    "                sim = np.dot(query_embedding, doc_emb) / (\n",
    "                    np.linalg.norm(query_embedding) * np.linalg.norm(doc_emb)\n",
    "                )\n",
    "                similarities.append(sim)\n",
    "\n",
    "            results[model_name] = {\n",
    "                \"dimension\": len(doc_embeddings[0]),\n",
    "                \"doc_time\": doc_time,\n",
    "                \"query_time\": query_time,\n",
    "                \"avg_similarity\": np.mean(similarities),\n",
    "                \"max_similarity\": np.max(similarities)\n",
    "            }\n",
    "\n",
    "        except Exception as e:\n",
    "            results[model_name] = {\"error\": str(e)}\n",
    "\n",
    "    # æ˜¾ç¤ºç»“æœ\n",
    "    print(\"\\næ€§èƒ½å¯¹æ¯”ç»“æœ:\")\n",
    "    print(f\"{'æ¨¡å‹':<15} {'ç»´åº¦':<8} {'æ–‡æ¡£æ—¶é—´(s)':<12} {'æŸ¥è¯¢æ—¶é—´(s)':<12} {'å¹³å‡ç›¸ä¼¼åº¦':<12} {'æœ€é«˜ç›¸ä¼¼åº¦':<12}\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "    for model_name, result in results.items():\n",
    "        if \"error\" not in result:\n",
    "            print(f\"{model_name:<15} {result['dimension']:<8} {result['doc_time']:<12.4f} \"\n",
    "                  f\"{result['query_time']:<12.4f} {result['avg_similarity']:<12.4f} \"\n",
    "                  f\"{result['max_similarity']:<12.4f}\")\n",
    "        else:\n",
    "            print(f\"{model_name:<15} é”™è¯¯: {result['error'][:50]}...\")"
   ],
   "id": "6a3420fb4bd3fe3c",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### é«˜çº§åµŒå…¥æŠ€æœ¯",
   "id": "9e7696d32b7f474b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "def advanced_embedding_techniques():\n",
    "    \"\"\"é«˜çº§åµŒå…¥æŠ€æœ¯\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"7. é«˜çº§åµŒå…¥æŠ€æœ¯\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # 7.1 åµŒå…¥ç¼“å­˜\n",
    "    print(\"\\n7.1 åµŒå…¥ç¼“å­˜æœºåˆ¶\")\n",
    "\n",
    "    class CachedEmbeddings:\n",
    "        \"\"\"å¸¦ç¼“å­˜çš„åµŒå…¥æ¨¡å‹\"\"\"\n",
    "\n",
    "        def __init__(self, base_embeddings):\n",
    "            self.base_embeddings = base_embeddings\n",
    "            self.cache = {}\n",
    "\n",
    "        def embed_query(self, text: str) -> List[float]:\n",
    "            if text in self.cache:\n",
    "                print(f\"ç¼“å­˜å‘½ä¸­: {text[:30]}...\")\n",
    "                return self.cache[text]\n",
    "\n",
    "            embedding = self.base_embeddings.embed_query(text)\n",
    "            self.cache[text] = embedding\n",
    "            print(f\"æ–°è®¡ç®—: {text[:30]}...\")\n",
    "            return embedding\n",
    "\n",
    "        def embed_documents(self, texts: List[str]) -> List[List[float]]:\n",
    "            embeddings = []\n",
    "            for text in texts:\n",
    "                embeddings.append(self.embed_query(text))\n",
    "            return embeddings\n",
    "\n",
    "    # ä½¿ç”¨ç¼“å­˜åµŒå…¥\n",
    "    try:\n",
    "        base_model = HuggingFaceEmbeddings(\n",
    "            model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "        )\n",
    "        cached_model = CachedEmbeddings(base_model)\n",
    "\n",
    "        # ç¬¬ä¸€æ¬¡è®¡ç®—\n",
    "        test_texts = [\"AIæŠ€æœ¯\", \"æœºå™¨å­¦ä¹ \", \"AIæŠ€æœ¯\"]  # é‡å¤æ–‡æœ¬\n",
    "        embeddings1 = cached_model.embed_documents(test_texts)\n",
    "\n",
    "        # ç¬¬äºŒæ¬¡è®¡ç®—ï¼ˆåº”è¯¥ä½¿ç”¨ç¼“å­˜ï¼‰\n",
    "        embeddings2 = cached_model.embed_documents(test_texts)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"ç¼“å­˜åµŒå…¥ç¤ºä¾‹å¤±è´¥: {e}\")\n",
    "\n",
    "    # 7.2 æ‰¹é‡å¤„ç†ä¼˜åŒ–\n",
    "    print(\"\\n7.2 æ‰¹é‡å¤„ç†ä¼˜åŒ–\")\n",
    "\n",
    "    def batch_embed_documents(embeddings_model, texts: List[str], batch_size: int = 32):\n",
    "        \"\"\"æ‰¹é‡å¤„ç†åµŒå…¥\"\"\"\n",
    "        all_embeddings = []\n",
    "\n",
    "        for i in range(0, len(texts), batch_size):\n",
    "            batch = texts[i:i + batch_size]\n",
    "            print(f\"å¤„ç†æ‰¹æ¬¡ {i//batch_size + 1}: {len(batch)} ä¸ªæ–‡æ¡£\")\n",
    "\n",
    "            batch_embeddings = embeddings_model.embed_documents(batch)\n",
    "            all_embeddings.extend(batch_embeddings)\n",
    "\n",
    "        return all_embeddings\n",
    "\n",
    "    # 7.3 å¼‚æ­¥åµŒå…¥å¤„ç†\n",
    "    print(\"\\n7.3 å¼‚æ­¥åµŒå…¥å¤„ç†\")\n",
    "\n",
    "    async def async_embed_documents(embeddings_model, texts: List[str]):\n",
    "        \"\"\"å¼‚æ­¥å¤„ç†åµŒå…¥\"\"\"\n",
    "        loop = asyncio.get_event_loop()\n",
    "\n",
    "        # å°†æ–‡æœ¬åˆ†ç»„\n",
    "        chunk_size = len(texts) // 4 + 1\n",
    "        tasks = []\n",
    "\n",
    "        for i in range(0, len(texts), chunk_size):\n",
    "            chunk = texts[i:i + chunk_size]\n",
    "            task = loop.run_in_executor(\n",
    "                None,\n",
    "                embeddings_model.embed_documents,\n",
    "                chunk\n",
    "            )\n",
    "            tasks.append(task)\n",
    "\n",
    "        # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ\n",
    "        results = await asyncio.gather(*tasks)\n",
    "\n",
    "        # åˆå¹¶ç»“æœ\n",
    "        all_embeddings = []\n",
    "        for result in results:\n",
    "            all_embeddings.extend(result)\n",
    "\n",
    "        return all_embeddings"
   ],
   "id": "d6105b1dabd48ae7",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### åµŒå…¥è´¨é‡è¯„ä¼°",
   "id": "c6cdfe35150caa2b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "def embedding_quality_evaluation():\n",
    "    \"\"\"åµŒå…¥è´¨é‡è¯„ä¼°\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"8. åµŒå…¥è´¨é‡è¯„ä¼°\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # 8.1 è¯­ä¹‰ç›¸ä¼¼åº¦æµ‹è¯•\n",
    "    print(\"\\n8.1 è¯­ä¹‰ç›¸ä¼¼åº¦æµ‹è¯•\")\n",
    "\n",
    "    # å®šä¹‰æµ‹è¯•ç”¨ä¾‹\n",
    "    similarity_tests = [\n",
    "        (\"äººå·¥æ™ºèƒ½\", \"AIæŠ€æœ¯\", \"é«˜ç›¸ä¼¼åº¦\"),\n",
    "        (\"æœºå™¨å­¦ä¹ \", \"æ·±åº¦å­¦ä¹ \", \"ä¸­ç­‰ç›¸ä¼¼åº¦\"),\n",
    "        (\"è®¡ç®—æœº\", \"è‹¹æœ\", \"ä½ç›¸ä¼¼åº¦\"),\n",
    "        (\"ç‹—\", \"çŒ«\", \"ä¸­ç­‰ç›¸ä¼¼åº¦\"),\n",
    "        (\"æ±½è½¦\", \"é£æœº\", \"ä½ç›¸ä¼¼åº¦\")\n",
    "    ]\n",
    "\n",
    "    try:\n",
    "        embeddings = HuggingFaceEmbeddings(\n",
    "            model_name=\"sentence-transformers/paraphrase-multilingual-mpnet-base-v2\"\n",
    "        )\n",
    "\n",
    "        print(\"è¯­ä¹‰ç›¸ä¼¼åº¦æµ‹è¯•ç»“æœ:\")\n",
    "        for text1, text2, expected in similarity_tests:\n",
    "            emb1 = embeddings.embed_query(text1)\n",
    "            emb2 = embeddings.embed_query(text2)\n",
    "\n",
    "            similarity = np.dot(emb1, emb2) / (np.linalg.norm(emb1) * np.linalg.norm(emb2))\n",
    "            print(f\"{text1} vs {text2}: {similarity:.4f} ({expected})\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"è¯­ä¹‰ç›¸ä¼¼åº¦æµ‹è¯•å¤±è´¥: {e}\")\n",
    "\n",
    "    # 8.2 èšç±»è´¨é‡è¯„ä¼°\n",
    "    print(\"\\n8.2 èšç±»è´¨é‡è¯„ä¼°\")\n",
    "\n",
    "    def evaluate_clustering_quality(embeddings_model, texts: List[str], labels: List[str]):\n",
    "        \"\"\"è¯„ä¼°èšç±»è´¨é‡\"\"\"\n",
    "        try:\n",
    "            from sklearn.cluster import KMeans\n",
    "            from sklearn.metrics import adjusted_rand_score\n",
    "\n",
    "            # ç”ŸæˆåµŒå…¥\n",
    "            embeddings = embeddings_model.embed_documents(texts)\n",
    "            embeddings_array = np.array(embeddings)\n",
    "\n",
    "            # æ‰§è¡Œèšç±»\n",
    "            n_clusters = len(set(labels))\n",
    "            kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "            predicted_labels = kmeans.fit_predict(embeddings_array)\n",
    "\n",
    "            # è®¡ç®—è°ƒæ•´å…°å¾·æŒ‡æ•°\n",
    "            ari_score = adjusted_rand_score(labels, predicted_labels)\n",
    "            print(f\"èšç±»è´¨é‡ (ARI): {ari_score:.4f}\")\n",
    "\n",
    "            return ari_score\n",
    "\n",
    "        except ImportError:\n",
    "            print(\"éœ€è¦å®‰è£…scikit-learn: pip install scikit-learn\")\n",
    "        except Exception as e:\n",
    "            print(f\"èšç±»è¯„ä¼°å¤±è´¥: {e}\")"
   ],
   "id": "2cb39f33305df0a",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### è‡ªå®šä¹‰åµŒå…¥åŒ…è£…å™¨",
   "id": "2bda2842fef09cab"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "def custom_embedding_wrapper():\n",
    "    \"\"\"è‡ªå®šä¹‰åµŒå…¥åŒ…è£…å™¨\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"9. è‡ªå®šä¹‰åµŒå…¥åŒ…è£…å™¨\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    from langchain_core.embeddings import Embeddings\n",
    "\n",
    "    class MultiModelEmbeddings(Embeddings):\n",
    "        \"\"\"å¤šæ¨¡å‹é›†æˆåµŒå…¥\"\"\"\n",
    "\n",
    "        def __init__(self, models: List[Embeddings], weights: Optional[List[float]] = None):\n",
    "            self.models = models\n",
    "            self.weights = weights or [1.0] * len(models)\n",
    "\n",
    "            # æ ‡å‡†åŒ–æƒé‡\n",
    "            total_weight = sum(self.weights)\n",
    "            self.weights = [w / total_weight for w in self.weights]\n",
    "\n",
    "        def embed_documents(self, texts: List[str]) -> List[List[float]]:\n",
    "            \"\"\"é›†æˆå¤šä¸ªæ¨¡å‹çš„æ–‡æ¡£åµŒå…¥\"\"\"\n",
    "            all_embeddings = []\n",
    "\n",
    "            # è·å–æ¯ä¸ªæ¨¡å‹çš„åµŒå…¥\n",
    "            model_embeddings = []\n",
    "            for model in self.models:\n",
    "                embeddings = model.embed_documents(texts)\n",
    "                model_embeddings.append(embeddings)\n",
    "\n",
    "            # åŠ æƒå¹³å‡\n",
    "            for i in range(len(texts)):\n",
    "                combined_embedding = np.zeros(len(model_embeddings[0][i]))\n",
    "\n",
    "                for j, (embeddings, weight) in enumerate(zip(model_embeddings, self.weights)):\n",
    "                    combined_embedding += np.array(embeddings[i]) * weight\n",
    "\n",
    "                all_embeddings.append(combined_embedding.tolist())\n",
    "\n",
    "            return all_embeddings\n",
    "\n",
    "        def embed_query(self, text: str) -> List[float]:\n",
    "            \"\"\"é›†æˆå¤šä¸ªæ¨¡å‹çš„æŸ¥è¯¢åµŒå…¥\"\"\"\n",
    "            embeddings = self.embed_documents([text])\n",
    "            return embeddings[0]\n",
    "\n",
    "    # ä½¿ç”¨ç¤ºä¾‹\n",
    "    try:\n",
    "        # åˆ›å»ºå¤šä¸ªåŸºç¡€æ¨¡å‹\n",
    "        model1 = HuggingFaceEmbeddings(\n",
    "            model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "        )\n",
    "\n",
    "        # å¦‚æœæœ‰å¤šä¸ªæ¨¡å‹å¯ç”¨\n",
    "        models = [model1]  # å¯ä»¥æ·»åŠ æ›´å¤šæ¨¡å‹\n",
    "        weights = [1.0]    # å¯¹åº”çš„æƒé‡\n",
    "\n",
    "        multi_embeddings = MultiModelEmbeddings(models, weights)\n",
    "\n",
    "        test_text = \"å¤šæ¨¡å‹åµŒå…¥æµ‹è¯•\"\n",
    "        result = multi_embeddings.embed_query(test_text)\n",
    "        print(f\"å¤šæ¨¡å‹åµŒå…¥ç»´åº¦: {len(result)}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"å¤šæ¨¡å‹åµŒå…¥å¤±è´¥: {e}\")"
   ],
   "id": "d8d2f141a929d7d",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "def main():\n",
    "    \"\"\"è¿è¡Œæ‰€æœ‰åµŒå…¥æ¨¡å‹ç¤ºä¾‹\"\"\"\n",
    "    print(\"ğŸš€ LangChain 0.3 Embedding Models å®Œæ•´ç¤ºä¾‹\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    # è¿è¡Œæ‰€æœ‰ç¤ºä¾‹\n",
    "    ollama_embeddings = ollama_embeddings_example()\n",
    "    openai_embeddings = openai_embeddings_example()\n",
    "    hf_embeddings = huggingface_embeddings_example()\n",
    "    st_embeddings = sentence_transformers_example()\n",
    "    cloud_embeddings_example()\n",
    "    embedding_performance_comparison()\n",
    "    advanced_embedding_techniques()\n",
    "    embedding_quality_evaluation()\n",
    "    custom_embedding_wrapper()\n",
    "\n",
    "    print(\"\\nğŸ‰ æ‰€æœ‰åµŒå…¥æ¨¡å‹ç¤ºä¾‹è¿è¡Œå®Œæˆï¼\")\n",
    "\n",
    "    # æœ€ä½³å®è·µå»ºè®®\n",
    "    print(\"\\nğŸ“‹ åµŒå…¥æ¨¡å‹é€‰æ‹©å»ºè®®:\")\n",
    "    print(\"1. æœ¬åœ°éƒ¨ç½²ï¼šOllama + nomic-embed-text\")\n",
    "    print(\"2. äº‘ç«¯æœåŠ¡ï¼šOpenAI text-embedding-3-small\")\n",
    "    print(\"3. å¼€æºæ–¹æ¡ˆï¼šHuggingFace sentence-transformers\")\n",
    "    print(\"4. ä¸­æ–‡ä¼˜åŒ–ï¼šBAAI/bge-small-zh-v1.5\")\n",
    "    print(\"5. å¤šè¯­è¨€ï¼šparaphrase-multilingual-mpnet-base-v2\")\n",
    "    print(\"6. é«˜æ€§èƒ½ï¼štext-embedding-3-large\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "id": "9fa80966af27d1d4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T06:54:26.718459Z",
     "start_time": "2025-07-23T06:54:26.199599Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 3. Embedding Models ç¤ºä¾‹\n",
    "def embedding_models_example():\n",
    "    \"\"\"åµŒå…¥æ¨¡å‹ç¤ºä¾‹\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"3. Embedding Models åµŒå…¥æ¨¡å‹ç¤ºä¾‹\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # 3.1 OllamaåµŒå…¥æ¨¡å‹\n",
    "    print(\"\\n3.1 OllamaåµŒå…¥æ¨¡å‹\")\n",
    "    try:\n",
    "        embeddings = OllamaEmbeddings(\n",
    "            base_url=\"http://localhost:11434\",\n",
    "            model=\"nomic-embed-text\"  # æˆ–ä½¿ç”¨å…¶ä»–åµŒå…¥æ¨¡å‹\n",
    "        )\n",
    "\n",
    "        # æµ‹è¯•æ–‡æœ¬\n",
    "        texts = [\n",
    "            \"äººå·¥æ™ºèƒ½æ˜¯è®¡ç®—æœºç§‘å­¦çš„åˆ†æ”¯\",\n",
    "            \"æœºå™¨å­¦ä¹ æ˜¯AIçš„å­é›†\",\n",
    "            \"æ·±åº¦å­¦ä¹ ä½¿ç”¨ç¥ç»ç½‘ç»œ\",\n",
    "            \"ä»Šå¤©å¤©æ°”å¾ˆå¥½\"\n",
    "        ]\n",
    "\n",
    "        # ç”ŸæˆåµŒå…¥å‘é‡\n",
    "        text_embeddings = embeddings.embed_documents(texts)\n",
    "        query_embedding = embeddings.embed_query(\"ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿ\")\n",
    "\n",
    "        print(f\"æ–‡æ¡£åµŒå…¥æ•°é‡: {len(text_embeddings)}\")\n",
    "        print(f\"åµŒå…¥å‘é‡ç»´åº¦: {len(text_embeddings[0])}\")\n",
    "        print(f\"æŸ¥è¯¢åµŒå…¥ç»´åº¦: {len(query_embedding)}\")\n",
    "\n",
    "        # è®¡ç®—ç›¸ä¼¼åº¦\n",
    "        import numpy as np\n",
    "\n",
    "        def cosine_similarity(a, b):\n",
    "            return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "\n",
    "        print(\"\\nç›¸ä¼¼åº¦è®¡ç®—:\")\n",
    "        for i, text in enumerate(texts):\n",
    "            similarity = cosine_similarity(query_embedding, text_embeddings[i])\n",
    "            print(f\"'{text}' ç›¸ä¼¼åº¦: {similarity:.4f}\")\n",
    "\n",
    "        return embeddings\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"OllamaåµŒå…¥æ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {e}\")\n",
    "        print(\"è¯·ç¡®ä¿OllamaæœåŠ¡æ­£åœ¨è¿è¡Œå¹¶å®‰è£…äº†åµŒå…¥æ¨¡å‹\")\n",
    "        return None\n",
    "# 3. åµŒå…¥æ¨¡å‹\n",
    "embeddings = embedding_models_example()"
   ],
   "id": "95925b2d3a7b8cd2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "3. Embedding Models åµŒå…¥æ¨¡å‹ç¤ºä¾‹\n",
      "============================================================\n",
      "\n",
      "3.1 OllamaåµŒå…¥æ¨¡å‹\n",
      "æ–‡æ¡£åµŒå…¥æ•°é‡: 4\n",
      "åµŒå…¥å‘é‡ç»´åº¦: 768\n",
      "æŸ¥è¯¢åµŒå…¥ç»´åº¦: 768\n",
      "\n",
      "ç›¸ä¼¼åº¦è®¡ç®—:\n",
      "'äººå·¥æ™ºèƒ½æ˜¯è®¡ç®—æœºç§‘å­¦çš„åˆ†æ”¯' ç›¸ä¼¼åº¦: 0.8551\n",
      "'æœºå™¨å­¦ä¹ æ˜¯AIçš„å­é›†' ç›¸ä¼¼åº¦: 0.6135\n",
      "'æ·±åº¦å­¦ä¹ ä½¿ç”¨ç¥ç»ç½‘ç»œ' ç›¸ä¼¼åº¦: 0.5818\n",
      "'ä»Šå¤©å¤©æ°”å¾ˆå¥½' ç›¸ä¼¼åº¦: 0.5851\n"
     ]
    }
   ],
   "execution_count": 42
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 4. Vector Stores ç¤ºä¾‹",
   "id": "8127affda365bc76"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### FAISSå‘é‡å­˜å‚¨",
   "id": "7bc71c31bc626a09"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T07:26:37.066513Z",
     "start_time": "2025-07-23T07:26:37.061484Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "LangChain 0.3 Vector Stores å®Œæ•´ç¤ºä¾‹\n",
    "åŒ…å«æ‰€æœ‰ä¸»è¦å‘é‡å­˜å‚¨å’Œé«˜çº§ç”¨æ³•\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import asyncio\n",
    "from typing import List, Dict, Any, Optional, Tuple\n",
    "import time\n",
    "import json\n",
    "\n",
    "# æ ¸å¿ƒå¯¼å…¥\n",
    "from langchain_core.documents import Document\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "# å‘é‡å­˜å‚¨å¯¼å…¥\n",
    "from langchain_community.vectorstores import (\n",
    "    FAISS,\n",
    "    Chroma,\n",
    "    Qdrant,\n",
    "    Pinecone,\n",
    "    Weaviate,\n",
    "    Milvus,\n",
    "    ElasticsearchStore,\n",
    "    Redis,\n",
    "    PGVector,\n",
    "    SupabaseVectorStore\n",
    ")"
   ],
   "id": "e7cc29d4cd56791",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T07:27:19.412142Z",
     "start_time": "2025-07-23T07:27:19.291241Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# åˆå§‹åŒ–åµŒå…¥æ¨¡å‹\n",
    "embeddings = OllamaEmbeddings(\n",
    "    base_url=\"http://localhost:11434\",\n",
    "    model=\"nomic-embed-text:latest\"\n",
    ")\n",
    "\n",
    "# å‡†å¤‡æ–‡æ¡£\n",
    "documents = [\n",
    "    Document(\n",
    "        page_content=\"äººå·¥æ™ºèƒ½æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªåˆ†æ”¯\",\n",
    "        metadata={\"source\": \"ai_intro.txt\", \"category\": \"technology\"}\n",
    "    )\n",
    "    # ... æ›´å¤šæ–‡æ¡£\n",
    "]\n",
    "\n",
    "# åˆ›å»ºFAISSå‘é‡å­˜å‚¨\n",
    "faiss_vectorstore = FAISS.from_documents(documents, embeddings)\n",
    "\n",
    "# ä¿å­˜ç´¢å¼•åˆ°æœ¬åœ°\n",
    "index_path = \"faiss_index\"\n",
    "faiss_vectorstore.save_local(index_path)\n",
    "\n",
    "# åŠ è½½å·²ä¿å­˜çš„ç´¢å¼•\n",
    "loaded_vectorstore = FAISS.load_local(\n",
    "    index_path,\n",
    "    embeddings,\n",
    "    allow_dangerous_deserialization=True\n",
    ")\n",
    "\n",
    "# åŸºç¡€ç›¸ä¼¼æ€§æœç´¢\n",
    "query = \"ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½æŠ€æœ¯ï¼Ÿ\"\n",
    "similar_docs = faiss_vectorstore.similarity_search(query, k=3)\n",
    "print(similar_docs)\n",
    "# # å¸¦åˆ†æ•°çš„ç›¸ä¼¼æ€§æœç´¢\n",
    "# similar_docs_with_scores = faiss_vectorstore.similarity_search_with_score(query, k=3)\n",
    "#\n",
    "# # # åŸºäºé˜ˆå€¼çš„æœç´¢\n",
    "# # threshold = 0.8\n",
    "# # similar_docs_threshold = faiss_vectorstore.similarity_search_with_score_threshold(\n",
    "# #     query,\n",
    "# #     score_threshold=threshold\n",
    "# # )\n",
    "#\n",
    "# # æ·»åŠ æ–°æ–‡æ¡£\n",
    "# new_documents = [\n",
    "#     Document(\n",
    "#         page_content=\"é‡å­è®¡ç®—æ˜¯ä¸€ç§åˆ©ç”¨é‡å­åŠ›å­¦åŸç†çš„è®¡ç®—æ–¹å¼\",\n",
    "#         metadata={\"source\": \"quantum.txt\", \"category\": \"technology\"}\n",
    "#     )\n",
    "# ]\n",
    "#\n",
    "# faiss_vectorstore.add_documents(new_documents)\n",
    "\n",
    "# # å‘é‡å­˜å‚¨åˆå¹¶\n",
    "# store1.merge_from(store2)\n",
    "#\n",
    "# # ä¸åŒç´¢å¼•ç±»å‹ï¼ˆé€‚åˆå¤§è§„æ¨¡æ•°æ®ï¼‰\n",
    "# faiss_ivf = FAISS.from_documents(\n",
    "#     documents,\n",
    "#     embeddings,\n",
    "#     index_type=\"IVF\",\n",
    "#     nlist=10  # èšç±»ä¸­å¿ƒæ•°é‡\n",
    "# )"
   ],
   "id": "3feab481e112dd99",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(id='6ad706ea-41d7-43a2-ab98-6798c1322bda', metadata={'source': 'ai_intro.txt', 'category': 'technology'}, page_content='äººå·¥æ™ºèƒ½æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªåˆ†æ”¯')]\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T07:03:56.273154Z",
     "start_time": "2025-07-23T07:03:53.944736Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "def faiss_vectorstore_example():\n",
    "    \"\"\"FAISSå‘é‡å­˜å‚¨è¯¦ç»†ç¤ºä¾‹\"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"1. FAISSå‘é‡å­˜å‚¨ç¤ºä¾‹\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    try:\n",
    "        # åˆå§‹åŒ–åµŒå…¥æ¨¡å‹\n",
    "        # embeddings = HuggingFaceEmbeddings(\n",
    "        #     model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "        # )\n",
    "        embeddings = OllamaEmbeddings(\n",
    "        base_url=\"http://localhost:11434\",\n",
    "        model=\"nomic-embed-text:latest\",  # æ¨èçš„åµŒå…¥æ¨¡å‹\n",
    "        )\n",
    "\n",
    "        # å‡†å¤‡æ–‡æ¡£\n",
    "        documents = [\n",
    "            Document(\n",
    "                page_content=\"äººå·¥æ™ºèƒ½æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œè‡´åŠ›äºåˆ›å»ºæ™ºèƒ½æœºå™¨\",\n",
    "                metadata={\"source\": \"ai_intro.txt\", \"category\": \"technology\", \"date\": \"2024-01-01\"}\n",
    "            ),\n",
    "            Document(\n",
    "                page_content=\"æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„å­é›†ï¼Œä½¿è®¡ç®—æœºèƒ½å¤Ÿä»æ•°æ®ä¸­å­¦ä¹ \",\n",
    "                metadata={\"source\": \"ml_basics.txt\", \"category\": \"technology\", \"date\": \"2024-01-02\"}\n",
    "            ),\n",
    "            Document(\n",
    "                page_content=\"æ·±åº¦å­¦ä¹ ä½¿ç”¨ç¥ç»ç½‘ç»œæ¥æ¨¡æ‹Ÿäººè„‘çš„å·¥ä½œæ–¹å¼\",\n",
    "                metadata={\"source\": \"dl_guide.txt\", \"category\": \"technology\", \"date\": \"2024-01-03\"}\n",
    "            ),\n",
    "            Document(\n",
    "                page_content=\"è‡ªç„¶è¯­è¨€å¤„ç†è®©è®¡ç®—æœºç†è§£å’Œç”Ÿæˆäººç±»è¯­è¨€\",\n",
    "                metadata={\"source\": \"nlp_overview.txt\", \"category\": \"technology\", \"date\": \"2024-01-04\"}\n",
    "            ),\n",
    "            Document(\n",
    "                page_content=\"è®¡ç®—æœºè§†è§‰æŠ€æœ¯å¯ä»¥è¯†åˆ«å’Œåˆ†æå›¾åƒä¸­çš„å†…å®¹\",\n",
    "                metadata={\"source\": \"cv_intro.txt\", \"category\": \"technology\", \"date\": \"2024-01-05\"}\n",
    "            ),\n",
    "            Document(\n",
    "                page_content=\"ä»Šå¤©å¤©æ°”å¾ˆå¥½ï¼Œé€‚åˆå‡ºé—¨æ•£æ­¥å’Œè¿åŠ¨\",\n",
    "                metadata={\"source\": \"weather.txt\", \"category\": \"daily\", \"date\": \"2024-01-06\"}\n",
    "            ),\n",
    "            Document(\n",
    "                page_content=\"è‚¡ç¥¨å¸‚åœºä»Šå¤©è¡¨ç°è‰¯å¥½ï¼Œç§‘æŠ€è‚¡é¢†æ¶¨\",\n",
    "                metadata={\"source\": \"market.txt\", \"category\": \"finance\", \"date\": \"2024-01-07\"}\n",
    "            )\n",
    "        ]\n",
    "\n",
    "        # 1.1 åŸºç¡€FAISSåˆ›å»º\n",
    "        print(\"\\n1.1 åŸºç¡€FAISSå‘é‡å­˜å‚¨åˆ›å»º\")\n",
    "        faiss_vectorstore = FAISS.from_documents(documents, embeddings)\n",
    "        print(f\"âœ… FAISSå‘é‡å­˜å‚¨åˆ›å»ºæˆåŠŸï¼ŒåŒ…å« {len(documents)} ä¸ªæ–‡æ¡£\")\n",
    "\n",
    "        # 1.2 ä¿å­˜å’ŒåŠ è½½\n",
    "        print(\"\\n1.2 FAISSç´¢å¼•ä¿å­˜å’ŒåŠ è½½\")\n",
    "        index_path = \"faiss_index\"\n",
    "        faiss_vectorstore.save_local(index_path)\n",
    "        print(f\"âœ… FAISSç´¢å¼•å·²ä¿å­˜åˆ° {index_path}\")\n",
    "\n",
    "        # åŠ è½½ç´¢å¼•\n",
    "        loaded_vectorstore = FAISS.load_local(\n",
    "            index_path,\n",
    "            embeddings,\n",
    "            allow_dangerous_deserialization=True\n",
    "        )\n",
    "        print(\"âœ… FAISSç´¢å¼•åŠ è½½æˆåŠŸ\")\n",
    "\n",
    "        # 1.3 åŸºç¡€ç›¸ä¼¼æ€§æœç´¢\n",
    "        print(\"\\n1.3 åŸºç¡€ç›¸ä¼¼æ€§æœç´¢\")\n",
    "        query = \"ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½æŠ€æœ¯ï¼Ÿ\"\n",
    "        similar_docs = faiss_vectorstore.similarity_search(query, k=3)\n",
    "\n",
    "        print(f\"æŸ¥è¯¢: '{query}'\")\n",
    "        print(\"æœ€ç›¸ä¼¼çš„æ–‡æ¡£:\")\n",
    "        for i, doc in enumerate(similar_docs):\n",
    "            print(f\"{i+1}. {doc.page_content}\")\n",
    "            print(f\"   å…ƒæ•°æ®: {doc.metadata}\")\n",
    "            print()\n",
    "\n",
    "        # 1.4 å¸¦åˆ†æ•°çš„ç›¸ä¼¼æ€§æœç´¢\n",
    "        print(\"\\n1.4 å¸¦åˆ†æ•°çš„ç›¸ä¼¼æ€§æœç´¢\")\n",
    "        similar_docs_with_scores = faiss_vectorstore.similarity_search_with_score(query, k=3)\n",
    "\n",
    "        print(\"å¸¦åˆ†æ•°çš„æœç´¢ç»“æœ:\")\n",
    "        for i, (doc, score) in enumerate(similar_docs_with_scores):\n",
    "            print(f\"{i+1}. åˆ†æ•°: {score:.4f}\")\n",
    "            print(f\"   å†…å®¹: {doc.page_content[:80]}...\")\n",
    "            print(f\"   æ¥æº: {doc.metadata.get('source', 'unknown')}\")\n",
    "            print()\n",
    "\n",
    "        # 1.5 åŸºäºé˜ˆå€¼çš„æœç´¢\n",
    "        print(\"\\n1.5 åŸºäºé˜ˆå€¼çš„ç›¸ä¼¼æ€§æœç´¢\")\n",
    "        threshold = 0.8\n",
    "        similar_docs_threshold = faiss_vectorstore.similarity_search_with_score_threshold(\n",
    "            query,\n",
    "            score_threshold=threshold\n",
    "        )\n",
    "\n",
    "        print(f\"é˜ˆå€¼ {threshold} ä»¥ä¸Šçš„æ–‡æ¡£:\")\n",
    "        for doc, score in similar_docs_threshold:\n",
    "            print(f\"åˆ†æ•°: {score:.4f} - {doc.page_content[:60]}...\")\n",
    "\n",
    "        # 1.6 æ·»åŠ æ–°æ–‡æ¡£\n",
    "        print(\"\\n1.6 æ·»åŠ æ–°æ–‡æ¡£\")\n",
    "        new_documents = [\n",
    "            Document(\n",
    "                page_content=\"é‡å­è®¡ç®—æ˜¯ä¸€ç§åˆ©ç”¨é‡å­åŠ›å­¦åŸç†çš„è®¡ç®—æ–¹å¼\",\n",
    "                metadata={\"source\": \"quantum.txt\", \"category\": \"technology\", \"date\": \"2024-01-08\"}\n",
    "            ),\n",
    "            Document(\n",
    "                page_content=\"åŒºå—é“¾æŠ€æœ¯æä¾›äº†å»ä¸­å¿ƒåŒ–çš„æ•°æ®å­˜å‚¨æ–¹æ¡ˆ\",\n",
    "                metadata={\"source\": \"blockchain.txt\", \"category\": \"technology\", \"date\": \"2024-01-09\"}\n",
    "            )\n",
    "        ]\n",
    "\n",
    "        # æ·»åŠ æ–‡æ¡£\n",
    "        faiss_vectorstore.add_documents(new_documents)\n",
    "        print(f\"âœ… å·²æ·»åŠ  {len(new_documents)} ä¸ªæ–°æ–‡æ¡£\")\n",
    "\n",
    "        # éªŒè¯æ·»åŠ ç»“æœ\n",
    "        new_query = \"é‡å­è®¡ç®—çš„åŸç†\"\n",
    "        new_results = faiss_vectorstore.similarity_search(new_query, k=2)\n",
    "        print(f\"æ–°æŸ¥è¯¢ '{new_query}' çš„ç»“æœ:\")\n",
    "        for doc in new_results:\n",
    "            print(f\"- {doc.page_content[:50]}...\")\n",
    "\n",
    "        # 1.7 ä¸åŒæœç´¢ç®—æ³•\n",
    "        print(\"\\n1.7 ä¸åŒFAISSæœç´¢ç®—æ³•\")\n",
    "\n",
    "        # ä½¿ç”¨ä¸åŒçš„ç´¢å¼•ç±»å‹\n",
    "        try:\n",
    "            # åˆ›å»ºIVFç´¢å¼•ï¼ˆé€‚åˆå¤§è§„æ¨¡æ•°æ®ï¼‰\n",
    "            faiss_ivf = FAISS.from_documents(\n",
    "                documents,\n",
    "                embeddings,\n",
    "                index_type=\"IVF\",\n",
    "                nlist=10  # èšç±»ä¸­å¿ƒæ•°é‡\n",
    "            )\n",
    "\n",
    "            ivf_results = faiss_ivf.similarity_search(\"æœºå™¨å­¦ä¹ ç®—æ³•\", k=2)\n",
    "            print(\"IVFç´¢å¼•æœç´¢ç»“æœ:\")\n",
    "            for doc in ivf_results:\n",
    "                print(f\"- {doc.page_content[:50]}...\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"IVFç´¢å¼•åˆ›å»ºå¤±è´¥: {e}\")\n",
    "\n",
    "        return faiss_vectorstore\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"FAISSç¤ºä¾‹å¤±è´¥: {e}\")\n",
    "        return None\n",
    "faiss_vectorstore_example()"
   ],
   "id": "d902c11974c014e9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "1. FAISSå‘é‡å­˜å‚¨ç¤ºä¾‹\n",
      "============================================================\n",
      "\n",
      "1.1 åŸºç¡€FAISSå‘é‡å­˜å‚¨åˆ›å»º\n",
      "âœ… FAISSå‘é‡å­˜å‚¨åˆ›å»ºæˆåŠŸï¼ŒåŒ…å« 7 ä¸ªæ–‡æ¡£\n",
      "\n",
      "1.2 FAISSç´¢å¼•ä¿å­˜å’ŒåŠ è½½\n",
      "âœ… FAISSç´¢å¼•å·²ä¿å­˜åˆ° faiss_index\n",
      "âœ… FAISSç´¢å¼•åŠ è½½æˆåŠŸ\n",
      "\n",
      "1.3 åŸºç¡€ç›¸ä¼¼æ€§æœç´¢\n",
      "æŸ¥è¯¢: 'ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½æŠ€æœ¯ï¼Ÿ'\n",
      "æœ€ç›¸ä¼¼çš„æ–‡æ¡£:\n",
      "1. äººå·¥æ™ºèƒ½æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œè‡´åŠ›äºåˆ›å»ºæ™ºèƒ½æœºå™¨\n",
      "   å…ƒæ•°æ®: {'source': 'ai_intro.txt', 'category': 'technology', 'date': '2024-01-01'}\n",
      "\n",
      "2. æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„å­é›†ï¼Œä½¿è®¡ç®—æœºèƒ½å¤Ÿä»æ•°æ®ä¸­å­¦ä¹ \n",
      "   å…ƒæ•°æ®: {'source': 'ml_basics.txt', 'category': 'technology', 'date': '2024-01-02'}\n",
      "\n",
      "3. è‡ªç„¶è¯­è¨€å¤„ç†è®©è®¡ç®—æœºç†è§£å’Œç”Ÿæˆäººç±»è¯­è¨€\n",
      "   å…ƒæ•°æ®: {'source': 'nlp_overview.txt', 'category': 'technology', 'date': '2024-01-04'}\n",
      "\n",
      "\n",
      "1.4 å¸¦åˆ†æ•°çš„ç›¸ä¼¼æ€§æœç´¢\n",
      "å¸¦åˆ†æ•°çš„æœç´¢ç»“æœ:\n",
      "1. åˆ†æ•°: 0.3241\n",
      "   å†…å®¹: äººå·¥æ™ºèƒ½æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œè‡´åŠ›äºåˆ›å»ºæ™ºèƒ½æœºå™¨...\n",
      "   æ¥æº: ai_intro.txt\n",
      "\n",
      "2. åˆ†æ•°: 0.3984\n",
      "   å†…å®¹: æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„å­é›†ï¼Œä½¿è®¡ç®—æœºèƒ½å¤Ÿä»æ•°æ®ä¸­å­¦ä¹ ...\n",
      "   æ¥æº: ml_basics.txt\n",
      "\n",
      "3. åˆ†æ•°: 0.6391\n",
      "   å†…å®¹: è‡ªç„¶è¯­è¨€å¤„ç†è®©è®¡ç®—æœºç†è§£å’Œç”Ÿæˆäººç±»è¯­è¨€...\n",
      "   æ¥æº: nlp_overview.txt\n",
      "\n",
      "\n",
      "1.5 åŸºäºé˜ˆå€¼çš„ç›¸ä¼¼æ€§æœç´¢\n",
      "FAISSç¤ºä¾‹å¤±è´¥: 'FAISS' object has no attribute 'similarity_search_with_score_threshold'\n"
     ]
    }
   ],
   "execution_count": 48
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "def chroma_vectorstore_example():\n",
    "    \"\"\"Chromaå‘é‡å­˜å‚¨è¯¦ç»†ç¤ºä¾‹\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"2. Chromaå‘é‡å­˜å‚¨ç¤ºä¾‹\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    try:\n",
    "        # åˆå§‹åŒ–åµŒå…¥æ¨¡å‹\n",
    "        embeddings = HuggingFaceEmbeddings(\n",
    "            model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "        )\n",
    "\n",
    "        # å‡†å¤‡æ–‡æ¡£\n",
    "        documents = [\n",
    "            Document(\n",
    "                page_content=\"Pythonæ˜¯ä¸€ç§é«˜çº§ç¼–ç¨‹è¯­è¨€ï¼Œè¯­æ³•ç®€æ´æ˜“è¯»\",\n",
    "                metadata={\"language\": \"python\", \"difficulty\": \"beginner\", \"topic\": \"programming\"}\n",
    "            ),\n",
    "            Document(\n",
    "                page_content=\"JavaScriptæ˜¯Webå¼€å‘çš„æ ¸å¿ƒè¯­è¨€ä¹‹ä¸€\",\n",
    "                metadata={\"language\": \"javascript\", \"difficulty\": \"intermediate\", \"topic\": \"web\"}\n",
    "            ),\n",
    "            Document(\n",
    "                page_content=\"Javaæ˜¯ä¸€ç§é¢å‘å¯¹è±¡çš„ç¼–ç¨‹è¯­è¨€ï¼Œå¹¿æ³›ç”¨äºä¼ä¸šå¼€å‘\",\n",
    "                metadata={\"language\": \"java\", \"difficulty\": \"intermediate\", \"topic\": \"enterprise\"}\n",
    "            ),\n",
    "            Document(\n",
    "                page_content=\"C++æ˜¯ä¸€ç§é«˜æ€§èƒ½çš„ç³»ç»Ÿç¼–ç¨‹è¯­è¨€\",\n",
    "                metadata={\"language\": \"cpp\", \"difficulty\": \"advanced\", \"topic\": \"system\"}\n",
    "            ),\n",
    "            Document(\n",
    "                page_content=\"Goè¯­è¨€æ˜¯Googleå¼€å‘çš„ç°ä»£ç¼–ç¨‹è¯­è¨€ï¼Œé€‚åˆå¹¶å‘ç¼–ç¨‹\",\n",
    "                metadata={\"language\": \"go\", \"difficulty\": \"intermediate\", \"topic\": \"concurrent\"}\n",
    "            )\n",
    "        ]\n",
    "\n",
    "        # 2.1 åŸºç¡€Chromaåˆ›å»º\n",
    "        print(\"\\n2.1 åŸºç¡€Chromaå‘é‡å­˜å‚¨åˆ›å»º\")\n",
    "        persist_directory = \"./chroma_db\"\n",
    "\n",
    "        chroma_vectorstore = Chroma.from_documents(\n",
    "            documents=documents,\n",
    "            embedding=embeddings,\n",
    "            persist_directory=persist_directory,\n",
    "            collection_name=\"programming_languages\"\n",
    "        )\n",
    "\n",
    "        print(f\"âœ… Chromaå‘é‡å­˜å‚¨åˆ›å»ºæˆåŠŸï¼ŒæŒä¹…åŒ–ç›®å½•: {persist_directory}\")\n",
    "\n",
    "        # 2.2 æŒä¹…åŒ–\n",
    "        print(\"\\n2.2 Chromaæ•°æ®æŒä¹…åŒ–\")\n",
    "        chroma_vectorstore.persist()\n",
    "        print(\"âœ… Chromaæ•°æ®å·²æŒä¹…åŒ–\")\n",
    "\n",
    "        # 2.3 åŸºç¡€æœç´¢\n",
    "        print(\"\\n2.3 åŸºç¡€ç›¸ä¼¼æ€§æœç´¢\")\n",
    "        query = \"é€‚åˆåˆå­¦è€…çš„ç¼–ç¨‹è¯­è¨€\"\n",
    "        results = chroma_vectorstore.similarity_search(query, k=3)\n",
    "\n",
    "        print(f\"æŸ¥è¯¢: '{query}'\")\n",
    "        for i, doc in enumerate(results):\n",
    "            print(f\"{i+1}. {doc.page_content}\")\n",
    "            print(f\"   å…ƒæ•°æ®: {doc.metadata}\")\n",
    "            print()\n",
    "\n",
    "        # 2.4 å…ƒæ•°æ®è¿‡æ»¤æœç´¢\n",
    "        print(\"\\n2.4 åŸºäºå…ƒæ•°æ®çš„è¿‡æ»¤æœç´¢\")\n",
    "\n",
    "        # æœç´¢åˆå­¦è€…éš¾åº¦çš„è¯­è¨€\n",
    "        beginner_results = chroma_vectorstore.similarity_search(\n",
    "            query=\"ç¼–ç¨‹è¯­è¨€\",\n",
    "            k=5,\n",
    "            filter={\"difficulty\": \"beginner\"}\n",
    "        )\n",
    "\n",
    "        print(\"åˆå­¦è€…éš¾åº¦çš„ç¼–ç¨‹è¯­è¨€:\")\n",
    "        for doc in beginner_results:\n",
    "            print(f\"- {doc.page_content}\")\n",
    "            print(f\"  éš¾åº¦: {doc.metadata['difficulty']}\")\n",
    "\n",
    "        # æœç´¢Webç›¸å…³çš„è¯­è¨€\n",
    "        web_results = chroma_vectorstore.similarity_search(\n",
    "            query=\"å¼€å‘è¯­è¨€\",\n",
    "            k=5,\n",
    "            filter={\"topic\": \"web\"}\n",
    "        )\n",
    "\n",
    "        print(\"\\nWebå¼€å‘ç›¸å…³çš„è¯­è¨€:\")\n",
    "        for doc in web_results:\n",
    "            print(f\"- {doc.page_content}\")\n",
    "            print(f\"  ä¸»é¢˜: {doc.metadata['topic']}\")\n",
    "\n",
    "        # 2.5 å¤æ‚è¿‡æ»¤æ¡ä»¶\n",
    "        print(\"\\n2.5 å¤æ‚è¿‡æ»¤æ¡ä»¶\")\n",
    "\n",
    "        # ä½¿ç”¨$inæ“ä½œç¬¦\n",
    "        intermediate_results = chroma_vectorstore.similarity_search(\n",
    "            query=\"ç¼–ç¨‹\",\n",
    "            k=5,\n",
    "            filter={\"difficulty\": {\"$in\": [\"intermediate\", \"advanced\"]}}\n",
    "        )\n",
    "\n",
    "        print(\"ä¸­çº§å’Œé«˜çº§éš¾åº¦çš„è¯­è¨€:\")\n",
    "        for doc in intermediate_results:\n",
    "            print(f\"- {doc.page_content}\")\n",
    "            print(f\"  éš¾åº¦: {doc.metadata['difficulty']}\")\n",
    "\n",
    "        # 2.6 è·å–é›†åˆä¿¡æ¯\n",
    "        print(\"\\n2.6 Chromaé›†åˆä¿¡æ¯\")\n",
    "        collection = chroma_vectorstore._collection\n",
    "        print(f\"é›†åˆåç§°: {collection.name}\")\n",
    "        print(f\"æ–‡æ¡£æ•°é‡: {collection.count()}\")\n",
    "\n",
    "        # 2.7 åˆ é™¤æ–‡æ¡£\n",
    "        print(\"\\n2.7 åˆ é™¤æ–‡æ¡£æ“ä½œ\")\n",
    "\n",
    "        # æ·»åŠ ä¸€ä¸ªä¸´æ—¶æ–‡æ¡£ç”¨äºåˆ é™¤æ¼”ç¤º\n",
    "        temp_doc = Document(\n",
    "            page_content=\"ä¸´æ—¶æµ‹è¯•æ–‡æ¡£ï¼Œå°†è¢«åˆ é™¤\",\n",
    "            metadata={\"temp\": True, \"id\": \"temp_doc_1\"}\n",
    "        )\n",
    "\n",
    "        doc_ids = chroma_vectorstore.add_documents([temp_doc])\n",
    "        print(f\"æ·»åŠ ä¸´æ—¶æ–‡æ¡£ï¼ŒID: {doc_ids}\")\n",
    "\n",
    "        # åˆ é™¤æ–‡æ¡£\n",
    "        if doc_ids:\n",
    "            chroma_vectorstore.delete(doc_ids)\n",
    "            print(\"âœ… ä¸´æ—¶æ–‡æ¡£å·²åˆ é™¤\")\n",
    "\n",
    "        # 2.8 ä»ç°æœ‰é›†åˆåŠ è½½\n",
    "        print(\"\\n2.8 ä»ç°æœ‰é›†åˆåŠ è½½\")\n",
    "\n",
    "        # åˆ›å»ºæ–°çš„Chromaå®ä¾‹è¿æ¥åˆ°ç°æœ‰é›†åˆ\n",
    "        existing_chroma = Chroma(\n",
    "            persist_directory=persist_directory,\n",
    "            embedding_function=embeddings,\n",
    "            collection_name=\"programming_languages\"\n",
    "        )\n",
    "\n",
    "        # éªŒè¯åŠ è½½æˆåŠŸ\n",
    "        test_results = existing_chroma.similarity_search(\"Python\", k=1)\n",
    "        print(f\"ä»ç°æœ‰é›†åˆåŠ è½½æˆåŠŸï¼Œæµ‹è¯•æœç´¢ç»“æœ: {len(test_results)} ä¸ªæ–‡æ¡£\")\n",
    "\n",
    "        return chroma_vectorstore\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Chromaç¤ºä¾‹å¤±è´¥: {e}\")\n",
    "        return None\n",
    "\n",
    "def qdrant_vectorstore_example():\n",
    "    \"\"\"Qdrantå‘é‡å­˜å‚¨ç¤ºä¾‹\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"3. Qdrantå‘é‡å­˜å‚¨ç¤ºä¾‹\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    try:\n",
    "        from qdrant_client import QdrantClient\n",
    "        from qdrant_client.models import Distance, VectorParams\n",
    "\n",
    "        # åˆå§‹åŒ–åµŒå…¥æ¨¡å‹\n",
    "        embeddings = HuggingFaceEmbeddings(\n",
    "            model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "        )\n",
    "\n",
    "        # 3.1 æœ¬åœ°Qdrantè®¾ç½®\n",
    "        print(\"\\n3.1 æœ¬åœ°Qdrantå‘é‡å­˜å‚¨\")\n",
    "\n",
    "        # åˆ›å»ºQdrantå®¢æˆ·ç«¯ï¼ˆå†…å­˜æ¨¡å¼ï¼‰\n",
    "        client = QdrantClient(\":memory:\")\n",
    "\n",
    "        # å‡†å¤‡æ–‡æ¡£\n",
    "        documents = [\n",
    "            Document(\n",
    "                page_content=\"æœºå™¨å­¦ä¹ ç®—æ³•å¯ä»¥åˆ†ä¸ºç›‘ç£å­¦ä¹ ã€æ— ç›‘ç£å­¦ä¹ å’Œå¼ºåŒ–å­¦ä¹ \",\n",
    "                metadata={\"category\": \"ml\", \"type\": \"algorithm\", \"level\": \"intermediate\"}\n",
    "            ),\n",
    "            Document(\n",
    "                page_content=\"ç›‘ç£å­¦ä¹ ä½¿ç”¨æ ‡è®°æ•°æ®è®­ç»ƒæ¨¡å‹è¿›è¡Œé¢„æµ‹\",\n",
    "                metadata={\"category\": \"ml\", \"type\": \"supervised\", \"level\": \"beginner\"}\n",
    "            ),\n",
    "            Document(\n",
    "                page_content=\"æ— ç›‘ç£å­¦ä¹ ä»æœªæ ‡è®°æ•°æ®ä¸­å‘ç°éšè—æ¨¡å¼\",\n",
    "                metadata={\"category\": \"ml\", \"type\": \"unsupervised\", \"level\": \"intermediate\"}\n",
    "            ),\n",
    "            Document(\n",
    "                page_content=\"å¼ºåŒ–å­¦ä¹ é€šè¿‡ä¸ç¯å¢ƒäº¤äº’æ¥å­¦ä¹ æœ€ä¼˜ç­–ç•¥\",\n",
    "                metadata={\"category\": \"ml\", \"type\": \"reinforcement\", \"level\": \"advanced\"}\n",
    "            ),\n",
    "            Document(\n",
    "                page_content=\"ç¥ç»ç½‘ç»œæ˜¯æ·±åº¦å­¦ä¹ çš„åŸºç¡€æ¶æ„\",\n",
    "                metadata={\"category\": \"dl\", \"type\": \"architecture\", \"level\": \"intermediate\"}\n",
    "            )\n",
    "        ]\n",
    "\n",
    "        # åˆ›å»ºQdrantå‘é‡å­˜å‚¨\n",
    "        collection_name = \"ml_knowledge\"\n",
    "        qdrant_vectorstore = Qdrant.from_documents(\n",
    "            documents,\n",
    "            embeddings,\n",
    "            client=client,\n",
    "            collection_name=collection_name,\n",
    "            force_recreate=True\n",
    "        )\n",
    "\n",
    "        print(f\"âœ… Qdrantå‘é‡å­˜å‚¨åˆ›å»ºæˆåŠŸï¼Œé›†åˆ: {collection_name}\")\n",
    "\n",
    "        # 3.2 åŸºç¡€æœç´¢\n",
    "        print(\"\\n3.2 Qdrantç›¸ä¼¼æ€§æœç´¢\")\n",
    "        query = \"ä»€ä¹ˆæ˜¯ç›‘ç£å­¦ä¹ ï¼Ÿ\"\n",
    "        results = qdrant_vectorstore.similarity_search(query, k=3)\n",
    "\n",
    "        print(f\"æŸ¥è¯¢: '{query}'\")\n",
    "        for i, doc in enumerate(results):\n",
    "            print(f\"{i+1}. {doc.page_content}\")\n",
    "            print(f\"   ç±»åˆ«: {doc.metadata.get('category')}\")\n",
    "            print(f\"   ç±»å‹: {doc.metadata.get('type')}\")\n",
    "            print()\n",
    "\n",
    "        # 3.3 å¸¦åˆ†æ•°æœç´¢\n",
    "        print(\"\\n3.3 Qdrantå¸¦åˆ†æ•°æœç´¢\")\n",
    "        results_with_scores = qdrant_vectorstore.similarity_search_with_score(query, k=3)\n",
    "\n",
    "        for i, (doc, score) in enumerate(results_with_scores):\n",
    "            print(f\"{i+1}. åˆ†æ•°: {score:.4f}\")\n",
    "            print(f\"   å†…å®¹: {doc.page_content[:60]}...\")\n",
    "            print()\n",
    "\n",
    "        # 3.4 è¿‡æ»¤æœç´¢\n",
    "        print(\"\\n3.4 Qdrantè¿‡æ»¤æœç´¢\")\n",
    "\n",
    "        # æœç´¢ç‰¹å®šç±»åˆ«\n",
    "        ml_results = qdrant_vectorstore.similarity_search(\n",
    "            query=\"å­¦ä¹ ç®—æ³•\",\n",
    "            k=5,\n",
    "            filter={\"category\": \"ml\"}\n",
    "        )\n",
    "\n",
    "        print(\"æœºå™¨å­¦ä¹ ç±»åˆ«çš„æ–‡æ¡£:\")\n",
    "        for doc in ml_results:\n",
    "            print(f\"- {doc.page_content[:50]}...\")\n",
    "            print(f\"  ç±»å‹: {doc.metadata.get('type')}\")\n",
    "\n",
    "        # 3.5 æ·»åŠ æ–°æ–‡æ¡£\n",
    "        print(\"\\n3.5 æ·»åŠ æ–°æ–‡æ¡£åˆ°Qdrant\")\n",
    "        new_docs = [\n",
    "            Document(\n",
    "                page_content=\"å·ç§¯ç¥ç»ç½‘ç»œç‰¹åˆ«é€‚åˆå›¾åƒå¤„ç†ä»»åŠ¡\",\n",
    "                metadata={\"category\": \"dl\", \"type\": \"cnn\", \"level\": \"intermediate\"}\n",
    "            ),\n",
    "            Document(\n",
    "                page_content=\"å¾ªç¯ç¥ç»ç½‘ç»œæ“…é•¿å¤„ç†åºåˆ—æ•°æ®\",\n",
    "                metadata={\"category\": \"dl\", \"type\": \"rnn\", \"level\": \"intermediate\"}\n",
    "            )\n",
    "        ]\n",
    "\n",
    "        qdrant_vectorstore.add_documents(new_docs)\n",
    "        print(f\"âœ… å·²æ·»åŠ  {len(new_docs)} ä¸ªæ–°æ–‡æ¡£\")\n",
    "\n",
    "        # éªŒè¯æ·»åŠ \n",
    "        cnn_results = qdrant_vectorstore.similarity_search(\"å·ç§¯ç¥ç»ç½‘ç»œ\", k=1)\n",
    "        print(f\"éªŒè¯æ·»åŠ : {cnn_results[0].page_content[:40]}...\")\n",
    "\n",
    "        return qdrant_vectorstore\n",
    "\n",
    "    except ImportError:\n",
    "        print(\"éœ€è¦å®‰è£…qdrant-client: pip install qdrant-client\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Qdrantç¤ºä¾‹å¤±è´¥: {e}\")\n",
    "        return None\n",
    "\n",
    "def pinecone_vectorstore_example():\n",
    "    \"\"\"Pineconeå‘é‡å­˜å‚¨ç¤ºä¾‹\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"4. Pineconeå‘é‡å­˜å‚¨ç¤ºä¾‹\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    try:\n",
    "        import pinecone\n",
    "\n",
    "        # 4.1 Pineconeåˆå§‹åŒ–\n",
    "        print(\"\\n4.1 Pineconeåˆå§‹åŒ–\")\n",
    "\n",
    "        api_key = os.getenv(\"PINECONE_API_KEY\")\n",
    "        if not api_key:\n",
    "            print(\"è¯·è®¾ç½®PINECONE_API_KEYç¯å¢ƒå˜é‡\")\n",
    "            return None\n",
    "\n",
    "        # åˆå§‹åŒ–Pinecone\n",
    "        pinecone.init(\n",
    "            api_key=api_key,\n",
    "            environment=os.getenv(\"PINECONE_ENV\", \"us-west1-gcp\")\n",
    "        )\n",
    "\n",
    "        # åˆå§‹åŒ–åµŒå…¥æ¨¡å‹\n",
    "        embeddings = OpenAIEmbeddings(\n",
    "            api_key=os.getenv(\"OPENAI_API_KEY\")\n",
    "        )\n",
    "\n",
    "        # å‡†å¤‡æ–‡æ¡£\n",
    "        documents = [\n",
    "            Document(\n",
    "                page_content=\"äº‘è®¡ç®—æä¾›æŒ‰éœ€è®¿é—®çš„è®¡ç®—èµ„æº\",\n",
    "                metadata={\"service\": \"cloud\", \"provider\": \"general\", \"type\": \"infrastructure\"}\n",
    "            ),\n",
    "            Document(\n",
    "                page_content=\"AWSæ˜¯äºšé©¬é€Šæä¾›çš„äº‘è®¡ç®—å¹³å°\",\n",
    "                metadata={\"service\": \"cloud\", \"provider\": \"aws\", \"type\": \"platform\"}\n",
    "            ),\n",
    "            Document(\n",
    "                page_content=\"Azureæ˜¯å¾®è½¯çš„äº‘è®¡ç®—æœåŠ¡\",\n",
    "                metadata={\"service\": \"cloud\", \"provider\": \"microsoft\", \"type\": \"platform\"}\n",
    "            ),\n",
    "            Document(\n",
    "                page_content=\"Google Cloud Platformæä¾›å„ç§äº‘æœåŠ¡\",\n",
    "                metadata={\"service\": \"cloud\", \"provider\": \"google\", \"type\": \"platform\"}\n",
    "            )\n",
    "        ]\n",
    "\n",
    "        # 4.2 åˆ›å»ºPineconeç´¢å¼•\n",
    "        print(\"\\n4.2 åˆ›å»ºPineconeç´¢å¼•\")\n",
    "\n",
    "        index_name = \"langchain-demo\"\n",
    "        dimension = 1536  # OpenAIåµŒå…¥ç»´åº¦\n",
    "\n",
    "        # æ£€æŸ¥ç´¢å¼•æ˜¯å¦å­˜åœ¨\n",
    "        if index_name not in pinecone.list_indexes():\n",
    "            pinecone.create_index(\n",
    "                name=index_name,\n",
    "                dimension=dimension,\n",
    "                metric=\"cosine\"\n",
    "            )\n",
    "            print(f\"âœ… åˆ›å»ºPineconeç´¢å¼•: {index_name}\")\n",
    "        else:\n",
    "            print(f\"âœ… ä½¿ç”¨ç°æœ‰Pineconeç´¢å¼•: {index_name}\")\n",
    "\n",
    "        # 4.3 åˆ›å»ºPineconeå‘é‡å­˜å‚¨\n",
    "        print(\"\\n4.3 åˆ›å»ºPineconeå‘é‡å­˜å‚¨\")\n",
    "\n",
    "        pinecone_vectorstore = Pinecone.from_documents(\n",
    "            documents,\n",
    "            embeddings,\n",
    "            index_name=index_name\n",
    "        )\n",
    "\n",
    "        print(\"âœ… Pineconeå‘é‡å­˜å‚¨åˆ›å»ºæˆåŠŸ\")\n",
    "\n",
    "        # 4.4 æœç´¢æµ‹è¯•\n",
    "        print(\"\\n4.4 Pineconeæœç´¢æµ‹è¯•\")\n",
    "\n",
    "        query = \"ä»€ä¹ˆæ˜¯äº‘è®¡ç®—å¹³å°ï¼Ÿ\"\n",
    "        results = pinecone_vectorstore.similarity_search(query, k=3)\n",
    "\n",
    "        print(f\"æŸ¥è¯¢: '{query}'\")\n",
    "        for i, doc in enumerate(results):\n",
    "            print(f\"{i+1}. {doc.page_content}\")\n",
    "            print(f\"   æä¾›å•†: {doc.metadata.get('provider')}\")\n",
    "            print()\n",
    "\n",
    "        # 4.5 å‘½åç©ºé—´ä½¿ç”¨\n",
    "        print(\"\\n4.5 Pineconeå‘½åç©ºé—´\")\n",
    "\n",
    "        # ä½¿ç”¨å‘½åç©ºé—´åˆ†ç¦»ä¸åŒç±»å‹çš„æ•°æ®\n",
    "        namespace_vectorstore = Pinecone.from_existing_index(\n",
    "            index_name=index_name,\n",
    "            embedding=embeddings,\n",
    "            namespace=\"cloud_services\"\n",
    "        )\n",
    "\n",
    "        # æ·»åŠ æ–‡æ¡£åˆ°ç‰¹å®šå‘½åç©ºé—´\n",
    "        namespace_docs = [\n",
    "            Document(\n",
    "                page_content=\"Kubernetesæ˜¯å®¹å™¨ç¼–æ’å¹³å°\",\n",
    "                metadata={\"type\": \"orchestration\", \"category\": \"devops\"}\n",
    "            )\n",
    "        ]\n",
    "\n",
    "        namespace_vectorstore.add_documents(namespace_docs)\n",
    "        print(\"âœ… æ–‡æ¡£å·²æ·»åŠ åˆ°å‘½åç©ºé—´\")\n",
    "\n",
    "        return pinecone_vectorstore\n",
    "\n",
    "    except ImportError:\n",
    "        print(\"éœ€è¦å®‰è£…pinecone-client: pip install pinecone-client\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Pineconeç¤ºä¾‹å¤±è´¥: {e}\")\n",
    "        return None\n",
    "\n",
    "def elasticsearch_vectorstore_example():\n",
    "    \"\"\"Elasticsearchå‘é‡å­˜å‚¨ç¤ºä¾‹\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"5. Elasticsearchå‘é‡å­˜å‚¨ç¤ºä¾‹\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    try:\n",
    "        from elasticsearch import Elasticsearch\n",
    "\n",
    "        # 5.1 Elasticsearchè¿æ¥\n",
    "        print(\"\\n5.1 Elasticsearchè¿æ¥\")\n",
    "\n",
    "        # è¿æ¥åˆ°æœ¬åœ°Elasticsearch\n",
    "        es_client = Elasticsearch(\n",
    "            [{\"host\": \"localhost\", \"port\": 9200}],\n",
    "            # å¦‚æœæœ‰è®¤è¯ï¼Œæ·»åŠ ä»¥ä¸‹é…ç½®\n",
    "            # http_auth=(\"username\", \"password\"),\n",
    "            # use_ssl=True,\n",
    "            # verify_certs=True\n",
    "        )\n",
    "\n",
    "        # æ£€æŸ¥è¿æ¥\n",
    "        if not es_client.ping():\n",
    "            print(\"æ— æ³•è¿æ¥åˆ°Elasticsearchï¼Œè¯·ç¡®ä¿æœåŠ¡æ­£åœ¨è¿è¡Œ\")\n",
    "            return None\n",
    "\n",
    "        print(\"âœ… Elasticsearchè¿æ¥æˆåŠŸ\")\n",
    "\n",
    "        # åˆå§‹åŒ–åµŒå…¥æ¨¡å‹\n",
    "        embeddings = HuggingFaceEmbeddings(\n",
    "            model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "        )\n",
    "\n",
    "        # å‡†å¤‡æ–‡æ¡£\n",
    "        documents = [\n",
    "            Document(\n",
    "                page_content=\"æœç´¢å¼•æ“æŠ€æœ¯æ˜¯ä¿¡æ¯æ£€ç´¢çš„æ ¸å¿ƒ\",\n",
    "                metadata={\"domain\": \"search\", \"complexity\": \"medium\", \"year\": 2024}\n",
    "            ),\n",
    "            Document(\n",
    "                page_content=\"å…¨æ–‡æœç´¢å¯ä»¥åœ¨å¤§é‡æ–‡æ¡£ä¸­å¿«é€Ÿæ‰¾åˆ°ç›¸å…³å†…å®¹\",\n",
    "                metadata={\"domain\": \"search\", \"complexity\": \"low\", \"year\": 2024}\n",
    "            ),\n",
    "            Document(\n",
    "                page_content=\"å€’æ’ç´¢å¼•æ˜¯æœç´¢å¼•æ“çš„åŸºç¡€æ•°æ®ç»“æ„\",\n",
    "                metadata={\"domain\": \"search\", \"complexity\": \"high\", \"year\": 2024}\n",
    "            ),\n",
    "            Document(\n",
    "                page_content=\"åˆ†å¸ƒå¼æœç´¢å¯ä»¥å¤„ç†å¤§è§„æ¨¡æ•°æ®é›†\",\n",
    "                metadata={\"domain\": \"distributed\", \"complexity\": \"high\", \"year\": 2024}\n",
    "            )\n",
    "        ]\n",
    "\n",
    "        # 5.2 åˆ›å»ºElasticsearchå‘é‡å­˜å‚¨\n",
    "        print(\"\\n5.2 åˆ›å»ºElasticsearchå‘é‡å­˜å‚¨\")\n",
    "\n",
    "        index_name = \"langchain_demo\"\n",
    "\n",
    "        es_vectorstore = ElasticsearchStore.from_documents(\n",
    "            documents,\n",
    "            embeddings,\n",
    "            es_connection=es_client,\n",
    "            index_name=index_name,\n",
    "            distance_strategy=\"COSINE\"\n",
    "        )\n",
    "\n",
    "        print(f\"âœ… Elasticsearchå‘é‡å­˜å‚¨åˆ›å»ºæˆåŠŸï¼Œç´¢å¼•: {index_name}\")\n",
    "\n",
    "        # 5.3 åŸºç¡€æœç´¢\n",
    "        print(\"\\n5.3 Elasticsearchæœç´¢\")\n",
    "\n",
    "        query = \"å¦‚ä½•å®ç°å¿«é€Ÿæœç´¢ï¼Ÿ\"\n",
    "        results = es_vectorstore.similarity_search(query, k=3)\n",
    "\n",
    "        print(f\"æŸ¥è¯¢: '{query}'\")\n",
    "        for i, doc in enumerate(results):\n",
    "            print(f\"{i+1}. {doc.page_content}\")\n",
    "            print(f\"   é¢†åŸŸ: {doc.metadata.get('domain')}\")\n",
    "            print(f\"   å¤æ‚åº¦: {doc.metadata.get('complexity')}\")\n",
    "            print()\n",
    "\n",
    "        # 5.4 æ··åˆæœç´¢ï¼ˆå‘é‡+å…³é”®è¯ï¼‰\n",
    "        print(\"\\n5.4 Elasticsearchæ··åˆæœç´¢\")\n",
    "\n",
    "        # ç»“åˆå‘é‡æœç´¢å’Œå…³é”®è¯æœç´¢\n",
    "        hybrid_results = es_vectorstore.similarity_search(\n",
    "            query=\"æœç´¢å¼•æ“\",\n",
    "            k=3,\n",
    "            filter={\"term\": {\"metadata.domain.keyword\": \"search\"}}\n",
    "        )\n",
    "\n",
    "        print(\"æ··åˆæœç´¢ç»“æœ:\")\n",
    "        for doc in hybrid_results:\n",
    "            print(f\"- {doc.page_content}\")\n",
    "            print(f\"  å¤æ‚åº¦: {doc.metadata.get('complexity')}\")\n",
    "\n",
    "        # 5.5 èšåˆæŸ¥è¯¢\n",
    "        print(\"\\n5.5 ElasticsearchèšåˆæŸ¥è¯¢\")\n",
    "\n",
    "        # è·å–ä¸åŒå¤æ‚åº¦çš„æ–‡æ¡£æ•°é‡\n",
    "        agg_query = {\n",
    "            \"aggs\": {\n",
    "                \"complexity_count\": {\n",
    "                    \"terms\": {\n",
    "                        \"field\": \"metadata.complexity.keyword\"\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "\n",
    "        # æ‰§è¡ŒèšåˆæŸ¥è¯¢\n",
    "        agg_results = es_client.search(\n",
    "            index=index_name,\n",
    "            body=agg_query,\n",
    "            size=0\n",
    "        )\n",
    "\n",
    "        print(\"å¤æ‚åº¦åˆ†å¸ƒ:\")\n",
    "        for bucket in agg_results[\"aggregations\"][\"complexity_count\"][\"buckets\"]:\n",
    "            print(f\"- {bucket['key']}: {bucket['doc_count']} ä¸ªæ–‡æ¡£\")\n",
    "\n",
    "        return es_vectorstore\n",
    "\n",
    "    except ImportError:\n",
    "        print(\"éœ€è¦å®‰è£…elasticsearch: pip install elasticsearch\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Elasticsearchç¤ºä¾‹å¤±è´¥: {e}\")\n",
    "        return None\n",
    "\n",
    "def redis_vectorstore_example():\n",
    "    \"\"\"Rediså‘é‡å­˜å‚¨ç¤ºä¾‹\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"6. Rediså‘é‡å­˜å‚¨ç¤ºä¾‹\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    try:\n",
    "        import redis\n",
    "\n",
    "        # 6.1 Redisè¿æ¥\n",
    "        print(\"\\n6.1 Redisè¿æ¥\")\n",
    "\n",
    "        redis_client = redis.Redis(\n",
    "            host=\"localhost\",\n",
    "            port=6379,\n",
    "            db=0,\n",
    "            decode_responses=True\n",
    "        )\n",
    "\n",
    "        # æ£€æŸ¥è¿æ¥\n",
    "        redis_client.ping()\n",
    "        print(\"âœ… Redisè¿æ¥æˆåŠŸ\")\n",
    "\n",
    "        # åˆå§‹åŒ–åµŒå…¥æ¨¡å‹\n",
    "        embeddings = HuggingFaceEmbeddings(\n",
    "            model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "        )\n",
    "\n",
    "        # å‡†å¤‡æ–‡æ¡£\n",
    "        documents = [\n",
    "            Document(\n",
    "                page_content=\"Redisæ˜¯ä¸€ä¸ªé«˜æ€§èƒ½çš„é”®å€¼å­˜å‚¨æ•°æ®åº“\",\n",
    "                metadata={\"type\": \"database\", \"performance\": \"high\", \"category\": \"nosql\"}\n",
    "            ),\n",
    "            Document(\n",
    "                page_content=\"Redisæ”¯æŒå¤šç§æ•°æ®ç»“æ„ï¼Œå¦‚å­—ç¬¦ä¸²ã€åˆ—è¡¨ã€é›†åˆç­‰\",\n",
    "                metadata={\"type\": \"database\", \"feature\": \"data_structures\", \"category\": \"nosql\"}\n",
    "            ),\n",
    "            Document(\n",
    "                page_content=\"Rediså¯ä»¥ç”¨ä½œç¼“å­˜ã€æ¶ˆæ¯é˜Ÿåˆ—å’Œä¼šè¯å­˜å‚¨\",\n",
    "                metadata={\"type\": \"database\", \"use_case\": \"cache\", \"category\": \"nosql\"}\n",
    "            ),\n",
    "            Document(\n",
    "                page_content=\"Redisé›†ç¾¤æä¾›äº†é«˜å¯ç”¨æ€§å’Œæ°´å¹³æ‰©å±•èƒ½åŠ›\",\n",
    "                metadata={\"type\": \"database\", \"feature\": \"clustering\", \"category\": \"nosql\"}\n",
    "            )\n",
    "        ]\n",
    "\n",
    "        # 6.2 åˆ›å»ºRediså‘é‡å­˜å‚¨\n",
    "        print(\"\\n6.2 åˆ›å»ºRediså‘é‡å­˜å‚¨\")\n",
    "\n",
    "        index_name = \"redis_langchain_demo\"\n",
    "\n",
    "        redis_vectorstore = Redis.from_documents(\n",
    "            documents,\n",
    "            embeddings,\n",
    "            redis_url=\"redis://localhost:6379\",\n",
    "            index_name=index_name\n",
    "        )\n",
    "\n",
    "        print(f\"âœ… Rediså‘é‡å­˜å‚¨åˆ›å»ºæˆåŠŸï¼Œç´¢å¼•: {index_name}\")\n",
    "\n",
    "        # 6.3 æœç´¢æµ‹è¯•\n",
    "        print(\"\\n6.3 Redisæœç´¢æµ‹è¯•\")\n",
    "\n",
    "        query = \"ä»€ä¹ˆæ˜¯é«˜æ€§èƒ½æ•°æ®åº“ï¼Ÿ\"\n",
    "        results = redis_vectorstore.similarity_search(query, k=3)\n",
    "\n",
    "        print(f\"æŸ¥è¯¢: '{query}'\")\n",
    "        for i, doc in enumerate(results):\n",
    "            print(f\"{i+1}. {doc.page_content}\")\n",
    "            print(f\"   ç±»å‹: {doc.metadata.get('type')}\")\n",
    "            print(f\"   ç‰¹æ€§: {doc.metadata.get('feature', doc.metadata.get('performance', 'N/A'))}\")\n",
    "            print()\n",
    "\n",
    "        # 6.4 è¿‡æ»¤æœç´¢\n",
    "        print(\"\\n6.4 Redisè¿‡æ»¤æœç´¢\")\n",
    "\n",
    "        # æœç´¢ç‰¹å®šç‰¹æ€§çš„æ–‡æ¡£\n",
    "        feature_results = redis_vectorstore.similarity_search(\n",
    "            query=\"RedisåŠŸèƒ½\",\n",
    "            k=5,\n",
    "            filter={\"feature\": \"data_structures\"}\n",
    "        )\n",
    "\n",
    "        print(\"æ•°æ®ç»“æ„ç›¸å…³çš„æ–‡æ¡£:\")\n",
    "        for doc in feature_results:\n",
    "            print(f\"- {doc.page_content}\")\n",
    "\n",
    "        return redis_vectorstore\n",
    "\n",
    "    except ImportError:\n",
    "        print(\"éœ€è¦å®‰è£…redis: pip install redis\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Redisç¤ºä¾‹å¤±è´¥: {e}\")\n",
    "        return None\n",
    "\n",
    "def vector_store_comparison():\n",
    "    \"\"\"å‘é‡å­˜å‚¨æ€§èƒ½å¯¹æ¯”\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"7. å‘é‡å­˜å‚¨æ€§èƒ½å¯¹æ¯”\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # å‡†å¤‡æµ‹è¯•æ•°æ®\n",
    "    embeddings = HuggingFaceEmbeddings(\n",
    "        model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "    )\n",
    "\n",
    "    # ç”Ÿæˆæµ‹è¯•æ–‡æ¡£\n",
    "    test_documents = []\n",
    "    for i in range(100):\n",
    "        doc = Document(\n",
    "            page_content=f\"è¿™æ˜¯ç¬¬{i}ä¸ªæµ‹è¯•æ–‡æ¡£ï¼ŒåŒ…å«ä¸€äº›ç¤ºä¾‹å†…å®¹ç”¨äºæ€§èƒ½æµ‹è¯•ã€‚æ–‡æ¡£ç¼–å·ï¼š{i}\",\n",
    "            metadata={\"doc_id\": i, \"category\": f\"cat_{i % 5}\", \"priority\": i % 3}\n",
    "        )\n",
    "        test_documents.append(doc)\n",
    "\n",
    "    test_query = \"æµ‹è¯•æ–‡æ¡£å†…å®¹\"\n",
    "\n",
    "    # æµ‹è¯•ä¸åŒå‘é‡å­˜å‚¨çš„æ€§èƒ½\n",
    "    stores_to_test = []\n",
    "\n",
    "    # FAISSæµ‹è¯•\n",
    "    try:\n",
    "        print(\"\\n7.1 FAISSæ€§èƒ½æµ‹è¯•\")\n",
    "        start_time = time.time()\n",
    "        faiss_store = FAISS.from_documents(test_documents, embeddings)\n",
    "        creation_time = time.time() - start_time\n",
    "\n",
    "        start_time = time.time()\n",
    "        faiss_results = faiss_store.similarity_search(test_query, k=5)\n",
    "        search_time = time.time() - start_time\n",
    "\n",
    "        stores_to_test.append({\n",
    "            \"name\": \"FAISS\",\n",
    "            \"creation_time\": creation_time,\n",
    "            \"search_time\": search_time,\n",
    "            \"results_count\": len(faiss_results)\n",
    "        })\n",
    "\n",
    "        print(f\"FAISS - åˆ›å»ºæ—¶é—´: {creation_time:.4f}s, æœç´¢æ—¶é—´: {search_time:.4f}s\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"FAISSæµ‹è¯•å¤±è´¥: {e}\")\n",
    "\n",
    "    # Chromaæµ‹è¯•\n",
    "    try:\n",
    "        print(\"\\n7.2 Chromaæ€§èƒ½æµ‹è¯•\")\n",
    "        start_time = time.time()\n",
    "        chroma_store = Chroma.from_documents(\n",
    "            test_documents,\n",
    "            embeddings,\n",
    "            persist_directory=\"./test_chroma\",\n",
    "            collection_name=\"performance_test\"\n",
    "        )\n",
    "        creation_time = time.time() - start_time\n",
    "\n",
    "        start_time = time.time()\n",
    "        chroma_results = chroma_store.similarity_search(test_query, k=5)\n",
    "        search_time = time.time() - start_time\n",
    "\n",
    "        stores_to_test.append({\n",
    "            \"name\": \"Chroma\",\n",
    "            \"creation_time\": creation_time,\n",
    "            \"search_time\": search_time,\n",
    "            \"results_count\": len(chroma_results)\n",
    "        })\n",
    "\n",
    "        print(f\"Chroma - åˆ›å»ºæ—¶é—´: {creation_time:.4f}s, æœç´¢æ—¶é—´: {search_time:.4f}s\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Chromaæµ‹è¯•å¤±è´¥: {e}\")\n",
    "\n",
    "    # æ€§èƒ½å¯¹æ¯”æ€»ç»“\n",
    "    print(\"\\n7.3 æ€§èƒ½å¯¹æ¯”æ€»ç»“\")\n",
    "    print(f\"{'å­˜å‚¨ç±»å‹':<15} {'åˆ›å»ºæ—¶é—´(s)':<12} {'æœç´¢æ—¶é—´(s)':<12} {'ç»“æœæ•°é‡':<10}\")\n",
    "    print(\"-\" * 55)\n",
    "\n",
    "    for store in stores_to_test:\n",
    "        print(f\"{store['name']:<15} {store['creation_time']:<12.4f} \"\n",
    "              f\"{store['search_time']:<12.4f} {store['results_count']:<10}\")\n",
    "\n",
    "    # æ¨èå»ºè®®\n",
    "    print(\"\\n7.4 é€‰æ‹©å»ºè®®\")\n",
    "    print(\"ğŸ“‹ å‘é‡å­˜å‚¨é€‰æ‹©æŒ‡å—:\")\n",
    "    print(\"1. å°è§„æ¨¡æœ¬åœ°åº”ç”¨: FAISS\")\n",
    "    print(\"2. éœ€è¦æŒä¹…åŒ–: Chroma\")\n",
    "    print(\"3. ç”Ÿäº§ç¯å¢ƒå¤§è§„æ¨¡: Pinecone/Qdrant\")\n",
    "    print(\"4. å·²æœ‰Elasticsearch: ElasticsearchStore\")\n",
    "    print(\"5. éœ€è¦é«˜æ€§èƒ½ç¼“å­˜: Redis\")\n",
    "    print(\"6. ä¼ä¸šçº§åº”ç”¨: Weaviate/Milvus\")\n",
    "\n",
    "def advanced_vector_operations():\n",
    "    \"\"\"é«˜çº§å‘é‡æ“ä½œç¤ºä¾‹\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"8. é«˜çº§å‘é‡æ“ä½œç¤ºä¾‹\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    try:\n",
    "        embeddings = HuggingFaceEmbeddings(\n",
    "            model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "        )\n",
    "\n",
    "        # 8.1 å‘é‡å­˜å‚¨åˆå¹¶\n",
    "        print(\"\\n8.1 å‘é‡å­˜å‚¨åˆå¹¶\")\n",
    "\n",
    "        # åˆ›å»ºä¸¤ä¸ªä¸åŒçš„å‘é‡å­˜å‚¨\n",
    "        docs1 = [\n",
    "            Document(page_content=\"ç¬¬ä¸€ä¸ªå­˜å‚¨çš„æ–‡æ¡£1\", metadata={\"store\": \"store1\"}),\n",
    "            Document(page_content=\"ç¬¬ä¸€ä¸ªå­˜å‚¨çš„æ–‡æ¡£2\", metadata={\"store\": \"store1\"})\n",
    "        ]\n",
    "\n",
    "        docs2 = [\n",
    "            Document(page_content=\"ç¬¬äºŒä¸ªå­˜å‚¨çš„æ–‡æ¡£1\", metadata={\"store\": \"store2\"}),\n",
    "            Document(page_content=\"ç¬¬äºŒä¸ªå­˜å‚¨çš„æ–‡æ¡£2\", metadata={\"store\": \"store2\"})\n",
    "        ]\n",
    "\n",
    "        store1 = FAISS.from_documents(docs1, embeddings)\n",
    "        store2 = FAISS.from_documents(docs2, embeddings)\n",
    "\n",
    "        # åˆå¹¶å‘é‡å­˜å‚¨\n",
    "        store1.merge_from(store2)\n",
    "        print(\"âœ… å‘é‡å­˜å‚¨åˆå¹¶å®Œæˆ\")\n",
    "\n",
    "        # éªŒè¯åˆå¹¶ç»“æœ\n",
    "        all_results = store1.similarity_search(\"æ–‡æ¡£\", k=4)\n",
    "        print(f\"åˆå¹¶åæ€»æ–‡æ¡£æ•°: {len(all_results)}\")\n",
    "        for doc in all_results:\n",
    "            print(f\"- {doc.page_content} (æ¥æº: {doc.metadata['store']})\")\n",
    "\n",
    "        # 8.2 æ‰¹é‡æ“ä½œ\n",
    "        print(\"\\n8.2 æ‰¹é‡å‘é‡æ“ä½œ\")\n",
    "\n",
    "        # æ‰¹é‡æ·»åŠ æ–‡æ¡£\n",
    "        batch_docs = [\n",
    "            Document(page_content=f\"æ‰¹é‡æ–‡æ¡£{i}\", metadata={\"batch\": True, \"id\": i})\n",
    "            for i in range(10)\n",
    "        ]\n",
    "\n",
    "        start_time = time.time()\n",
    "        store1.add_documents(batch_docs)\n",
    "        batch_time = time.time() - start_time\n",
    "\n",
    "        print(f\"æ‰¹é‡æ·»åŠ 10ä¸ªæ–‡æ¡£è€—æ—¶: {batch_time:.4f}s\")\n",
    "\n",
    "        # 8.3 å‘é‡å­˜å‚¨ç»Ÿè®¡\n",
    "        print(\"\\n8.3 å‘é‡å­˜å‚¨ç»Ÿè®¡ä¿¡æ¯\")\n",
    "\n",
    "        # è·å–æ‰€æœ‰æ–‡æ¡£è¿›è¡Œç»Ÿè®¡\n",
    "        all_docs = store1.similarity_search(\"\", k=100)  # è·å–æ›´å¤šæ–‡æ¡£\n",
    "\n",
    "        # ç»Ÿè®¡ä¸åŒæ¥æºçš„æ–‡æ¡£æ•°é‡\n",
    "        store_counts = {}\n",
    "        batch_count = 0\n",
    "\n",
    "        for doc in all_docs:\n",
    "            if doc.metadata.get(\"batch\"):\n",
    "                batch_count += 1\n",
    "            else:\n",
    "                store = doc.metadata.get(\"store\", \"unknown\")\n",
    "                store_counts[store] = store_counts.get(store, 0) + 1\n",
    "\n",
    "        print(\"æ–‡æ¡£ç»Ÿè®¡:\")\n",
    "        for store, count in store_counts.items():\n",
    "            print(f\"- {store}: {count} ä¸ªæ–‡æ¡£\")\n",
    "        print(f\"- æ‰¹é‡æ–‡æ¡£: {batch_count} ä¸ªæ–‡æ¡£\")\n",
    "\n",
    "        # 8.4 ç›¸ä¼¼åº¦é˜ˆå€¼è¿‡æ»¤\n",
    "        print(\"\\n8.4 ç›¸ä¼¼åº¦é˜ˆå€¼è¿‡æ»¤\")\n",
    "\n",
    "        query = \"æ‰¹é‡æ–‡æ¡£\"\n",
    "        threshold = 0.5\n",
    "\n",
    "        # è·å–æ‰€æœ‰ç»“æœå¹¶æ‰‹åŠ¨è¿‡æ»¤\n",
    "        all_results_with_scores = store1.similarity_search_with_score(query, k=20)\n",
    "\n",
    "        filtered_results = [\n",
    "            (doc, score) for doc, score in all_results_with_scores\n",
    "            if score <= threshold  # FAISSä½¿ç”¨è·ç¦»ï¼Œè¶Šå°è¶Šç›¸ä¼¼\n",
    "        ]\n",
    "\n",
    "        print(f\"é˜ˆå€¼ {threshold} ä»¥ä¸‹çš„ç»“æœ:\")\n",
    "        for doc, score in filtered_results[:5]:\n",
    "            print(f\"- åˆ†æ•°: {score:.4f} - {doc.page_content}\")\n",
    "\n",
    "        return store1\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"é«˜çº§å‘é‡æ“ä½œå¤±è´¥: {e}\")\n",
    "        return None\n",
    "\n",
    "def main():\n",
    "    \"\"\"è¿è¡Œæ‰€æœ‰å‘é‡å­˜å‚¨ç¤ºä¾‹\"\"\"\n",
    "    print(\"ğŸš€ LangChain 0.3 Vector Stores å®Œæ•´ç¤ºä¾‹\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    # è¿è¡Œæ‰€æœ‰ç¤ºä¾‹\n",
    "    faiss_store = faiss_vectorstore_example()\n",
    "    chroma_store = chroma_vectorstore_example()\n",
    "    qdrant_store = qdrant_vectorstore_example()\n",
    "    pinecone_store = pinecone_vectorstore_example()\n",
    "    es_store = elasticsearch_vectorstore_example()\n",
    "    redis_store = redis_vectorstore_example()\n",
    "\n",
    "    # æ€§èƒ½å¯¹æ¯”\n",
    "    vector_store_comparison()\n",
    "\n",
    "    # é«˜çº§æ“ä½œ\n",
    "    advanced_vector_operations()\n",
    "\n",
    "    print(\"\\nğŸ‰ æ‰€æœ‰å‘é‡å­˜å‚¨ç¤ºä¾‹è¿è¡Œå®Œæˆï¼\")\n",
    "\n",
    "    # æ¸…ç†ä¸´æ—¶æ–‡ä»¶\n",
    "    import shutil\n",
    "    temp_dirs = [\"faiss_index\", \"chroma_db\", \"test_chroma\"]\n",
    "    for temp_dir in temp_dirs:\n",
    "        if os.path.exists(temp_dir):\n",
    "            try:\n",
    "                shutil.rmtree(temp_dir)\n",
    "                print(f\"ğŸ§¹ å·²æ¸…ç†ä¸´æ—¶ç›®å½•: {temp_dir}\")\n",
    "            except Exception as e:\n",
    "                print(f\"æ¸…ç† {temp_dir} å¤±è´¥: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "id": "b94579b900fba73c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T06:59:58.792957Z",
     "start_time": "2025-07-23T06:59:55.770318Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "# 4. Vector Stores ç¤ºä¾‹\n",
    "def vector_stores_example(chunks: List[Document], embeddings):\n",
    "    \"\"\"å‘é‡å­˜å‚¨ç¤ºä¾‹\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"4. Vector Stores å‘é‡å­˜å‚¨ç¤ºä¾‹\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    if embeddings is None:\n",
    "        print(\"è·³è¿‡å‘é‡å­˜å‚¨ç¤ºä¾‹ï¼ˆåµŒå…¥æ¨¡å‹ä¸å¯ç”¨ï¼‰\")\n",
    "        return None, None\n",
    "\n",
    "    # 4.1 FAISSå‘é‡å­˜å‚¨\n",
    "    print(\"\\n4.1 FAISSå‘é‡å­˜å‚¨\")\n",
    "    try:\n",
    "        # åˆ›å»ºFAISSå‘é‡å­˜å‚¨\n",
    "        faiss_vectorstore = FAISS.from_documents(chunks, embeddings)\n",
    "\n",
    "        # ä¿å­˜åˆ°æœ¬åœ°\n",
    "        faiss_vectorstore.save_local(\"faiss_index\")\n",
    "        print(\"âœ… FAISSç´¢å¼•å·²ä¿å­˜\")\n",
    "\n",
    "        # ç›¸ä¼¼æ€§æœç´¢\n",
    "        query = \"äººå·¥æ™ºèƒ½çš„å‘å±•\"\n",
    "        similar_docs = faiss_vectorstore.similarity_search(query, k=3)\n",
    "\n",
    "        print(f\"\\næŸ¥è¯¢: '{query}'\")\n",
    "        print(\"ç›¸ä¼¼æ–‡æ¡£:\")\n",
    "        for i, doc in enumerate(similar_docs):\n",
    "            print(f\"{i + 1}. {doc.page_content[:100]}...\")\n",
    "\n",
    "        # å¸¦åˆ†æ•°çš„ç›¸ä¼¼æ€§æœç´¢\n",
    "        similar_docs_with_scores = faiss_vectorstore.similarity_search_with_score(query, k=3)\n",
    "        print(\"\\nå¸¦åˆ†æ•°çš„æœç´¢ç»“æœ:\")\n",
    "        for i, (doc, score) in enumerate(similar_docs_with_scores):\n",
    "            print(f\"{i + 1}. åˆ†æ•°: {score:.4f} - {doc.page_content[:80]}...\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"FAISSåˆ›å»ºå¤±è´¥: {e}\")\n",
    "        faiss_vectorstore = None\n",
    "\n",
    "    # 4.2 Chromaå‘é‡å­˜å‚¨\n",
    "    print(\"\\n4.2 Chromaå‘é‡å­˜å‚¨\")\n",
    "    try:\n",
    "        chroma_vectorstore = Chroma.from_documents(\n",
    "            chunks,\n",
    "            embeddings,\n",
    "            persist_directory=\"./chroma_db\"\n",
    "        )\n",
    "\n",
    "        # æŒä¹…åŒ–\n",
    "        chroma_vectorstore.persist()\n",
    "        print(\"âœ… Chromaæ•°æ®åº“å·²æŒä¹…åŒ–\")\n",
    "\n",
    "        # æœç´¢æµ‹è¯•\n",
    "        chroma_results = chroma_vectorstore.similarity_search(\"æœºå™¨å­¦ä¹ \", k=2)\n",
    "        print(f\"Chromaæœç´¢ç»“æœæ•°é‡: {len(chroma_results)}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Chromaåˆ›å»ºå¤±è´¥: {e}\")\n",
    "        chroma_vectorstore = None\n",
    "\n",
    "    return faiss_vectorstore, chroma_vectorstore\n",
    "\n",
    "# 4. å‘é‡å­˜å‚¨\n",
    "faiss_store, chroma_store = vector_stores_example(chunks, embeddings)"
   ],
   "id": "6e01b3deffa2493e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "4. Vector Stores å‘é‡å­˜å‚¨ç¤ºä¾‹\n",
      "============================================================\n",
      "\n",
      "4.1 FAISSå‘é‡å­˜å‚¨\n",
      "âœ… FAISSç´¢å¼•å·²ä¿å­˜\n",
      "\n",
      "æŸ¥è¯¢: 'äººå·¥æ™ºèƒ½çš„å‘å±•'\n",
      "ç›¸ä¼¼æ–‡æ¡£:\n",
      "1. äººå·¥æ™ºèƒ½çš„å‘å±•å†ç¨‹å¯ä»¥è¿½æº¯åˆ°20ä¸–çºª50å¹´ä»£ã€‚å½“æ—¶ï¼Œè®¡ç®—æœºç§‘å­¦å®¶å¼€å§‹æ¢ç´¢è®©æœºå™¨æ¨¡æ‹Ÿäººç±»æ™ºèƒ½çš„å¯èƒ½æ€§ã€‚\n",
      "\n",
      "    åœ¨1956å¹´çš„è¾¾ç‰¹èŒ…æ–¯ä¼šè®®ä¸Šï¼Œäººå·¥æ™ºèƒ½è¿™ä¸ªæœ¯è¯­é¦–æ¬¡è¢«æ­£å¼æå‡ºã€‚è¿™æ ‡å¿—ç€AIä½œä¸ºä¸€ä¸ªç‹¬ç«‹å­¦...\n",
      "2. 80å¹´ä»£æœ«åˆ°90å¹´ä»£åˆï¼Œç”±äºæŠ€æœ¯é™åˆ¶å’Œè¿‡é«˜æœŸæœ›ï¼ŒAIè¿›å…¥äº†æ‰€è°“çš„\"AIå†¬å¤©\"ã€‚\n",
      "\n",
      "    21ä¸–çºªä»¥æ¥ï¼Œéšç€å¤§æ•°æ®ã€äº‘è®¡ç®—å’Œæ·±åº¦å­¦ä¹ çš„å‘å±•ï¼ŒAIè¿æ¥äº†æ–°çš„æ˜¥å¤©ã€‚\n",
      "\n",
      "    ä»Šå¤©ï¼ŒAIå·²ç»åœ¨å›¾åƒè¯†åˆ«ã€è‡ª...\n",
      "\n",
      "å¸¦åˆ†æ•°çš„æœç´¢ç»“æœ:\n",
      "1. åˆ†æ•°: 0.4337 - äººå·¥æ™ºèƒ½çš„å‘å±•å†ç¨‹å¯ä»¥è¿½æº¯åˆ°20ä¸–çºª50å¹´ä»£ã€‚å½“æ—¶ï¼Œè®¡ç®—æœºç§‘å­¦å®¶å¼€å§‹æ¢ç´¢è®©æœºå™¨æ¨¡æ‹Ÿäººç±»æ™ºèƒ½çš„å¯èƒ½æ€§ã€‚\n",
      "\n",
      "    åœ¨1956å¹´çš„è¾¾ç‰¹èŒ…æ–¯ä¼šè®®ä¸Šï¼Œäººå·¥æ™ºèƒ½è¿™ä¸ªæœ¯è¯­é¦–...\n",
      "2. åˆ†æ•°: 0.7495 - 80å¹´ä»£æœ«åˆ°90å¹´ä»£åˆï¼Œç”±äºæŠ€æœ¯é™åˆ¶å’Œè¿‡é«˜æœŸæœ›ï¼ŒAIè¿›å…¥äº†æ‰€è°“çš„\"AIå†¬å¤©\"ã€‚\n",
      "\n",
      "    21ä¸–çºªä»¥æ¥ï¼Œéšç€å¤§æ•°æ®ã€äº‘è®¡ç®—å’Œæ·±åº¦å­¦ä¹ çš„å‘å±•ï¼ŒAIè¿æ¥äº†æ–°çš„æ˜¥å¤©ã€‚...\n",
      "\n",
      "4.2 Chromaå‘é‡å­˜å‚¨\n",
      "âœ… Chromaæ•°æ®åº“å·²æŒä¹…åŒ–\n",
      "Chromaæœç´¢ç»“æœæ•°é‡: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\34769\\AppData\\Local\\Temp\\ipykernel_12904\\3854282456.py:51: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
      "  chroma_vectorstore.persist()\n"
     ]
    }
   ],
   "execution_count": 43
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 5. Retrievers ç¤ºä¾‹",
   "id": "ad86b09d47e42ea4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T03:08:21.798555Z",
     "start_time": "2025-07-23T03:08:21.776446Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "# 5. Retrievers ç¤ºä¾‹\n",
    "def retrievers_example(vectorstore, chunks: List[Document]):\n",
    "    \"\"\"æ£€ç´¢å™¨ç¤ºä¾‹\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"5. Retrievers æ£€ç´¢å™¨ç¤ºä¾‹\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # 5.1 å‘é‡å­˜å‚¨æ£€ç´¢å™¨\n",
    "    print(\"\\n5.1 å‘é‡å­˜å‚¨æ£€ç´¢å™¨\")\n",
    "    if vectorstore:\n",
    "        vector_retriever = vectorstore.as_retriever(\n",
    "            search_type=\"similarity\",\n",
    "            search_kwargs={\"k\": 3}\n",
    "        )\n",
    "\n",
    "        results = vector_retriever.invoke(\"äººå·¥æ™ºèƒ½çš„åº”ç”¨\")\n",
    "        print(f\"å‘é‡æ£€ç´¢ç»“æœæ•°é‡: {len(results)}\")\n",
    "        for i, doc in enumerate(results):\n",
    "            print(f\"{i + 1}. {doc.page_content[:100]}...\")\n",
    "\n",
    "    # 5.2 BM25æ£€ç´¢å™¨\n",
    "    print(\"\\n5.2 BM25æ£€ç´¢å™¨\")\n",
    "    try:\n",
    "        bm25_retriever = BM25Retriever.from_documents(chunks)\n",
    "        bm25_retriever.k = 3\n",
    "\n",
    "        bm25_results = bm25_retriever.invoke(\"äººå·¥æ™ºèƒ½å‘å±•\")\n",
    "        print(f\"BM25æ£€ç´¢ç»“æœæ•°é‡: {len(bm25_results)}\")\n",
    "        for i, doc in enumerate(bm25_results):\n",
    "            print(f\"{i + 1}. {doc.page_content[:100]}...\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"BM25æ£€ç´¢å™¨åˆ›å»ºå¤±è´¥: {e}\")\n",
    "        bm25_retriever = None\n",
    "\n",
    "    # 5.3 é›†æˆæ£€ç´¢å™¨\n",
    "    print(\"\\n5.3 é›†æˆæ£€ç´¢å™¨\")\n",
    "    if vectorstore and bm25_retriever:\n",
    "        try:\n",
    "            ensemble_retriever = EnsembleRetriever(\n",
    "                retrievers=[vector_retriever, bm25_retriever],\n",
    "                weights=[0.7, 0.3]  # å‘é‡æœç´¢æƒé‡0.7ï¼ŒBM25æƒé‡0.3\n",
    "            )\n",
    "\n",
    "            ensemble_results = ensemble_retriever.invoke(\"æœºå™¨å­¦ä¹ æŠ€æœ¯\")\n",
    "            print(f\"é›†æˆæ£€ç´¢ç»“æœæ•°é‡: {len(ensemble_results)}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"é›†æˆæ£€ç´¢å™¨åˆ›å»ºå¤±è´¥: {e}\")\n",
    "\n",
    "    # 5.4 å¤šæŸ¥è¯¢æ£€ç´¢å™¨\n",
    "    print(\"\\n5.4 å¤šæŸ¥è¯¢æ£€ç´¢å™¨\")\n",
    "    if vectorstore:\n",
    "        try:\n",
    "            llm = ChatOllama(\n",
    "                base_url=\"http://localhost:11434\",\n",
    "                model=\"gemma3:4b\"\n",
    "            )\n",
    "\n",
    "            multi_query_retriever = MultiQueryRetriever.from_llm(\n",
    "                retriever=vector_retriever,\n",
    "                llm=llm\n",
    "            )\n",
    "\n",
    "            multi_results = multi_query_retriever.invoke(\"AIçš„æœªæ¥å‘å±•è¶‹åŠ¿\")\n",
    "            print(f\"å¤šæŸ¥è¯¢æ£€ç´¢ç»“æœæ•°é‡: {len(multi_results)}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"å¤šæŸ¥è¯¢æ£€ç´¢å™¨åˆ›å»ºå¤±è´¥: {e}\")\n",
    "# 5. æ£€ç´¢å™¨\n",
    "retrievers_example(faiss_store, chunks)"
   ],
   "id": "f14c88e8a32df855",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "5. Retrievers æ£€ç´¢å™¨ç¤ºä¾‹\n",
      "============================================================\n",
      "\n",
      "5.1 å‘é‡å­˜å‚¨æ£€ç´¢å™¨\n",
      "\n",
      "5.2 BM25æ£€ç´¢å™¨\n",
      "BM25æ£€ç´¢ç»“æœæ•°é‡: 2\n",
      "1. 80å¹´ä»£æœ«åˆ°90å¹´ä»£åˆï¼Œç”±äºæŠ€æœ¯é™åˆ¶å’Œè¿‡é«˜æœŸæœ›ï¼ŒAIè¿›å…¥äº†æ‰€è°“çš„\"AIå†¬å¤©\"ã€‚\n",
      "\n",
      "    21ä¸–çºªä»¥æ¥ï¼Œéšç€å¤§æ•°æ®ã€äº‘è®¡ç®—å’Œæ·±åº¦å­¦ä¹ çš„å‘å±•ï¼ŒAIè¿æ¥äº†æ–°çš„æ˜¥å¤©ã€‚\n",
      "\n",
      "    ä»Šå¤©ï¼ŒAIå·²ç»åœ¨å›¾åƒè¯†åˆ«ã€è‡ª...\n",
      "2. äººå·¥æ™ºèƒ½çš„å‘å±•å†ç¨‹å¯ä»¥è¿½æº¯åˆ°20ä¸–çºª50å¹´ä»£ã€‚å½“æ—¶ï¼Œè®¡ç®—æœºç§‘å­¦å®¶å¼€å§‹æ¢ç´¢è®©æœºå™¨æ¨¡æ‹Ÿäººç±»æ™ºèƒ½çš„å¯èƒ½æ€§ã€‚\n",
      "\n",
      "    åœ¨1956å¹´çš„è¾¾ç‰¹èŒ…æ–¯ä¼šè®®ä¸Šï¼Œäººå·¥æ™ºèƒ½è¿™ä¸ªæœ¯è¯­é¦–æ¬¡è¢«æ­£å¼æå‡ºã€‚è¿™æ ‡å¿—ç€AIä½œä¸ºä¸€ä¸ªç‹¬ç«‹å­¦...\n",
      "\n",
      "5.3 é›†æˆæ£€ç´¢å™¨\n",
      "\n",
      "5.4 å¤šæŸ¥è¯¢æ£€ç´¢å™¨\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 6. å®Œæ•´RAGæµç¨‹ç¤ºä¾‹",
   "id": "f90a79c3cfcb7331"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T08:40:23.593705Z",
     "start_time": "2025-07-23T08:40:11.224941Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# 6. å®Œæ•´RAGæµç¨‹ç¤ºä¾‹\n",
    "def complete_rag_example():\n",
    "    \"\"\"å®Œæ•´çš„RAGæµç¨‹ç¤ºä¾‹\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"6. å®Œæ•´RAGæµç¨‹ç¤ºä¾‹\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    try:\n",
    "        # 6.1 å‡†å¤‡æ•°æ®\n",
    "        documents = [\n",
    "            Document(page_content=\"LangChainæ˜¯ä¸€ä¸ªç”¨äºæ„å»ºLLMåº”ç”¨çš„æ¡†æ¶\", metadata={\"source\": \"doc1\"}),\n",
    "            Document(page_content=\"å‘é‡æ•°æ®åº“å¯ä»¥å­˜å‚¨å’Œæ£€ç´¢é«˜ç»´å‘é‡\", metadata={\"source\": \"doc2\"}),\n",
    "            Document(page_content=\"RAGç»“åˆäº†æ£€ç´¢å’Œç”Ÿæˆï¼Œæé«˜äº†AIå›ç­”çš„å‡†ç¡®æ€§\", metadata={\"source\": \"doc3\"}),\n",
    "            Document(page_content=\"åµŒå…¥æ¨¡å‹å°†æ–‡æœ¬è½¬æ¢ä¸ºæ•°å€¼å‘é‡è¡¨ç¤º\", metadata={\"source\": \"doc4\"})\n",
    "        ]\n",
    "\n",
    "        # 6.2 æ–‡æœ¬åˆ†å‰²\n",
    "        splitter = RecursiveCharacterTextSplitter(chunk_size=100, chunk_overlap=20)\n",
    "        chunks = splitter.split_documents(documents)\n",
    "\n",
    "        # 6.3 åˆ›å»ºåµŒå…¥å’Œå‘é‡å­˜å‚¨\n",
    "        embeddings = OllamaEmbeddings(\n",
    "            base_url=\"http://localhost:11434\",\n",
    "            model=\"nomic-embed-text\"\n",
    "        )\n",
    "\n",
    "        vectorstore = FAISS.from_documents(chunks, embeddings)\n",
    "        retriever = vectorstore.as_retriever(k=2)\n",
    "\n",
    "        # 6.4 åˆ›å»ºRAGé“¾\n",
    "        from langchain_core.prompts import ChatPromptTemplate\n",
    "        from langchain_core.output_parsers import StrOutputParser\n",
    "        from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "        llm = ChatOllama(\n",
    "            base_url=\"http://localhost:11434\",\n",
    "            model=\"gemma3:4b\"\n",
    "        )\n",
    "\n",
    "        prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "        åŸºäºä»¥ä¸‹ä¸Šä¸‹æ–‡å›ç­”é—®é¢˜ï¼š\n",
    "\n",
    "        ä¸Šä¸‹æ–‡ï¼š{context}\n",
    "\n",
    "        é—®é¢˜ï¼š{question}\n",
    "\n",
    "        è¯·æä¾›å‡†ç¡®ã€ç®€æ´çš„å›ç­”ï¼š\n",
    "        \"\"\")\n",
    "\n",
    "        def format_docs(docs):\n",
    "            return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "        rag_chain = (\n",
    "                {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "                | prompt\n",
    "                | llm\n",
    "                | StrOutputParser()\n",
    "        )\n",
    "\n",
    "        # 6.5 æµ‹è¯•RAGç³»ç»Ÿ\n",
    "        questions = [\n",
    "            \"ä»€ä¹ˆæ˜¯LangChainï¼Ÿ\",\n",
    "            \"å‘é‡æ•°æ®åº“çš„ä½œç”¨æ˜¯ä»€ä¹ˆï¼Ÿ\",\n",
    "            \"RAGæŠ€æœ¯æœ‰ä»€ä¹ˆä¼˜åŠ¿ï¼Ÿ\"\n",
    "        ]\n",
    "\n",
    "        for question in questions:\n",
    "            print(f\"\\né—®é¢˜: {question}\")\n",
    "            answer = rag_chain.invoke(question)\n",
    "            print(f\"å›ç­”: {answer}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"RAGæµç¨‹æ‰§è¡Œå¤±è´¥: {e}\")\n",
    "complete_rag_example()"
   ],
   "id": "e78b5eb261c8ae41",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "6. å®Œæ•´RAGæµç¨‹ç¤ºä¾‹\n",
      "============================================================\n",
      "\n",
      "é—®é¢˜: ä»€ä¹ˆæ˜¯LangChainï¼Ÿ\n",
      "å›ç­”: LangChain æ˜¯ä¸€ä¸ªç”¨äºæ„å»ºLLMåº”ç”¨çš„æ¡†æ¶ã€‚\n",
      "\n",
      "\n",
      "é—®é¢˜: å‘é‡æ•°æ®åº“çš„ä½œç”¨æ˜¯ä»€ä¹ˆï¼Ÿ\n",
      "å›ç­”: å‘é‡æ•°æ®åº“çš„ä½œç”¨æ˜¯å­˜å‚¨å’Œæ£€ç´¢é«˜ç»´å‘é‡ã€‚\n",
      "\n",
      "\n",
      "é—®é¢˜: RAGæŠ€æœ¯æœ‰ä»€ä¹ˆä¼˜åŠ¿ï¼Ÿ\n",
      "å›ç­”: RAGæŠ€æœ¯æé«˜äº†AIå›ç­”çš„å‡†ç¡®æ€§ã€‚\n",
      "\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 7. é«˜çº§åŠŸèƒ½ç¤ºä¾‹",
   "id": "7af8f010e7932d07"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T03:08:25.967216Z",
     "start_time": "2025-07-23T03:08:25.960219Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# 7. é«˜çº§åŠŸèƒ½ç¤ºä¾‹\n",
    "def advanced_features_example():\n",
    "    \"\"\"é«˜çº§åŠŸèƒ½ç¤ºä¾‹\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"7. é«˜çº§åŠŸèƒ½ç¤ºä¾‹\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # 7.1 è‡ªå®šä¹‰æ–‡æ¡£åŠ è½½å™¨\n",
    "    print(\"\\n7.1 è‡ªå®šä¹‰æ–‡æ¡£åŠ è½½å™¨\")\n",
    "\n",
    "    class CustomLoader:\n",
    "        def __init__(self, data_source):\n",
    "            self.data_source = data_source\n",
    "\n",
    "        def load(self):\n",
    "            # æ¨¡æ‹Ÿä»APIæˆ–æ•°æ®åº“åŠ è½½æ•°æ®\n",
    "            documents = []\n",
    "            for i, item in enumerate(self.data_source):\n",
    "                doc = Document(\n",
    "                    page_content=item[\"content\"],\n",
    "                    metadata={\"id\": i, \"type\": item[\"type\"]}\n",
    "                )\n",
    "                documents.append(doc)\n",
    "            return documents\n",
    "\n",
    "    # ä½¿ç”¨è‡ªå®šä¹‰åŠ è½½å™¨\n",
    "    custom_data = [\n",
    "        {\"content\": \"Pythonæ˜¯ä¸€ç§ç¼–ç¨‹è¯­è¨€\", \"type\": \"æŠ€æœ¯\"},\n",
    "        {\"content\": \"æ•°æ®ç§‘å­¦éœ€è¦ç»Ÿè®¡çŸ¥è¯†\", \"type\": \"ç§‘å­¦\"},\n",
    "        {\"content\": \"æœºå™¨å­¦ä¹ ç®—æ³•å¾ˆé‡è¦\", \"type\": \"AI\"}\n",
    "    ]\n",
    "\n",
    "    custom_loader = CustomLoader(custom_data)\n",
    "    custom_docs = custom_loader.load()\n",
    "    print(f\"è‡ªå®šä¹‰åŠ è½½å™¨æ–‡æ¡£æ•°é‡: {len(custom_docs)}\")\n",
    "\n",
    "    # 7.2 æ–‡æ¡£è¿‡æ»¤å’Œé¢„å¤„ç†\n",
    "    print(\"\\n7.2 æ–‡æ¡£è¿‡æ»¤å’Œé¢„å¤„ç†\")\n",
    "\n",
    "    def preprocess_documents(documents):\n",
    "        \"\"\"æ–‡æ¡£é¢„å¤„ç†å‡½æ•°\"\"\"\n",
    "        processed_docs = []\n",
    "        for doc in documents:\n",
    "            # æ¸…ç†æ–‡æœ¬\n",
    "            content = doc.page_content.strip()\n",
    "            content = content.replace(\"\\n\", \" \").replace(\"\\t\", \" \")\n",
    "\n",
    "            # è¿‡æ»¤çŸ­æ–‡æ¡£\n",
    "            if len(content) > 10:\n",
    "                doc.page_content = content\n",
    "                processed_docs.append(doc)\n",
    "\n",
    "        return processed_docs\n",
    "\n",
    "    processed_docs = preprocess_documents(custom_docs)\n",
    "    print(f\"é¢„å¤„ç†åæ–‡æ¡£æ•°é‡: {len(processed_docs)}\")\n",
    "advanced_features_example()"
   ],
   "id": "1fd34ff050068f21",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "7. é«˜çº§åŠŸèƒ½ç¤ºä¾‹\n",
      "============================================================\n",
      "\n",
      "7.1 è‡ªå®šä¹‰æ–‡æ¡£åŠ è½½å™¨\n",
      "è‡ªå®šä¹‰åŠ è½½å™¨æ–‡æ¡£æ•°é‡: 3\n",
      "\n",
      "7.2 æ–‡æ¡£è¿‡æ»¤å’Œé¢„å¤„ç†\n",
      "é¢„å¤„ç†åæ–‡æ¡£æ•°é‡: 1\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 8. æ€§èƒ½ä¼˜åŒ–ç¤ºä¾‹",
   "id": "7bddd0a43c7a49f4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T03:08:25.998972Z",
     "start_time": "2025-07-23T03:08:25.990972Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 8. æ€§èƒ½ä¼˜åŒ–ç¤ºä¾‹\n",
    "async def performance_optimization_example():\n",
    "    \"\"\"æ€§èƒ½ä¼˜åŒ–ç¤ºä¾‹\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"8. æ€§èƒ½ä¼˜åŒ–ç¤ºä¾‹\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # 8.1 æ‰¹é‡å¤„ç†\n",
    "    print(\"\\n8.1 æ‰¹é‡åµŒå…¥å¤„ç†\")\n",
    "\n",
    "    try:\n",
    "        embeddings = OllamaEmbeddings(\n",
    "            base_url=\"http://localhost:11434\",\n",
    "            model=\"nomic-embed-text\"\n",
    "        )\n",
    "\n",
    "        # å¤§é‡æ–‡æœ¬\n",
    "        texts = [f\"è¿™æ˜¯ç¬¬{i}ä¸ªæ–‡æ¡£çš„å†…å®¹\" for i in range(10)]\n",
    "\n",
    "        # æ‰¹é‡ç”ŸæˆåµŒå…¥\n",
    "        batch_embeddings = embeddings.embed_documents(texts)\n",
    "        print(f\"æ‰¹é‡å¤„ç†æ–‡æ¡£æ•°é‡: {len(batch_embeddings)}\")\n",
    "\n",
    "        # 8.2 å¼‚æ­¥å¤„ç†\n",
    "        print(\"\\n8.2 å¼‚æ­¥å¤„ç†ç¤ºä¾‹\")\n",
    "\n",
    "        async def async_embed_text(text):\n",
    "            # æ¨¡æ‹Ÿå¼‚æ­¥åµŒå…¥\n",
    "            await asyncio.sleep(0.1)\n",
    "            return embeddings.embed_query(text)\n",
    "\n",
    "        # å¹¶å‘å¤„ç†\n",
    "        tasks = [async_embed_text(f\"å¼‚æ­¥æ–‡æœ¬{i}\") for i in range(5)]\n",
    "        results = await asyncio.gather(*tasks)\n",
    "        print(f\"å¼‚æ­¥å¤„ç†ç»“æœæ•°é‡: {len(results)}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"æ€§èƒ½ä¼˜åŒ–ç¤ºä¾‹å¤±è´¥: {e}\")\n",
    "performance_optimization_example()\n"
   ],
   "id": "35a7d46e00e601f",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<coroutine object performance_optimization_example at 0x000002B59BA34260>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## ä¸»å‡½æ•°",
   "id": "2e5e75e1f7ea7e1f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T03:08:26.043412Z",
     "start_time": "2025-07-23T03:08:26.038469Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# ä¸»å‡½æ•°\n",
    "def main():\n",
    "    \"\"\"è¿è¡Œæ‰€æœ‰ç¤ºä¾‹\"\"\"\n",
    "    print(\"ğŸš€ LangChain 0.3 Data Connection å®Œæ•´ç¤ºä¾‹\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    # 1. æ–‡æ¡£åŠ è½½\n",
    "    documents = document_loaders_example()\n",
    "\n",
    "    # 2. æ–‡æœ¬åˆ†å‰²\n",
    "    chunks = text_splitters_example(documents)\n",
    "\n",
    "    # 3. åµŒå…¥æ¨¡å‹\n",
    "    embeddings = embedding_models_example()\n",
    "\n",
    "    # 4. å‘é‡å­˜å‚¨\n",
    "    faiss_store, chroma_store = vector_stores_example(chunks, embeddings)\n",
    "\n",
    "    # 5. æ£€ç´¢å™¨\n",
    "    retrievers_example(faiss_store, chunks)\n",
    "\n",
    "    # 6. å®Œæ•´RAGæµç¨‹\n",
    "    complete_rag_example()\n",
    "\n",
    "    # 7. é«˜çº§åŠŸèƒ½\n",
    "    advanced_features_example()\n",
    "\n",
    "    print(\"\\nğŸ‰ æ‰€æœ‰ç¤ºä¾‹è¿è¡Œå®Œæˆï¼\")\n",
    "\n",
    "    # æ¸…ç†ä¸´æ—¶æ–‡ä»¶\n",
    "    cleanup_files()\n",
    "\n",
    "\n",
    "def cleanup_files():\n",
    "    \"\"\"æ¸…ç†ä¸´æ—¶æ–‡ä»¶\"\"\"\n",
    "    import shutil\n",
    "\n",
    "    files_to_remove = [\"sample.txt\", \"sample.csv\", \"sample.json\"]\n",
    "    dirs_to_remove = [\"docs\", \"faiss_index\", \"chroma_db\"]\n",
    "\n",
    "    for file in files_to_remove:\n",
    "        if os.path.exists(file):\n",
    "            os.remove(file)\n",
    "\n",
    "    for dir in dirs_to_remove:\n",
    "        if os.path.exists(dir):\n",
    "            shutil.rmtree(dir)\n",
    "\n",
    "    print(\"ğŸ§¹ ä¸´æ—¶æ–‡ä»¶å·²æ¸…ç†\")\n"
   ],
   "id": "7f6e6f544e1599ac",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T03:08:34.297186Z",
     "start_time": "2025-07-23T03:08:26.078641Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # è¿è¡ŒåŒæ­¥ç¤ºä¾‹\n",
    "    main()\n",
    "\n",
    "    # è¿è¡Œå¼‚æ­¥ç¤ºä¾‹\n",
    "    # asyncio.run(performance_optimization_example())"
   ],
   "id": "1588268b0cb2662",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ LangChain 0.3 Data Connection å®Œæ•´ç¤ºä¾‹\n",
      "================================================================================\n",
      "\n",
      "============================================================\n",
      "1. Document Loaders æ–‡æ¡£åŠ è½½å™¨ç¤ºä¾‹\n",
      "============================================================\n",
      "\n",
      "1.1 æ–‡æœ¬æ–‡ä»¶åŠ è½½\n",
      "åŠ è½½çš„æ–‡æ¡£æ•°é‡: 1\n",
      "æ–‡æ¡£å†…å®¹é¢„è§ˆ: \n",
      "        äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œè‡´åŠ›äºåˆ›å»ºèƒ½å¤Ÿæ‰§è¡Œé€šå¸¸éœ€è¦äººç±»æ™ºèƒ½çš„ä»»åŠ¡çš„ç³»ç»Ÿã€‚\n",
      "        æœºå™¨å­¦ä¹ æ˜¯AIçš„ä¸€ä¸ªå­é›†ï¼Œå®ƒä½¿è®¡ç®—æœºèƒ½å¤Ÿä»æ•°æ®ä¸­å­¦ä¹ è€Œæ— éœ€æ˜ç¡®ç¼–ç¨‹ã€‚\n",
      "   ...\n",
      "\n",
      "1.2 CSVæ–‡ä»¶åŠ è½½\n",
      "CSVæ–‡æ¡£æ•°é‡: 3\n",
      "CSVæ–‡æ¡£ç¤ºä¾‹: name: å¼ ä¸‰\n",
      "age: 25\n",
      "city: åŒ—äº¬\n",
      "description: è½¯ä»¶å·¥ç¨‹å¸ˆ\n",
      "\n",
      "1.3 JSONæ–‡ä»¶åŠ è½½\n",
      "JSONæ–‡æ¡£æ•°é‡: 2\n",
      "JSONæ–‡æ¡£ç¤ºä¾‹: Pythonæ˜¯ä¸€ç§é«˜çº§ç¼–ç¨‹è¯­è¨€\n",
      "\n",
      "1.4 ç›®å½•æ‰¹é‡åŠ è½½\n",
      "ç›®å½•æ–‡æ¡£æ•°é‡: 3\n",
      "\n",
      "============================================================\n",
      "2. Text Splitters æ–‡æœ¬åˆ†å‰²å™¨ç¤ºä¾‹\n",
      "============================================================\n",
      "\n",
      "2.1 RecursiveCharacterTextSplitter\n",
      "é€’å½’åˆ†å‰²å—æ•°: 2\n",
      "å— 1: äººå·¥æ™ºèƒ½çš„å‘å±•å†ç¨‹å¯ä»¥è¿½æº¯åˆ°20ä¸–çºª50å¹´ä»£ã€‚å½“æ—¶ï¼Œè®¡ç®—æœºç§‘å­¦å®¶å¼€å§‹æ¢ç´¢è®©æœºå™¨æ¨¡æ‹Ÿäººç±»æ™ºèƒ½çš„å¯èƒ½æ€§ã€‚\n",
      "\n",
      "    åœ¨1956å¹´çš„è¾¾ç‰¹èŒ…æ–¯ä¼šè®®ä¸Šï¼Œäººå·¥æ™ºèƒ½è¿™ä¸ªæœ¯è¯­é¦–æ¬¡è¢«æ­£å¼æå‡ºã€‚è¿™æ ‡å¿—ç€AIä½œä¸ºä¸€ä¸ªç‹¬ç«‹å­¦...\n",
      "å— 2: 80å¹´ä»£æœ«åˆ°90å¹´ä»£åˆï¼Œç”±äºæŠ€æœ¯é™åˆ¶å’Œè¿‡é«˜æœŸæœ›ï¼ŒAIè¿›å…¥äº†æ‰€è°“çš„\"AIå†¬å¤©\"ã€‚\n",
      "\n",
      "    21ä¸–çºªä»¥æ¥ï¼Œéšç€å¤§æ•°æ®ã€äº‘è®¡ç®—å’Œæ·±åº¦å­¦ä¹ çš„å‘å±•ï¼ŒAIè¿æ¥äº†æ–°çš„æ˜¥å¤©ã€‚\n",
      "\n",
      "    ä»Šå¤©ï¼ŒAIå·²ç»åœ¨å›¾åƒè¯†åˆ«ã€è‡ª...\n",
      "\n",
      "2.2 CharacterTextSplitter\n",
      "å­—ç¬¦åˆ†å‰²å—æ•°: 1\n",
      "\n",
      "2.3 TokenTextSplitter\n",
      "Tokenåˆ†å‰²å—æ•°: 7\n",
      "\n",
      "2.4 MarkdownHeaderTextSplitter\n",
      "Markdownåˆ†å‰²å—æ•°: 4\n",
      "\n",
      "============================================================\n",
      "3. Embedding Models åµŒå…¥æ¨¡å‹ç¤ºä¾‹\n",
      "============================================================\n",
      "\n",
      "3.1 OllamaåµŒå…¥æ¨¡å‹\n",
      "OllamaåµŒå…¥æ¨¡å‹åˆå§‹åŒ–å¤±è´¥: Failed to connect to Ollama. Please check that Ollama is downloaded, running and accessible. https://ollama.com/download\n",
      "è¯·ç¡®ä¿OllamaæœåŠ¡æ­£åœ¨è¿è¡Œå¹¶å®‰è£…äº†åµŒå…¥æ¨¡å‹\n",
      "\n",
      "============================================================\n",
      "4. Vector Stores å‘é‡å­˜å‚¨ç¤ºä¾‹\n",
      "============================================================\n",
      "è·³è¿‡å‘é‡å­˜å‚¨ç¤ºä¾‹ï¼ˆåµŒå…¥æ¨¡å‹ä¸å¯ç”¨ï¼‰\n",
      "\n",
      "============================================================\n",
      "5. Retrievers æ£€ç´¢å™¨ç¤ºä¾‹\n",
      "============================================================\n",
      "\n",
      "5.1 å‘é‡å­˜å‚¨æ£€ç´¢å™¨\n",
      "\n",
      "5.2 BM25æ£€ç´¢å™¨\n",
      "BM25æ£€ç´¢ç»“æœæ•°é‡: 2\n",
      "1. 80å¹´ä»£æœ«åˆ°90å¹´ä»£åˆï¼Œç”±äºæŠ€æœ¯é™åˆ¶å’Œè¿‡é«˜æœŸæœ›ï¼ŒAIè¿›å…¥äº†æ‰€è°“çš„\"AIå†¬å¤©\"ã€‚\n",
      "\n",
      "    21ä¸–çºªä»¥æ¥ï¼Œéšç€å¤§æ•°æ®ã€äº‘è®¡ç®—å’Œæ·±åº¦å­¦ä¹ çš„å‘å±•ï¼ŒAIè¿æ¥äº†æ–°çš„æ˜¥å¤©ã€‚\n",
      "\n",
      "    ä»Šå¤©ï¼ŒAIå·²ç»åœ¨å›¾åƒè¯†åˆ«ã€è‡ª...\n",
      "2. äººå·¥æ™ºèƒ½çš„å‘å±•å†ç¨‹å¯ä»¥è¿½æº¯åˆ°20ä¸–çºª50å¹´ä»£ã€‚å½“æ—¶ï¼Œè®¡ç®—æœºç§‘å­¦å®¶å¼€å§‹æ¢ç´¢è®©æœºå™¨æ¨¡æ‹Ÿäººç±»æ™ºèƒ½çš„å¯èƒ½æ€§ã€‚\n",
      "\n",
      "    åœ¨1956å¹´çš„è¾¾ç‰¹èŒ…æ–¯ä¼šè®®ä¸Šï¼Œäººå·¥æ™ºèƒ½è¿™ä¸ªæœ¯è¯­é¦–æ¬¡è¢«æ­£å¼æå‡ºã€‚è¿™æ ‡å¿—ç€AIä½œä¸ºä¸€ä¸ªç‹¬ç«‹å­¦...\n",
      "\n",
      "5.3 é›†æˆæ£€ç´¢å™¨\n",
      "\n",
      "5.4 å¤šæŸ¥è¯¢æ£€ç´¢å™¨\n",
      "\n",
      "============================================================\n",
      "6. å®Œæ•´RAGæµç¨‹ç¤ºä¾‹\n",
      "============================================================\n",
      "RAGæµç¨‹æ‰§è¡Œå¤±è´¥: Failed to connect to Ollama. Please check that Ollama is downloaded, running and accessible. https://ollama.com/download\n",
      "\n",
      "============================================================\n",
      "7. é«˜çº§åŠŸèƒ½ç¤ºä¾‹\n",
      "============================================================\n",
      "\n",
      "7.1 è‡ªå®šä¹‰æ–‡æ¡£åŠ è½½å™¨\n",
      "è‡ªå®šä¹‰åŠ è½½å™¨æ–‡æ¡£æ•°é‡: 3\n",
      "\n",
      "7.2 æ–‡æ¡£è¿‡æ»¤å’Œé¢„å¤„ç†\n",
      "é¢„å¤„ç†åæ–‡æ¡£æ•°é‡: 1\n",
      "\n",
      "ğŸ‰ æ‰€æœ‰ç¤ºä¾‹è¿è¡Œå®Œæˆï¼\n",
      "ğŸ§¹ ä¸´æ—¶æ–‡ä»¶å·²æ¸…ç†\n"
     ]
    }
   ],
   "execution_count": 11
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
