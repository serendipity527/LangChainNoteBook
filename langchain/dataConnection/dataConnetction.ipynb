{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Data Connection 核心组件",
   "id": "d2588d34d8fa6662"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "* Data Connection 是 LangChain 中处理外部数据的核心模块，包含以下主要组件：\n",
    "* Document Loaders - 文档加载器\n",
    "* Text Splitters - 文本分割器\n",
    "* Embedding Models - 嵌入模型\n",
    "* Vector Stores - 向量存储\n",
    "* Retrievers - 检索器"
   ],
   "id": "1d00345d4349eba6"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "LangChain 0.3 Data Connection 完整示例\n",
    "包含文档加载、文本分割、向量化、存储和检索的完整流程\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "from typing import List, Dict, Any\n",
    "import asyncio\n",
    "\n",
    "# 核心导入\n",
    "from langchain_community.document_loaders import (\n",
    "    TextLoader,\n",
    "    PyPDFLoader,\n",
    "    CSVLoader,\n",
    "    JSONLoader,\n",
    "    WebBaseLoader,\n",
    "    DirectoryLoader\n",
    ")\n",
    "from langchain.text_splitter import (\n",
    "    RecursiveCharacterTextSplitter,\n",
    "    CharacterTextSplitter,\n",
    "    TokenTextSplitter,\n",
    "    MarkdownHeaderTextSplitter\n",
    ")\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_community.vectorstores import (\n",
    "    FAISS,\n",
    "    Chroma,\n",
    "    Qdrant\n",
    ")\n",
    "from langchain_core.documents import Document\n",
    "from langchain.retrievers import (\n",
    "    BM25Retriever,\n",
    "    EnsembleRetriever,\n",
    "    MultiQueryRetriever\n",
    ")\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "print(\"✅ 所有库导入成功\")"
   ],
   "id": "ed791ba631680edb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "# 1. Document Loaders 示例\n",
    "def document_loaders_example():\n",
    "    \"\"\"文档加载器示例\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"1. Document Loaders 文档加载器示例\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # 1.1 文本文件加载\n",
    "    print(\"\\n1.1 文本文件加载\")\n",
    "    # 创建示例文本文件\n",
    "    with open(\"sample.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"\"\"\n",
    "        人工智能（AI）是计算机科学的一个分支，致力于创建能够执行通常需要人类智能的任务的系统。\n",
    "        机器学习是AI的一个子集，它使计算机能够从数据中学习而无需明确编程。\n",
    "        深度学习是机器学习的一个子集，使用神经网络来模拟人脑的工作方式。\n",
    "        \"\"\")\n",
    "\n",
    "    loader = TextLoader(\"sample.txt\", encoding=\"utf-8\")\n",
    "    documents = loader.load()\n",
    "    print(f\"加载的文档数量: {len(documents)}\")\n",
    "    print(f\"文档内容预览: {documents[0].page_content[:100]}...\")\n",
    "\n",
    "    # 1.2 CSV文件加载\n",
    "    print(\"\\n1.2 CSV文件加载\")\n",
    "    import pandas as pd\n",
    "\n",
    "    # 创建示例CSV\n",
    "    df = pd.DataFrame({\n",
    "        'name': ['张三', '李四', '王五'],\n",
    "        'age': [25, 30, 35],\n",
    "        'city': ['北京', '上海', '深圳'],\n",
    "        'description': ['软件工程师', '数据科学家', '产品经理']\n",
    "    })\n",
    "    df.to_csv(\"sample.csv\", index=False, encoding=\"utf-8\")\n",
    "\n",
    "    csv_loader = CSVLoader(\"sample.csv\", encoding=\"utf-8\")\n",
    "    csv_docs = csv_loader.load()\n",
    "    print(f\"CSV文档数量: {len(csv_docs)}\")\n",
    "    print(f\"CSV文档示例: {csv_docs[0].page_content}\")\n",
    "\n",
    "    # 1.3 JSON文件加载\n",
    "    print(\"\\n1.3 JSON文件加载\")\n",
    "    import json\n",
    "\n",
    "    sample_data = [\n",
    "        {\"title\": \"Python编程\", \"content\": \"Python是一种高级编程语言\", \"category\": \"技术\"},\n",
    "        {\"title\": \"数据分析\", \"content\": \"数据分析是从数据中提取洞察的过程\", \"category\": \"数据科学\"}\n",
    "    ]\n",
    "\n",
    "    with open(\"sample.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(sample_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    json_loader = JSONLoader(\"sample.json\", jq_schema=\".[].content\")\n",
    "    json_docs = json_loader.load()\n",
    "    print(f\"JSON文档数量: {len(json_docs)}\")\n",
    "    print(f\"JSON文档示例: {json_docs[0].page_content}\")\n",
    "\n",
    "    # 1.4 目录批量加载\n",
    "    print(\"\\n1.4 目录批量加载\")\n",
    "    os.makedirs(\"docs\", exist_ok=True)\n",
    "\n",
    "    # 创建多个文档\n",
    "    for i in range(3):\n",
    "        with open(f\"docs/doc_{i}.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(f\"这是第{i+1}个文档的内容。包含关于技术{i+1}的详细信息。\")\n",
    "\n",
    "    dir_loader = DirectoryLoader(\"docs\", glob=\"*.txt\", loader_cls=TextLoader)\n",
    "    dir_docs = dir_loader.load()\n",
    "    print(f\"目录文档数量: {len(dir_docs)}\")\n",
    "\n",
    "    return documents + csv_docs + json_docs + dir_docs\n",
    "\n",
    "# 2. Text Splitters 示例\n",
    "def text_splitters_example(documents: List[Document]):\n",
    "    \"\"\"文本分割器示例\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"2. Text Splitters 文本分割器示例\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # 创建长文本用于分割\n",
    "    long_text = \"\"\"\n",
    "    人工智能的发展历程可以追溯到20世纪50年代。当时，计算机科学家开始探索让机器模拟人类智能的可能性。\n",
    "\n",
    "    在1956年的达特茅斯会议上，人工智能这个术语首次被正式提出。这标志着AI作为一个独立学科的诞生。\n",
    "\n",
    "    随后的几十年里，AI经历了多次起伏。60-70年代是第一个AI春天，专家系统得到了广泛应用。\n",
    "\n",
    "    80年代末到90年代初，由于技术限制和过高期望，AI进入了所谓的\"AI冬天\"。\n",
    "\n",
    "    21世纪以来，随着大数据、云计算和深度学习的发展，AI迎来了新的春天。\n",
    "\n",
    "    今天，AI已经在图像识别、自然语言处理、推荐系统等领域取得了突破性进展。\n",
    "    \"\"\"\n",
    "\n",
    "    long_doc = Document(page_content=long_text, metadata={\"source\": \"ai_history\"})\n",
    "\n",
    "    # 2.1 递归字符分割器（推荐）\n",
    "    print(\"\\n2.1 RecursiveCharacterTextSplitter\")\n",
    "    recursive_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=200,\n",
    "        chunk_overlap=50,\n",
    "        length_function=len,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \"。\", \"，\", \" \", \"\"]\n",
    "    )\n",
    "\n",
    "    recursive_chunks = recursive_splitter.split_documents([long_doc])\n",
    "    print(f\"递归分割块数: {len(recursive_chunks)}\")\n",
    "    for i, chunk in enumerate(recursive_chunks[:2]):\n",
    "        print(f\"块 {i+1}: {chunk.page_content[:100]}...\")\n",
    "\n",
    "    # 2.2 字符分割器\n",
    "    print(\"\\n2.2 CharacterTextSplitter\")\n",
    "    char_splitter = CharacterTextSplitter(\n",
    "        chunk_size=300,\n",
    "        chunk_overlap=50,\n",
    "        separator=\"\\n\\n\"\n",
    "    )\n",
    "\n",
    "    char_chunks = char_splitter.split_documents([long_doc])\n",
    "    print(f\"字符分割块数: {len(char_chunks)}\")\n",
    "\n",
    "    # 2.3 Token分割器\n",
    "    print(\"\\n2.3 TokenTextSplitter\")\n",
    "    token_splitter = TokenTextSplitter(\n",
    "        chunk_size=100,\n",
    "        chunk_overlap=20\n",
    "    )\n",
    "\n",
    "    token_chunks = token_splitter.split_documents([long_doc])\n",
    "    print(f\"Token分割块数: {len(token_chunks)}\")\n",
    "\n",
    "    # 2.4 Markdown分割器\n",
    "    print(\"\\n2.4 MarkdownHeaderTextSplitter\")\n",
    "    markdown_text = \"\"\"\n",
    "# 人工智能概述\n",
    "\n",
    "## 什么是人工智能\n",
    "人工智能是计算机科学的一个分支。\n",
    "\n",
    "## AI的应用领域\n",
    "\n",
    "### 自然语言处理\n",
    "NLP是AI的重要分支。\n",
    "\n",
    "### 计算机视觉\n",
    "计算机视觉让机器能够\"看见\"。\n",
    "\n",
    "## 未来发展\n",
    "AI将继续快速发展。\n",
    "\"\"\"\n",
    "\n",
    "    markdown_splitter = MarkdownHeaderTextSplitter(\n",
    "        headers_to_split_on=[\n",
    "            (\"#\", \"Header 1\"),\n",
    "            (\"##\", \"Header 2\"),\n",
    "            (\"###\", \"Header 3\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    md_doc = Document(page_content=markdown_text)\n",
    "    md_chunks = markdown_splitter.split_text(markdown_text)\n",
    "    print(f\"Markdown分割块数: {len(md_chunks)}\")\n",
    "\n",
    "    return recursive_chunks\n",
    "\n",
    "# 3. Embedding Models 示例\n",
    "def embedding_models_example():\n",
    "    \"\"\"嵌入模型示例\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"3. Embedding Models 嵌入模型示例\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # 3.1 Ollama嵌入模型\n",
    "    print(\"\\n3.1 Ollama嵌入模型\")\n",
    "    try:\n",
    "        embeddings = OllamaEmbeddings(\n",
    "            base_url=\"http://localhost:11434\",\n",
    "            model=\"nomic-embed-text\"  # 或使用其他嵌入模型\n",
    "        )\n",
    "\n",
    "        # 测试文本\n",
    "        texts = [\n",
    "            \"人工智能是计算机科学的分支\",\n",
    "            \"机器学习是AI的子集\",\n",
    "            \"深度学习使用神经网络\",\n",
    "            \"今天天气很好\"\n",
    "        ]\n",
    "\n",
    "        # 生成嵌入向量\n",
    "        text_embeddings = embeddings.embed_documents(texts)\n",
    "        query_embedding = embeddings.embed_query(\"什么是人工智能？\")\n",
    "\n",
    "        print(f\"文档嵌入数量: {len(text_embeddings)}\")\n",
    "        print(f\"嵌入向量维度: {len(text_embeddings[0])}\")\n",
    "        print(f\"查询嵌入维度: {len(query_embedding)}\")\n",
    "\n",
    "        # 计算相似度\n",
    "        import numpy as np\n",
    "\n",
    "        def cosine_similarity(a, b):\n",
    "            return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "\n",
    "        print(\"\\n相似度计算:\")\n",
    "        for i, text in enumerate(texts):\n",
    "            similarity = cosine_similarity(query_embedding, text_embeddings[i])\n",
    "            print(f\"'{text}' 相似度: {similarity:.4f}\")\n",
    "\n",
    "        return embeddings\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Ollama嵌入模型初始化失败: {e}\")\n",
    "        print(\"请确保Ollama服务正在运行并安装了嵌入模型\")\n",
    "        return None\n",
    "\n",
    "# 4. Vector Stores 示例\n",
    "def vector_stores_example(chunks: List[Document], embeddings):\n",
    "    \"\"\"向量存储示例\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"4. Vector Stores 向量存储示例\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    if embeddings is None:\n",
    "        print(\"跳过向量存储示例（嵌入模型不可用）\")\n",
    "        return None, None\n",
    "\n",
    "    # 4.1 FAISS向量存储\n",
    "    print(\"\\n4.1 FAISS向量存储\")\n",
    "    try:\n",
    "        # 创建FAISS向量存储\n",
    "        faiss_vectorstore = FAISS.from_documents(chunks, embeddings)\n",
    "\n",
    "        # 保存到本地\n",
    "        faiss_vectorstore.save_local(\"faiss_index\")\n",
    "        print(\"✅ FAISS索引已保存\")\n",
    "\n",
    "        # 相似性搜索\n",
    "        query = \"人工智能的发展\"\n",
    "        similar_docs = faiss_vectorstore.similarity_search(query, k=3)\n",
    "\n",
    "        print(f\"\\n查询: '{query}'\")\n",
    "        print(\"相似文档:\")\n",
    "        for i, doc in enumerate(similar_docs):\n",
    "            print(f\"{i+1}. {doc.page_content[:100]}...\")\n",
    "\n",
    "        # 带分数的相似性搜索\n",
    "        similar_docs_with_scores = faiss_vectorstore.similarity_search_with_score(query, k=3)\n",
    "        print(\"\\n带分数的搜索结果:\")\n",
    "        for i, (doc, score) in enumerate(similar_docs_with_scores):\n",
    "            print(f\"{i+1}. 分数: {score:.4f} - {doc.page_content[:80]}...\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"FAISS创建失败: {e}\")\n",
    "        faiss_vectorstore = None\n",
    "\n",
    "    # 4.2 Chroma向量存储\n",
    "    print(\"\\n4.2 Chroma向量存储\")\n",
    "    try:\n",
    "        chroma_vectorstore = Chroma.from_documents(\n",
    "            chunks,\n",
    "            embeddings,\n",
    "            persist_directory=\"./chroma_db\"\n",
    "        )\n",
    "\n",
    "        # 持久化\n",
    "        chroma_vectorstore.persist()\n",
    "        print(\"✅ Chroma数据库已持久化\")\n",
    "\n",
    "        # 搜索测试\n",
    "        chroma_results = chroma_vectorstore.similarity_search(\"机器学习\", k=2)\n",
    "        print(f\"Chroma搜索结果数量: {len(chroma_results)}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Chroma创建失败: {e}\")\n",
    "        chroma_vectorstore = None\n",
    "\n",
    "    return faiss_vectorstore, chroma_vectorstore\n",
    "\n",
    "# 5. Retrievers 示例\n",
    "def retrievers_example(vectorstore, chunks: List[Document]):\n",
    "    \"\"\"检索器示例\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"5. Retrievers 检索器示例\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # 5.1 向量存储检索器\n",
    "    print(\"\\n5.1 向量存储检索器\")\n",
    "    if vectorstore:\n",
    "        vector_retriever = vectorstore.as_retriever(\n",
    "            search_type=\"similarity\",\n",
    "            search_kwargs={\"k\": 3}\n",
    "        )\n",
    "\n",
    "        results = vector_retriever.invoke(\"人工智能的应用\")\n",
    "        print(f\"向量检索结果数量: {len(results)}\")\n",
    "        for i, doc in enumerate(results):\n",
    "            print(f\"{i+1}. {doc.page_content[:100]}...\")\n",
    "\n",
    "    # 5.2 BM25检索器\n",
    "    print(\"\\n5.2 BM25检索器\")\n",
    "    try:\n",
    "        bm25_retriever = BM25Retriever.from_documents(chunks)\n",
    "        bm25_retriever.k = 3\n",
    "\n",
    "        bm25_results = bm25_retriever.invoke(\"人工智能发展\")\n",
    "        print(f\"BM25检索结果数量: {len(bm25_results)}\")\n",
    "        for i, doc in enumerate(bm25_results):\n",
    "            print(f\"{i+1}. {doc.page_content[:100]}...\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"BM25检索器创建失败: {e}\")\n",
    "        bm25_retriever = None\n",
    "\n",
    "    # 5.3 集成检索器\n",
    "    print(\"\\n5.3 集成检索器\")\n",
    "    if vectorstore and bm25_retriever:\n",
    "        try:\n",
    "            ensemble_retriever = EnsembleRetriever(\n",
    "                retrievers=[vector_retriever, bm25_retriever],\n",
    "                weights=[0.7, 0.3]  # 向量搜索权重0.7，BM25权重0.3\n",
    "            )\n",
    "\n",
    "            ensemble_results = ensemble_retriever.invoke(\"机器学习技术\")\n",
    "            print(f\"集成检索结果数量: {len(ensemble_results)}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"集成检索器创建失败: {e}\")\n",
    "\n",
    "    # 5.4 多查询检索器\n",
    "    print(\"\\n5.4 多查询检索器\")\n",
    "    if vectorstore:\n",
    "        try:\n",
    "            llm = ChatOllama(\n",
    "                base_url=\"http://localhost:11434\",\n",
    "                model=\"qwen2.5:3b\"\n",
    "            )\n",
    "\n",
    "            multi_query_retriever = MultiQueryRetriever.from_llm(\n",
    "                retriever=vector_retriever,\n",
    "                llm=llm\n",
    "            )\n",
    "\n",
    "            multi_results = multi_query_retriever.invoke(\"AI的未来发展趋势\")\n",
    "            print(f\"多查询检索结果数量: {len(multi_results)}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"多查询检索器创建失败: {e}\")\n",
    "\n",
    "# 6. 完整RAG流程示例\n",
    "def complete_rag_example():\n",
    "    \"\"\"完整的RAG流程示例\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"6. 完整RAG流程示例\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    try:\n",
    "        # 6.1 准备数据\n",
    "        documents = [\n",
    "            Document(page_content=\"LangChain是一个用于构建LLM应用的框架\", metadata={\"source\": \"doc1\"}),\n",
    "            Document(page_content=\"向量数据库可以存储和检索高维向量\", metadata={\"source\": \"doc2\"}),\n",
    "            Document(page_content=\"RAG结合了检索和生成，提高了AI回答的准确性\", metadata={\"source\": \"doc3\"}),\n",
    "            Document(page_content=\"嵌入模型将文本转换为数值向量表示\", metadata={\"source\": \"doc4\"})\n",
    "        ]\n",
    "\n",
    "        # 6.2 文本分割\n",
    "        splitter = RecursiveCharacterTextSplitter(chunk_size=100, chunk_overlap=20)\n",
    "        chunks = splitter.split_documents(documents)\n",
    "\n",
    "        # 6.3 创建嵌入和向量存储\n",
    "        embeddings = OllamaEmbeddings(\n",
    "            base_url=\"http://localhost:11434\",\n",
    "            model=\"nomic-embed-text\"\n",
    "        )\n",
    "\n",
    "        vectorstore = FAISS.from_documents(chunks, embeddings)\n",
    "        retriever = vectorstore.as_retriever(k=2)\n",
    "\n",
    "        # 6.4 创建RAG链\n",
    "        from langchain_core.prompts import ChatPromptTemplate\n",
    "        from langchain_core.output_parsers import StrOutputParser\n",
    "        from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "        llm = ChatOllama(\n",
    "            base_url=\"http://localhost:11434\",\n",
    "            model=\"qwen2.5:3b\"\n",
    "        )\n",
    "\n",
    "        prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "        基于以下上下文回答问题：\n",
    "\n",
    "        上下文：{context}\n",
    "\n",
    "        问题：{question}\n",
    "\n",
    "        请提供准确、简洁的回答：\n",
    "        \"\"\")\n",
    "\n",
    "        def format_docs(docs):\n",
    "            return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "        rag_chain = (\n",
    "            {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "            | prompt\n",
    "            | llm\n",
    "            | StrOutputParser()\n",
    "        )\n",
    "\n",
    "        # 6.5 测试RAG系统\n",
    "        questions = [\n",
    "            \"什么是LangChain？\",\n",
    "            \"向量数据库的作用是什么？\",\n",
    "            \"RAG技术有什么优势？\"\n",
    "        ]\n",
    "\n",
    "        for question in questions:\n",
    "            print(f\"\\n问题: {question}\")\n",
    "            answer = rag_chain.invoke(question)\n",
    "            print(f\"回答: {answer}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"RAG流程执行失败: {e}\")\n",
    "\n",
    "# 7. 高级功能示例\n",
    "def advanced_features_example():\n",
    "    \"\"\"高级功能示例\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"7. 高级功能示例\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # 7.1 自定义文档加载器\n",
    "    print(\"\\n7.1 自定义文档加载器\")\n",
    "\n",
    "    class CustomLoader:\n",
    "        def __init__(self, data_source):\n",
    "            self.data_source = data_source\n",
    "\n",
    "        def load(self):\n",
    "            # 模拟从API或数据库加载数据\n",
    "            documents = []\n",
    "            for i, item in enumerate(self.data_source):\n",
    "                doc = Document(\n",
    "                    page_content=item[\"content\"],\n",
    "                    metadata={\"id\": i, \"type\": item[\"type\"]}\n",
    "                )\n",
    "                documents.append(doc)\n",
    "            return documents\n",
    "\n",
    "    # 使用自定义加载器\n",
    "    custom_data = [\n",
    "        {\"content\": \"Python是一种编程语言\", \"type\": \"技术\"},\n",
    "        {\"content\": \"数据科学需要统计知识\", \"type\": \"科学\"},\n",
    "        {\"content\": \"机器学习算法很重要\", \"type\": \"AI\"}\n",
    "    ]\n",
    "\n",
    "    custom_loader = CustomLoader(custom_data)\n",
    "    custom_docs = custom_loader.load()\n",
    "    print(f\"自定义加载器文档数量: {len(custom_docs)}\")\n",
    "\n",
    "    # 7.2 文档过滤和预处理\n",
    "    print(\"\\n7.2 文档过滤和预处理\")\n",
    "\n",
    "    def preprocess_documents(documents):\n",
    "        \"\"\"文档预处理函数\"\"\"\n",
    "        processed_docs = []\n",
    "        for doc in documents:\n",
    "            # 清理文本\n",
    "            content = doc.page_content.strip()\n",
    "            content = content.replace(\"\\n\", \" \").replace(\"\\t\", \" \")\n",
    "\n",
    "            # 过滤短文档\n",
    "            if len(content) > 10:\n",
    "                doc.page_content = content\n",
    "                processed_docs.append(doc)\n",
    "\n",
    "        return processed_docs\n",
    "\n",
    "    processed_docs = preprocess_documents(custom_docs)\n",
    "    print(f\"预处理后文档数量: {len(processed_docs)}\")\n",
    "\n",
    "# 8. 性能优化示例\n",
    "async def performance_optimization_example():\n",
    "    \"\"\"性能优化示例\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"8. 性能优化示例\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # 8.1 批量处理\n",
    "    print(\"\\n8.1 批量嵌入处理\")\n",
    "\n",
    "    try:\n",
    "        embeddings = OllamaEmbeddings(\n",
    "            base_url=\"http://localhost:11434\",\n",
    "            model=\"nomic-embed-text\"\n",
    "        )\n",
    "\n",
    "        # 大量文本\n",
    "        texts = [f\"这是第{i}个文档的内容\" for i in range(10)]\n",
    "\n",
    "        # 批量生成嵌入\n",
    "        batch_embeddings = embeddings.embed_documents(texts)\n",
    "        print(f\"批量处理文档数量: {len(batch_embeddings)}\")\n",
    "\n",
    "        # 8.2 异步处理\n",
    "        print(\"\\n8.2 异步处理示例\")\n",
    "\n",
    "        async def async_embed_text(text):\n",
    "            # 模拟异步嵌入\n",
    "            await asyncio.sleep(0.1)\n",
    "            return embeddings.embed_query(text)\n",
    "\n",
    "        # 并发处理\n",
    "        tasks = [async_embed_text(f\"异步文本{i}\") for i in range(5)]\n",
    "        results = await asyncio.gather(*tasks)\n",
    "        print(f\"异步处理结果数量: {len(results)}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"性能优化示例失败: {e}\")\n",
    "\n",
    "# 主函数\n",
    "def main():\n",
    "    \"\"\"运行所有示例\"\"\"\n",
    "    print(\"🚀 LangChain 0.3 Data Connection 完整示例\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    # 1. 文档加载\n",
    "    documents = document_loaders_example()\n",
    "\n",
    "    # 2. 文本分割\n",
    "    chunks = text_splitters_example(documents)\n",
    "\n",
    "    # 3. 嵌入模型\n",
    "    embeddings = embedding_models_example()\n",
    "\n",
    "    # 4. 向量存储\n",
    "    faiss_store, chroma_store = vector_stores_example(chunks, embeddings)\n",
    "\n",
    "    # 5. 检索器\n",
    "    retrievers_example(faiss_store, chunks)\n",
    "\n",
    "    # 6. 完整RAG流程\n",
    "    complete_rag_example()\n",
    "\n",
    "    # 7. 高级功能\n",
    "    advanced_features_example()\n",
    "\n",
    "    print(\"\\n🎉 所有示例运行完成！\")\n",
    "\n",
    "    # 清理临时文件\n",
    "    cleanup_files()\n",
    "\n",
    "def cleanup_files():\n",
    "    \"\"\"清理临时文件\"\"\"\n",
    "    import shutil\n",
    "\n",
    "    files_to_remove = [\"sample.txt\", \"sample.csv\", \"sample.json\"]\n",
    "    dirs_to_remove = [\"docs\", \"faiss_index\", \"chroma_db\"]\n",
    "\n",
    "    for file in files_to_remove:\n",
    "        if os.path.exists(file):\n",
    "            os.remove(file)\n",
    "\n",
    "    for dir in dirs_to_remove:\n",
    "        if os.path.exists(dir):\n",
    "            shutil.rmtree(dir)\n",
    "\n",
    "    print(\"🧹 临时文件已清理\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 运行同步示例\n",
    "    main()\n",
    "\n",
    "    # 运行异步示例\n",
    "    # asyncio.run(performance_optimization_example())"
   ],
   "id": "1588268b0cb2662"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
