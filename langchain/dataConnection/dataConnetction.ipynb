{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Data Connection 核心组件",
   "id": "d2588d34d8fa6662"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "* Data Connection 是 LangChain 中处理外部数据的核心模块，包含以下主要组件：\n",
    "* Document Loaders - 文档加载器\n",
    "* Text Splitters - 文本分割器\n",
    "* Embedding Models - 嵌入模型\n",
    "* Vector Stores - 向量存储\n",
    "* Retrievers - 检索器"
   ],
   "id": "1d00345d4349eba6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T08:40:09.251671Z",
     "start_time": "2025-07-23T08:40:08.277635Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "LangChain 0.3 Data Connection 完整示例\n",
    "包含文档加载、文本分割、向量化、存储和检索的完整流程\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "from typing import List, Dict, Any\n",
    "import asyncio\n",
    "\n",
    "# 核心导入\n",
    "from langchain_community.document_loaders import (\n",
    "    TextLoader,\n",
    "    PyPDFLoader,\n",
    "    CSVLoader,\n",
    "    JSONLoader,\n",
    "    WebBaseLoader,\n",
    "    DirectoryLoader\n",
    ")\n",
    "from langchain.text_splitter import (\n",
    "    RecursiveCharacterTextSplitter,\n",
    "    CharacterTextSplitter,\n",
    "    TokenTextSplitter,\n",
    "    MarkdownHeaderTextSplitter\n",
    ")\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_community.vectorstores import (\n",
    "    FAISS,\n",
    "    Chroma,\n",
    "    Qdrant\n",
    ")\n",
    "from langchain_core.documents import Document\n",
    "from langchain.retrievers import (\n",
    "    BM25Retriever,\n",
    "    EnsembleRetriever,\n",
    "    MultiQueryRetriever\n",
    ")\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "print(\"✅ 所有库导入成功\")"
   ],
   "id": "ed791ba631680edb",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 所有库导入成功\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 1. Document Loaders 示例",
   "id": "e09a684636812a07"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 1. 文本文件加载器",
   "id": "5a516641a4f1e1d0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T03:18:31.722562Z",
     "start_time": "2025-07-23T03:18:31.715330Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_core.documents import Document\n",
    "import os\n",
    "\n",
    "def text_loader_examples():\n",
    "    \"\"\"文本文件加载器示例\"\"\"\n",
    "\n",
    "    # 1.1 基础文本加载\n",
    "    with open(\"sample.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"人工智能是计算机科学的一个分支。\\n机器学习是AI的子集。\")\n",
    "\n",
    "    loader = TextLoader(\"sample.txt\", encoding=\"utf-8\")\n",
    "    documents = loader.load()\n",
    "    print(f\"文档数量: {len(documents)}\")\n",
    "    print(f\"内容: {documents[0].page_content}\")\n",
    "    print(f\"元数据: {documents[0].metadata}\")\n",
    "\n",
    "    # # 1.2 处理大文件\n",
    "    # loader_large = TextLoader(\"large_file.txt\", encoding=\"utf-8\")\n",
    "    # try:\n",
    "    #     docs = loader_large.load()\n",
    "    #     print(f\"大文件加载成功，文档数: {len(docs)}\")\n",
    "    # except Exception as e:\n",
    "    #     print(f\"加载失败: {e}\")\n",
    "\n",
    "    # # 1.3 自动编码检测\n",
    "    # loader_auto = TextLoader(\"file.txt\", autodetect_encoding=True)\n",
    "    # docs = loader_auto.load()\n",
    "text_loader_examples()"
   ],
   "id": "b21efe1b5e41c515",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "文档数量: 1\n",
      "内容: 人工智能是计算机科学的一个分支。\n",
      "机器学习是AI的子集。\n",
      "元数据: {'source': 'sample.txt'}\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 2. PDF 文档加载器",
   "id": "7be3a16ae64b0f31"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T03:25:40.450347Z",
     "start_time": "2025-07-23T03:25:36.529217Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader, PDFMinerLoader, PDFPlumberLoader\n",
    "\n",
    "def pdf_loader_examples():\n",
    "    \"\"\"PDF加载器示例\"\"\"\n",
    "\n",
    "    # 2.1 PyPDFLoader - 最常用\n",
    "    pdf_loader = PyPDFLoader(\"docs/Multi-level Wavelet-CNN for Image Restoration.pdf\")\n",
    "    pages = pdf_loader.load()\n",
    "    print(f\"PDF页数: {len(pages)}\")\n",
    "\n",
    "    for i, page in enumerate(pages[:2]):\n",
    "        print(f\"第{i+1}页内容: {page.page_content[:100]}...\")\n",
    "        print(f\"页面元数据: {page.metadata}\")\n",
    "\n",
    "    # 2.2 PDFMinerLoader - 更好的文本提取\n",
    "    pdf_miner_loader = PDFMinerLoader(\"docs/Multi-level Wavelet-CNN for Image Restoration.pdf\")\n",
    "    docs = pdf_miner_loader.load()\n",
    "\n",
    "    # 2.3 PDFPlumberLoader - 表格处理更好\n",
    "    pdf_plumber_loader = PDFPlumberLoader(\"docs/Multi-level Wavelet-CNN for Image Restoration.pdf\")\n",
    "    docs = pdf_plumber_loader.load()\n",
    "\n",
    "    # 2.4 分页加载\n",
    "    pdf_loader = PyPDFLoader(\"docs/Multi-level Wavelet-CNN for Image Restoration.pdf\")\n",
    "    pages = pdf_loader.load_and_split()\n",
    "\n",
    "    # # 2.5 密码保护的PDF\n",
    "    # protected_loader = PyPDFLoader(\"docs/Multi-level Wavelet-CNN for Image Restoration.pdf\", password=\"password123\")\n",
    "    # docs = protected_loader.load()\n",
    "\n",
    "pdf_loader_examples()"
   ],
   "id": "31f4771d335c1e14",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF页数: 10\n",
      "第1页内容: Multi-level Wavelet-CNN for Image Restoration\n",
      "Pengju Liu1, Hongzhi Zhang ∗1, Kai Zhang1, Liang Lin2,...\n",
      "页面元数据: {'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2018-05-23T00:19:39+00:00', 'author': '', 'keywords': '', 'moddate': '2018-05-23T00:19:39+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'docs/Multi-level Wavelet-CNN for Image Restoration.pdf', 'total_pages': 10, 'page': 0, 'page_label': '1'}\n",
      "第2页内容: is adopted to enlarge receptive ﬁeld without the sacriﬁce\n",
      "of computational cost. Dilated ﬁltering, h...\n",
      "页面元数据: {'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2018-05-23T00:19:39+00:00', 'author': '', 'keywords': '', 'moddate': '2018-05-23T00:19:39+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'docs/Multi-level Wavelet-CNN for Image Restoration.pdf', 'total_pages': 10, 'page': 1, 'page_label': '2'}\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 3. CSV 数据加载器",
   "id": "e13650d820fdd7bd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from langchain_community.document_loaders import CSVLoader\n",
    "import pandas as pd\n",
    "\n",
    "def csv_loader_examples():\n",
    "    \"\"\"CSV加载器示例\"\"\"\n",
    "\n",
    "    # 创建示例CSV\n",
    "    df = pd.DataFrame({\n",
    "        'name': ['张三', '李四', '王五'],\n",
    "        'age': [25, 30, 35],\n",
    "        'department': ['技术部', '销售部', '市场部'],\n",
    "        'description': ['Python开发工程师', '销售经理', '市场专员']\n",
    "    })\n",
    "    df.to_csv(\"employees.csv\", index=False, encoding=\"utf-8\")\n",
    "\n",
    "    # 3.1 基础CSV加载\n",
    "    csv_loader = CSVLoader(\"employees.csv\", encoding=\"utf-8\")\n",
    "    docs = csv_loader.load()\n",
    "    print(f\"CSV文档数量: {len(docs)}\")\n",
    "    print(f\"第一条记录: {docs[0].page_content}\")\n",
    "\n",
    "    # 3.2 指定源列\n",
    "    csv_loader_with_source = CSVLoader(\n",
    "        \"employees.csv\",\n",
    "        source_column=\"name\",\n",
    "        encoding=\"utf-8\"\n",
    "    )\n",
    "    docs = csv_loader_with_source.load()\n",
    "\n",
    "    # 3.3 自定义CSV参数\n",
    "    csv_loader_custom = CSVLoader(\n",
    "        \"employees.csv\",\n",
    "        csv_args={\n",
    "            'delimiter': ',',\n",
    "            'quotechar': '\"',\n",
    "            'fieldnames': ['姓名', '年龄', '部门', '描述']\n",
    "        }\n",
    "    )\n",
    "    docs = csv_loader_custom.load()\n",
    "\n",
    "    # 3.4 过滤特定列\n",
    "    csv_loader_filtered = CSVLoader(\n",
    "        \"employees.csv\",\n",
    "        content_columns=['name', 'description'],\n",
    "        encoding=\"utf-8\"\n",
    "    )\n",
    "    docs = csv_loader_filtered.load()\n",
    "csv_loader_examples()"
   ],
   "id": "e6361d2440059a5f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 4. JSON 数据加载器",
   "id": "9ded7230b64c406b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from langchain_community.document_loaders import JSONLoader\n",
    "import json\n",
    "\n",
    "def json_loader_examples():\n",
    "    \"\"\"JSON加载器示例\"\"\"\n",
    "\n",
    "    # 创建示例JSON数据\n",
    "    data = [\n",
    "        {\n",
    "            \"id\": 1,\n",
    "            \"title\": \"Python编程指南\",\n",
    "            \"content\": \"Python是一种高级编程语言，语法简洁优雅。\",\n",
    "            \"author\": \"张三\",\n",
    "            \"tags\": [\"编程\", \"Python\", \"教程\"]\n",
    "        },\n",
    "        {\n",
    "            \"id\": 2,\n",
    "            \"title\": \"机器学习入门\",\n",
    "            \"content\": \"机器学习是人工智能的一个重要分支。\",\n",
    "            \"author\": \"李四\",\n",
    "            \"tags\": [\"AI\", \"机器学习\", \"数据科学\"]\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    with open(\"articles.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    # 4.1 提取特定字段\n",
    "    json_loader = JSONLoader(\n",
    "        \"articles.json\",\n",
    "        jq_schema=\".[].content\",\n",
    "        text_content=False\n",
    "    )\n",
    "    docs = json_loader.load()\n",
    "    print(f\"JSON文档数量: {len(docs)}\")\n",
    "\n",
    "    # 4.2 提取多个字段\n",
    "    json_loader_multi = JSONLoader(\n",
    "        \"articles.json\",\n",
    "        jq_schema=\".[]\",\n",
    "        content_key=\"content\"\n",
    "    )\n",
    "    docs = json_loader_multi.load()\n",
    "\n",
    "    # 4.3 复杂JSON结构\n",
    "    complex_data = {\n",
    "        \"articles\": {\n",
    "            \"tech\": [\n",
    "                {\"title\": \"AI发展\", \"body\": \"人工智能快速发展\"},\n",
    "                {\"title\": \"云计算\", \"body\": \"云计算改变了IT架构\"}\n",
    "            ],\n",
    "            \"business\": [\n",
    "                {\"title\": \"数字化转型\", \"body\": \"企业数字化转型势在必行\"}\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "\n",
    "    with open(\"complex.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(complex_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    # 提取嵌套数据\n",
    "    json_loader_nested = JSONLoader(\n",
    "        \"complex.json\",\n",
    "        jq_schema=\".articles.tech[].body\"\n",
    "    )\n",
    "    docs = json_loader_nested.load()\n",
    "\n",
    "    # 4.4 JSONL格式\n",
    "    jsonl_data = [\n",
    "        {\"text\": \"第一行数据\", \"label\": \"A\"},\n",
    "        {\"text\": \"第二行数据\", \"label\": \"B\"}\n",
    "    ]\n",
    "\n",
    "    with open(\"data.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
    "        for item in jsonl_data:\n",
    "            f.write(json.dumps(item, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "    from langchain_community.document_loaders import JSONLinesLoader\n",
    "    jsonl_loader = JSONLinesLoader(\"data.jsonl\", jq_schema=\".text\")\n",
    "    docs = jsonl_loader.load()"
   ],
   "id": "c7f3dd2d242dc05b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 5. 网页内容加载器",
   "id": "3d0a379fe033b34"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader, AsyncHtmlLoader\n",
    "from langchain_community.document_transformers import Html2TextTransformer\n",
    "\n",
    "def web_loader_examples():\n",
    "    \"\"\"网页加载器示例\"\"\"\n",
    "\n",
    "    # 5.1 基础网页加载\n",
    "    web_loader = WebBaseLoader(\"https://example.com\")\n",
    "    docs = web_loader.load()\n",
    "    print(f\"网页文档: {docs[0].page_content[:200]}...\")\n",
    "\n",
    "    # 5.2 多个URL批量加载\n",
    "    urls = [\n",
    "        \"https://example.com/page1\",\n",
    "        \"https://example.com/page2\",\n",
    "        \"https://example.com/page3\"\n",
    "    ]\n",
    "    web_loader_multi = WebBaseLoader(urls)\n",
    "    docs = web_loader_multi.load()\n",
    "\n",
    "    # 5.3 自定义请求头\n",
    "    web_loader_headers = WebBaseLoader(\n",
    "        \"https://api.example.com/data\",\n",
    "        header_template={\n",
    "            \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\",\n",
    "            \"Authorization\": \"Bearer your-token\"\n",
    "        }\n",
    "    )\n",
    "    docs = web_loader_headers.load()\n",
    "\n",
    "    # 5.4 CSS选择器过滤\n",
    "    from bs4 import BeautifulSoup\n",
    "\n",
    "    web_loader_css = WebBaseLoader(\n",
    "        \"https://news.example.com\",\n",
    "        bs_kwargs={\n",
    "            \"parse_only\": BeautifulSoup.SoupStrainer(\"div\", {\"class\": \"article-content\"})\n",
    "        }\n",
    "    )\n",
    "    docs = web_loader_css.load()\n",
    "\n",
    "    # 5.5 异步网页加载\n",
    "    async def async_web_loading():\n",
    "        urls = [\"https://example.com/1\", \"https://example.com/2\"]\n",
    "        async_loader = AsyncHtmlLoader(urls)\n",
    "        html_docs = async_loader.load()\n",
    "\n",
    "        # HTML转文本\n",
    "        html2text = Html2TextTransformer()\n",
    "        text_docs = html2text.transform_documents(html_docs)\n",
    "        return text_docs\n",
    "\n",
    "    # 5.6 处理JavaScript渲染页面\n",
    "    from langchain_community.document_loaders import SeleniumURLLoader\n",
    "\n",
    "    selenium_loader = SeleniumURLLoader(\n",
    "        urls=[\"https://spa-example.com\"],\n",
    "        browser=\"chrome\",\n",
    "        headless=True\n",
    "    )\n",
    "    docs = selenium_loader.load()"
   ],
   "id": "be4ec69a05ebcd5c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 6. 目录批量加载器",
   "id": "9f794fef8c49c6a6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "from langchain_community.document_loaders import TextLoader, PyPDFLoader, CSVLoader\n",
    "\n",
    "def directory_loader_examples():\n",
    "    \"\"\"目录加载器示例\"\"\"\n",
    "\n",
    "    # 创建测试目录结构\n",
    "    os.makedirs(\"documents/texts\", exist_ok=True)\n",
    "    os.makedirs(\"documents/pdfs\", exist_ok=True)\n",
    "    os.makedirs(\"documents/data\", exist_ok=True)\n",
    "\n",
    "    # 创建测试文件\n",
    "    for i in range(3):\n",
    "        with open(f\"documents/texts/doc_{i}.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(f\"这是文档{i}的内容，包含重要信息。\")\n",
    "\n",
    "    # 6.1 加载特定类型文件\n",
    "    txt_loader = DirectoryLoader(\n",
    "        \"documents/texts\",\n",
    "        glob=\"*.txt\",\n",
    "        loader_cls=TextLoader,\n",
    "        loader_kwargs={\"encoding\": \"utf-8\"}\n",
    "    )\n",
    "    txt_docs = txt_loader.load()\n",
    "    print(f\"文本文档数量: {len(txt_docs)}\")\n",
    "\n",
    "    # 6.2 多种文件类型混合加载\n",
    "    from langchain_community.document_loaders import UnstructuredFileLoader\n",
    "\n",
    "    mixed_loader = DirectoryLoader(\n",
    "        \"documents\",\n",
    "        glob=\"**/*\",  # 递归搜索\n",
    "        loader_cls=UnstructuredFileLoader,\n",
    "        recursive=True,\n",
    "        show_progress=True\n",
    "    )\n",
    "    mixed_docs = mixed_loader.load()\n",
    "\n",
    "    # 6.3 自定义文件类型映射\n",
    "    def get_loader_for_file(file_path: str):\n",
    "        if file_path.endswith('.txt'):\n",
    "            return TextLoader(file_path, encoding=\"utf-8\")\n",
    "        elif file_path.endswith('.pdf'):\n",
    "            return PyPDFLoader(file_path)\n",
    "        elif file_path.endswith('.csv'):\n",
    "            return CSVLoader(file_path, encoding=\"utf-8\")\n",
    "        else:\n",
    "            return UnstructuredFileLoader(file_path)\n",
    "\n",
    "    # 6.4 过滤和排除文件\n",
    "    filtered_loader = DirectoryLoader(\n",
    "        \"documents\",\n",
    "        glob=\"*.txt\",\n",
    "        exclude=[\"temp_*\", \"*.tmp\"],\n",
    "        loader_cls=TextLoader,\n",
    "        loader_kwargs={\"encoding\": \"utf-8\"}\n",
    "    )\n",
    "    filtered_docs = filtered_loader.load()\n",
    "\n",
    "    # 6.5 并行加载\n",
    "    parallel_loader = DirectoryLoader(\n",
    "        \"documents\",\n",
    "        glob=\"**/*\",\n",
    "        loader_cls=UnstructuredFileLoader,\n",
    "        use_multithreading=True,\n",
    "        max_concurrency=4\n",
    "    )\n",
    "    parallel_docs = parallel_loader.load()"
   ],
   "id": "d450f59175737713"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 7. 数据库加载器",
   "id": "dc1720b2b32a1d88"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from langchain_community.document_loaders import SQLDatabaseLoader\n",
    "from sqlalchemy import create_engine, text\n",
    "\n",
    "def database_loader_examples():\n",
    "    \"\"\"数据库加载器示例\"\"\"\n",
    "\n",
    "    # 7.1 SQLite数据库加载\n",
    "    engine = create_engine(\"sqlite:///example.db\")\n",
    "\n",
    "    # 创建示例表和数据\n",
    "    with engine.connect() as conn:\n",
    "        conn.execute(text(\"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS articles (\n",
    "                id INTEGER PRIMARY KEY,\n",
    "                title TEXT,\n",
    "                content TEXT,\n",
    "                author TEXT,\n",
    "                created_at TIMESTAMP\n",
    "            )\n",
    "        \"\"\"))\n",
    "\n",
    "        conn.execute(text(\"\"\"\n",
    "            INSERT OR REPLACE INTO articles VALUES\n",
    "            (1, 'Python教程', 'Python是一种编程语言', '张三', '2024-01-01'),\n",
    "            (2, 'AI发展', '人工智能快速发展', '李四', '2024-01-02')\n",
    "        \"\"\"))\n",
    "        conn.commit()\n",
    "\n",
    "    # 加载数据库内容\n",
    "    db_loader = SQLDatabaseLoader(\n",
    "        query=\"SELECT title, content, author FROM articles\",\n",
    "        db=engine,\n",
    "        page_content_columns=[\"title\", \"content\"],\n",
    "        metadata_columns=[\"author\"]\n",
    "    )\n",
    "    docs = db_loader.load()\n",
    "    print(f\"数据库文档数量: {len(docs)}\")\n",
    "\n",
    "    # 7.2 PostgreSQL示例\n",
    "    # pg_engine = create_engine(\"postgresql://user:password@localhost/dbname\")\n",
    "    # pg_loader = SQLDatabaseLoader(\n",
    "    #     query=\"SELECT * FROM documents WHERE category = 'tech'\",\n",
    "    #     db=pg_engine\n",
    "    # )\n",
    "    # pg_docs = pg_loader.load()\n",
    "\n",
    "    # 7.3 MongoDB加载器\n",
    "    from langchain_community.document_loaders import MongodbLoader\n",
    "\n",
    "    # mongodb_loader = MongodbLoader(\n",
    "    #     connection_string=\"mongodb://localhost:27017/\",\n",
    "    #     db_name=\"mydb\",\n",
    "    #     collection_name=\"documents\",\n",
    "    #     filter_criteria={\"status\": \"published\"}\n",
    "    # )\n",
    "    # mongo_docs = mongodb_loader.load()"
   ],
   "id": "b58217d2dad099d5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 8. 云存储加载器",
   "id": "16cfd1e55688358"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def cloud_storage_examples():\n",
    "    \"\"\"云存储加载器示例\"\"\"\n",
    "\n",
    "    # 8.1 AWS S3加载器\n",
    "    from langchain_community.document_loaders import S3DirectoryLoader, S3FileLoader\n",
    "\n",
    "    # 单个S3文件\n",
    "    s3_file_loader = S3FileLoader(\n",
    "        bucket=\"my-bucket\",\n",
    "        key=\"documents/report.pdf\"\n",
    "    )\n",
    "    s3_docs = s3_file_loader.load()\n",
    "\n",
    "    # S3目录\n",
    "    s3_dir_loader = S3DirectoryLoader(\n",
    "        bucket=\"my-bucket\",\n",
    "        prefix=\"documents/\",\n",
    "        aws_access_key_id=\"your-access-key\",\n",
    "        aws_secret_access_key=\"your-secret-key\"\n",
    "    )\n",
    "    s3_dir_docs = s3_dir_loader.load()\n",
    "\n",
    "    # 8.2 Google Drive加载器\n",
    "    from langchain_community.document_loaders import GoogleDriveLoader\n",
    "\n",
    "    # gdrive_loader = GoogleDriveLoader(\n",
    "    #     folder_id=\"your-folder-id\",\n",
    "    #     credentials_path=\"path/to/credentials.json\",\n",
    "    #     token_path=\"path/to/token.json\"\n",
    "    # )\n",
    "    # gdrive_docs = gdrive_loader.load()\n",
    "\n",
    "    # 8.3 Azure Blob Storage\n",
    "    from langchain_community.document_loaders import AzureBlobStorageContainerLoader\n",
    "\n",
    "    # azure_loader = AzureBlobStorageContainerLoader(\n",
    "    #     conn_str=\"your-connection-string\",\n",
    "    #     container=\"documents\"\n",
    "    # )\n",
    "    # azure_docs = azure_loader.load()"
   ],
   "id": "cc9720e9a8356866"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 9. 自定义文档加载器",
   "id": "9da531146d384b92"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from langchain_core.document_loaders import BaseLoader\n",
    "from langchain_core.documents import Document\n",
    "from typing import List, Iterator\n",
    "import requests\n",
    "\n",
    "class CustomAPILoader(BaseLoader):\n",
    "    \"\"\"自定义API加载器\"\"\"\n",
    "\n",
    "    def __init__(self, api_url: str, headers: dict = None):\n",
    "        self.api_url = api_url\n",
    "        self.headers = headers or {}\n",
    "\n",
    "    def load(self) -> List[Document]:\n",
    "        \"\"\"加载文档\"\"\"\n",
    "        response = requests.get(self.api_url, headers=self.headers)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        data = response.json()\n",
    "        documents = []\n",
    "\n",
    "        for item in data.get('items', []):\n",
    "            doc = Document(\n",
    "                page_content=item.get('content', ''),\n",
    "                metadata={\n",
    "                    'source': self.api_url,\n",
    "                    'id': item.get('id'),\n",
    "                    'title': item.get('title'),\n",
    "                    'timestamp': item.get('created_at')\n",
    "                }\n",
    "            )\n",
    "            documents.append(doc)\n",
    "\n",
    "        return documents\n",
    "\n",
    "    def lazy_load(self) -> Iterator[Document]:\n",
    "        \"\"\"懒加载文档\"\"\"\n",
    "        response = requests.get(self.api_url, headers=self.headers)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        data = response.json()\n",
    "        for item in data.get('items', []):\n",
    "            yield Document(\n",
    "                page_content=item.get('content', ''),\n",
    "                metadata={\n",
    "                    'source': self.api_url,\n",
    "                    'id': item.get('id'),\n",
    "                    'title': item.get('title')\n",
    "                }\n",
    "            )\n",
    "\n",
    "class DatabaseStreamLoader(BaseLoader):\n",
    "    \"\"\"流式数据库加载器\"\"\"\n",
    "\n",
    "    def __init__(self, connection_string: str, query: str, batch_size: int = 1000):\n",
    "        self.connection_string = connection_string\n",
    "        self.query = query\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def lazy_load(self) -> Iterator[Document]:\n",
    "        \"\"\"分批加载大量数据\"\"\"\n",
    "        from sqlalchemy import create_engine, text\n",
    "\n",
    "        engine = create_engine(self.connection_string)\n",
    "        offset = 0\n",
    "\n",
    "        while True:\n",
    "            paginated_query = f\"{self.query} LIMIT {self.batch_size} OFFSET {offset}\"\n",
    "\n",
    "            with engine.connect() as conn:\n",
    "                result = conn.execute(text(paginated_query))\n",
    "                rows = result.fetchall()\n",
    "\n",
    "                if not rows:\n",
    "                    break\n",
    "\n",
    "                for row in rows:\n",
    "                    yield Document(\n",
    "                        page_content=str(row[1]),  # 假设第二列是内容\n",
    "                        metadata={\n",
    "                            'id': row[0],  # 假设第一列是ID\n",
    "                            'source': 'database',\n",
    "                            'batch': offset // self.batch_size\n",
    "                        }\n",
    "                    )\n",
    "\n",
    "                offset += self.batch_size\n",
    "\n",
    "def custom_loader_examples():\n",
    "    \"\"\"自定义加载器使用示例\"\"\"\n",
    "\n",
    "    # 使用自定义API加载器\n",
    "    api_loader = CustomAPILoader(\n",
    "        api_url=\"https://api.example.com/articles\",\n",
    "        headers={\"Authorization\": \"Bearer your-token\"}\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        api_docs = api_loader.load()\n",
    "        print(f\"API文档数量: {len(api_docs)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"API加载失败: {e}\")\n",
    "\n",
    "    # 使用流式数据库加载器\n",
    "    db_stream_loader = DatabaseStreamLoader(\n",
    "        connection_string=\"sqlite:///large_db.db\",\n",
    "        query=\"SELECT id, content FROM large_table\",\n",
    "        batch_size=500\n",
    "    )\n",
    "\n",
    "    # 懒加载处理大量数据\n",
    "    for i, doc in enumerate(db_stream_loader.lazy_load()):\n",
    "        if i >= 10:  # 只处理前10个文档作为示例\n",
    "            break\n",
    "        print(f\"文档 {i}: {doc.page_content[:50]}...\")"
   ],
   "id": "6705f6f795ac930b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 10. 完整使用示例",
   "id": "2ff9e60626b983a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def complete_document_loader_example():\n",
    "    \"\"\"完整的文档加载器使用示例\"\"\"\n",
    "\n",
    "    print(\"🚀 LangChain 0.3 Document Loaders 完整示例\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    all_documents = []\n",
    "\n",
    "    # 1. 文本文件\n",
    "    print(\"\\n📄 加载文本文件...\")\n",
    "    text_docs = text_loader_examples()\n",
    "    all_documents.extend(text_docs)\n",
    "\n",
    "    # 2. CSV数据\n",
    "    print(\"\\n📊 加载CSV数据...\")\n",
    "    csv_docs = csv_loader_examples()\n",
    "    all_documents.extend(csv_docs)\n",
    "\n",
    "    # 3. JSON数据\n",
    "    print(\"\\n🔧 加载JSON数据...\")\n",
    "    json_docs = json_loader_examples()\n",
    "    all_documents.extend(json_docs)\n",
    "\n",
    "    # 4. 目录批量加载\n",
    "    print(\"\\n📁 批量加载目录...\")\n",
    "    dir_docs = directory_loader_examples()\n",
    "    all_documents.extend(dir_docs)\n",
    "\n",
    "    # 5. 数据库加载\n",
    "    print(\"\\n🗄️ 加载数据库...\")\n",
    "    db_docs = database_loader_examples()\n",
    "    all_documents.extend(db_docs)\n",
    "\n",
    "    # 6. 自定义加载器\n",
    "    print(\"\\n⚙️ 自定义加载器...\")\n",
    "    custom_docs = custom_loader_examples()\n",
    "\n",
    "    # 统计信息\n",
    "    print(f\"\\n📈 加载统计:\")\n",
    "    print(f\"总文档数量: {len(all_documents)}\")\n",
    "\n",
    "    # 按来源分组\n",
    "    sources = {}\n",
    "    for doc in all_documents:\n",
    "        source = doc.metadata.get('source', 'unknown')\n",
    "        sources[source] = sources.get(source, 0) + 1\n",
    "\n",
    "    print(\"按来源分布:\")\n",
    "    for source, count in sources.items():\n",
    "        print(f\"  {source}: {count} 个文档\")\n",
    "\n",
    "    # 内容长度统计\n",
    "    lengths = [len(doc.page_content) for doc in all_documents]\n",
    "    if lengths:\n",
    "        print(f\"内容长度统计:\")\n",
    "        print(f\"  平均长度: {sum(lengths) / len(lengths):.0f} 字符\")\n",
    "        print(f\"  最短: {min(lengths)} 字符\")\n",
    "        print(f\"  最长: {max(lengths)} 字符\")\n",
    "\n",
    "    return all_documents\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    documents = complete_document_loader_example()\n",
    "\n",
    "    # 清理临时文件\n",
    "    import shutil\n",
    "    for path in [\"sample.txt\", \"employees.csv\", \"articles.json\", \"documents\"]:\n",
    "        if os.path.exists(path):\n",
    "            if os.path.isdir(path):\n",
    "                shutil.rmtree(path)\n",
    "            else:\n",
    "                os.remove(path)\n",
    "\n",
    "    print(\"\\n🧹 临时文件已清理\")"
   ],
   "id": "555928003009f244"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 总结\n",
    "1. LangChain 0.3 的 Document Loaders 提供了丰富的数据源支持：\n",
    "#### 主要特点：\n",
    "3. 统一的 Document 接口\n",
    "4. 丰富的文件格式支持\n",
    "5. 云存储集成\n",
    "6. 自定义加载器扩展\n",
    "7. 批量和流式处理\n",
    "8. 元数据保留\n",
    "\n",
    "#### 选择建议：\n",
    "10. 简单文本：使用 TextLoader\n",
    "11. PDF文档：推荐 PyPDFLoader\n",
    "12. 结构化数据：使用 CSVLoader 或 JSONLoader\n",
    "13. 网页内容：使用 WebBaseLoader\n",
    "14. 大量文件：使用 DirectoryLoader\n",
    "15. 云存储：使用对应的云存储加载器\n",
    "16. 特殊需求：实现自定义加载器"
   ],
   "id": "bd2a088707c0f236"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T03:31:32.029515Z",
     "start_time": "2025-07-23T03:31:32.012522Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# 1. Document Loaders 示例\n",
    "def document_loaders_example():\n",
    "    \"\"\"文档加载器示例\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"1. Document Loaders 文档加载器示例\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # 1.1 文本文件加载\n",
    "    print(\"\\n1.1 文本文件加载\")\n",
    "    # 创建示例文本文件\n",
    "    with open(\"sample.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"\"\"\n",
    "        人工智能（AI）是计算机科学的一个分支，致力于创建能够执行通常需要人类智能的任务的系统。\n",
    "        机器学习是AI的一个子集，它使计算机能够从数据中学习而无需明确编程。\n",
    "        深度学习是机器学习的一个子集，使用神经网络来模拟人脑的工作方式。\n",
    "        \"\"\")\n",
    "\n",
    "    loader = TextLoader(\"sample.txt\", encoding=\"utf-8\")\n",
    "    documents = loader.load()\n",
    "    print(f\"加载的文档数量: {len(documents)}\")\n",
    "    print(f\"文档内容预览: {documents[0].page_content[:100]}...\")\n",
    "\n",
    "    # 1.2 CSV文件加载\n",
    "    print(\"\\n1.2 CSV文件加载\")\n",
    "    import pandas as pd\n",
    "\n",
    "    # 创建示例CSV\n",
    "    df = pd.DataFrame({\n",
    "        'name': ['张三', '李四', '王五'],\n",
    "        'age': [25, 30, 35],\n",
    "        'city': ['北京', '上海', '深圳'],\n",
    "        'description': ['软件工程师', '数据科学家', '产品经理']\n",
    "    })\n",
    "    df.to_csv(\"sample.csv\", index=False, encoding=\"utf-8\")\n",
    "\n",
    "    csv_loader = CSVLoader(\"sample.csv\", encoding=\"utf-8\")\n",
    "    csv_docs = csv_loader.load()\n",
    "    print(f\"CSV文档数量: {len(csv_docs)}\")\n",
    "    print(f\"CSV文档示例: {csv_docs[0].page_content}\")\n",
    "\n",
    "    # 1.3 JSON文件加载\n",
    "    print(\"\\n1.3 JSON文件加载\")\n",
    "    import json\n",
    "\n",
    "    sample_data = [\n",
    "        {\"title\": \"Python编程\", \"content\": \"Python是一种高级编程语言\", \"category\": \"技术\"},\n",
    "        {\"title\": \"数据分析\", \"content\": \"数据分析是从数据中提取洞察的过程\", \"category\": \"数据科学\"}\n",
    "    ]\n",
    "\n",
    "    with open(\"sample.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(sample_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    json_loader = JSONLoader(\"sample.json\", jq_schema=\".[].content\")\n",
    "    json_docs = json_loader.load()\n",
    "    print(f\"JSON文档数量: {len(json_docs)}\")\n",
    "    print(f\"JSON文档示例: {json_docs[0].page_content}\")\n",
    "\n",
    "    # 1.4 目录批量加载\n",
    "    print(\"\\n1.4 目录批量加载\")\n",
    "    os.makedirs(\"docs\", exist_ok=True)\n",
    "\n",
    "    # 创建多个文档\n",
    "    for i in range(3):\n",
    "        with open(f\"docs/doc_{i}.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(f\"这是第{i + 1}个文档的内容。包含关于技术{i + 1}的详细信息。\")\n",
    "\n",
    "    dir_loader = DirectoryLoader(\"docs\", glob=\"*.txt\",\n",
    "                                 loader_cls=TextLoader,\n",
    "                                 loader_kwargs={\"encoding\": \"utf-8\"})\n",
    "    dir_docs = dir_loader.load()\n",
    "    print(f\"目录文档数量: {len(dir_docs)}\")\n",
    "\n",
    "    return documents + csv_docs + json_docs + dir_docs\n",
    "\n",
    "documents = document_loaders_example()"
   ],
   "id": "ba3eef49f7f1be9e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "1. Document Loaders 文档加载器示例\n",
      "============================================================\n",
      "\n",
      "1.1 文本文件加载\n",
      "加载的文档数量: 1\n",
      "文档内容预览: \n",
      "        人工智能（AI）是计算机科学的一个分支，致力于创建能够执行通常需要人类智能的任务的系统。\n",
      "        机器学习是AI的一个子集，它使计算机能够从数据中学习而无需明确编程。\n",
      "   ...\n",
      "\n",
      "1.2 CSV文件加载\n",
      "CSV文档数量: 3\n",
      "CSV文档示例: name: 张三\n",
      "age: 25\n",
      "city: 北京\n",
      "description: 软件工程师\n",
      "\n",
      "1.3 JSON文件加载\n",
      "JSON文档数量: 2\n",
      "JSON文档示例: Python是一种高级编程语言\n",
      "\n",
      "1.4 目录批量加载\n",
      "目录文档数量: 3\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 2. Text Splitters 示例",
   "id": "5fd3ca1111ba6401"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "LangChain 0.3 Text Splitters 完整示例\n",
    "包含所有主要分割器类型和高级用法\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "from typing import List, Dict, Any\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain.text_splitter import (\n",
    "    RecursiveCharacterTextSplitter,\n",
    "    CharacterTextSplitter,\n",
    "    TokenTextSplitter,\n",
    "    MarkdownHeaderTextSplitter,\n",
    "    HTMLHeaderTextSplitter,\n",
    "    PythonCodeTextSplitter,\n",
    "    LatexTextSplitter\n",
    ")\n",
    "from langchain_core.documents import Document"
   ],
   "id": "ce7e6218653160d3",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 创建示例文档用于测试",
   "id": "9892fb6b1e4a7dae"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T04:37:15.626366Z",
     "start_time": "2025-07-23T04:37:15.620363Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "def create_sample_documents():\n",
    "    \"\"\"创建示例文档用于测试\"\"\"\n",
    "\n",
    "    # 长文本示例\n",
    "    long_text = \"\"\"\n",
    "人工智能的发展历程可以追溯到20世纪50年代。当时，计算机科学家开始探索让机器模拟人类智能的可能性。\n",
    "\n",
    "在1956年的达特茅斯会议上，人工智能这个术语首次被正式提出。这标志着AI作为一个独立学科的诞生。\n",
    "\n",
    "随后的几十年里，AI经历了多次起伏。60-70年代是第一个AI春天，专家系统得到了广泛应用。\n",
    "\n",
    "80年代末到90年代初，由于技术限制和过高期望，AI进入了所谓的\"AI冬天\"。\n",
    "\n",
    "21世纪以来，随着大数据、云计算和深度学习的发展，AI迎来了新的春天。\n",
    "\n",
    "今天，AI已经在图像识别、自然语言处理、推荐系统等领域取得了突破性进展。\n",
    "\n",
    "机器学习作为AI的核心技术，包括监督学习、无监督学习和强化学习三大类。深度学习则是机器学习的一个重要分支。\n",
    "\n",
    "自然语言处理（NLP）让计算机能够理解和生成人类语言。计算机视觉让机器能够\"看见\"和理解图像。\n",
    "\n",
    "未来，AI将在更多领域发挥重要作用，包括医疗、教育、交通、金融等。同时，AI的伦理和安全问题也需要得到重视。\n",
    "    \"\"\"\n",
    "\n",
    "    return Document(page_content=long_text.strip(), metadata={\"source\": \"ai_history\"})\n",
    "create_sample_documents()"
   ],
   "id": "88469d08a6c00c3",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': 'ai_history'}, page_content='人工智能的发展历程可以追溯到20世纪50年代。当时，计算机科学家开始探索让机器模拟人类智能的可能性。\\n\\n在1956年的达特茅斯会议上，人工智能这个术语首次被正式提出。这标志着AI作为一个独立学科的诞生。\\n\\n随后的几十年里，AI经历了多次起伏。60-70年代是第一个AI春天，专家系统得到了广泛应用。\\n\\n80年代末到90年代初，由于技术限制和过高期望，AI进入了所谓的\"AI冬天\"。\\n\\n21世纪以来，随着大数据、云计算和深度学习的发展，AI迎来了新的春天。\\n\\n今天，AI已经在图像识别、自然语言处理、推荐系统等领域取得了突破性进展。\\n\\n机器学习作为AI的核心技术，包括监督学习、无监督学习和强化学习三大类。深度学习则是机器学习的一个重要分支。\\n\\n自然语言处理（NLP）让计算机能够理解和生成人类语言。计算机视觉让机器能够\"看见\"和理解图像。\\n\\n未来，AI将在更多领域发挥重要作用，包括医疗、教育、交通、金融等。同时，AI的伦理和安全问题也需要得到重视。')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 递归字符分割器示例 - 推荐使用",
   "id": "9d86747bd818921f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T04:38:06.056307Z",
     "start_time": "2025-07-23T04:38:06.048370Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "def recursive_character_splitter_example():\n",
    "    \"\"\"递归字符分割器示例 - 推荐使用\"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"1. RecursiveCharacterTextSplitter（推荐）\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    doc = create_sample_documents()\n",
    "\n",
    "    # 1.1 基础用法\n",
    "    print(\"\\n1.1 基础递归分割\")\n",
    "    basic_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=200,           # 块大小\n",
    "        chunk_overlap=50,         # 重叠大小\n",
    "        length_function=len,      # 长度计算函数\n",
    "        is_separator_regex=False  # 分隔符是否为正则表达式\n",
    "    )\n",
    "\n",
    "    basic_chunks = basic_splitter.split_documents([doc])\n",
    "    print(f\"基础分割块数: {len(basic_chunks)}\")\n",
    "    for i, chunk in enumerate(basic_chunks[:3]):\n",
    "        print(f\"块 {i+1} (长度: {len(chunk.page_content)}): {chunk.page_content[:80]}...\")\n",
    "\n",
    "    # 1.2 自定义分隔符\n",
    "    print(\"\\n1.2 自定义分隔符优先级\")\n",
    "    custom_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=150,\n",
    "        chunk_overlap=30,\n",
    "        separators=[\n",
    "            \"\\n\\n\",    # 段落分隔符（最高优先级）\n",
    "            \"\\n\",      # 行分隔符\n",
    "            \"。\",      # 中文句号\n",
    "            \"！\",      # 中文感叹号\n",
    "            \"？\",      # 中文问号\n",
    "            \"，\",      # 中文逗号\n",
    "            \" \",       # 空格\n",
    "            \"\"         # 字符级分割（最后手段）\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    custom_chunks = custom_splitter.split_documents([doc])\n",
    "    print(f\"自定义分割块数: {len(custom_chunks)}\")\n",
    "\n",
    "    # 1.3 保持段落完整性\n",
    "    print(\"\\n1.3 段落优先分割\")\n",
    "    paragraph_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=300,\n",
    "        chunk_overlap=50,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \"。\", \" \", \"\"]\n",
    "    )\n",
    "\n",
    "    para_chunks = paragraph_splitter.split_documents([doc])\n",
    "    print(f\"段落分割块数: {len(para_chunks)}\")\n",
    "\n",
    "    return basic_chunks\n",
    "recursive_character_splitter_example()"
   ],
   "id": "ce030a9b3e7cb3b3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "1. RecursiveCharacterTextSplitter（推荐）\n",
      "============================================================\n",
      "\n",
      "1.1 基础递归分割\n",
      "基础分割块数: 3\n",
      "块 1 (长度: 190): 人工智能的发展历程可以追溯到20世纪50年代。当时，计算机科学家开始探索让机器模拟人类智能的可能性。\n",
      "\n",
      "在1956年的达特茅斯会议上，人工智能这个术语首次被正式...\n",
      "块 2 (长度: 169): 80年代末到90年代初，由于技术限制和过高期望，AI进入了所谓的\"AI冬天\"。\n",
      "\n",
      "21世纪以来，随着大数据、云计算和深度学习的发展，AI迎来了新的春天。\n",
      "\n",
      "今天...\n",
      "块 3 (长度: 103): 自然语言处理（NLP）让计算机能够理解和生成人类语言。计算机视觉让机器能够\"看见\"和理解图像。\n",
      "\n",
      "未来，AI将在更多领域发挥重要作用，包括医疗、教育、交通、金融...\n",
      "\n",
      "1.2 自定义分隔符优先级\n",
      "自定义分割块数: 4\n",
      "\n",
      "1.3 段落优先分割\n",
      "段落分割块数: 2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'ai_history'}, page_content='人工智能的发展历程可以追溯到20世纪50年代。当时，计算机科学家开始探索让机器模拟人类智能的可能性。\\n\\n在1956年的达特茅斯会议上，人工智能这个术语首次被正式提出。这标志着AI作为一个独立学科的诞生。\\n\\n随后的几十年里，AI经历了多次起伏。60-70年代是第一个AI春天，专家系统得到了广泛应用。\\n\\n80年代末到90年代初，由于技术限制和过高期望，AI进入了所谓的\"AI冬天\"。'),\n",
       " Document(metadata={'source': 'ai_history'}, page_content='80年代末到90年代初，由于技术限制和过高期望，AI进入了所谓的\"AI冬天\"。\\n\\n21世纪以来，随着大数据、云计算和深度学习的发展，AI迎来了新的春天。\\n\\n今天，AI已经在图像识别、自然语言处理、推荐系统等领域取得了突破性进展。\\n\\n机器学习作为AI的核心技术，包括监督学习、无监督学习和强化学习三大类。深度学习则是机器学习的一个重要分支。'),\n",
       " Document(metadata={'source': 'ai_history'}, page_content='自然语言处理（NLP）让计算机能够理解和生成人类语言。计算机视觉让机器能够\"看见\"和理解图像。\\n\\n未来，AI将在更多领域发挥重要作用，包括医疗、教育、交通、金融等。同时，AI的伦理和安全问题也需要得到重视。')]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 字符文本分割器示例",
   "id": "9842d0500ccd9820"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "def character_text_splitter_example():\n",
    "    \"\"\"字符文本分割器示例\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"2. CharacterTextSplitter\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    doc = create_sample_documents()\n",
    "\n",
    "    # 2.1 按段落分割\n",
    "    print(\"\\n2.1 按段落分割\")\n",
    "    para_splitter = CharacterTextSplitter(\n",
    "        chunk_size=300,\n",
    "        chunk_overlap=50,\n",
    "        separator=\"\\n\\n\"  # 只使用段落分隔符\n",
    "    )\n",
    "\n",
    "    para_chunks = para_splitter.split_documents([doc])\n",
    "    print(f\"段落分割块数: {len(para_chunks)}\")\n",
    "\n",
    "    # 2.2 按句子分割\n",
    "    print(\"\\n2.2 按句子分割\")\n",
    "    sentence_splitter = CharacterTextSplitter(\n",
    "        chunk_size=100,\n",
    "        chunk_overlap=20,\n",
    "        separator=\"。\"  # 按中文句号分割\n",
    "    )\n",
    "\n",
    "    sentence_chunks = sentence_splitter.split_documents([doc])\n",
    "    print(f\"句子分割块数: {len(sentence_chunks)}\")\n",
    "\n",
    "    # 2.3 自定义分隔符\n",
    "    print(\"\\n2.3 自定义分隔符\")\n",
    "    custom_text = \"项目A|项目B|项目C|项目D|项目E的详细描述和分析报告\"\n",
    "    custom_doc = Document(page_content=custom_text)\n",
    "\n",
    "    custom_splitter = CharacterTextSplitter(\n",
    "        chunk_size=20,\n",
    "        chunk_overlap=0,\n",
    "        separator=\"|\"\n",
    "    )\n",
    "\n",
    "    custom_chunks = custom_splitter.split_documents([custom_doc])\n",
    "    print(f\"自定义分割块数: {len(custom_chunks)}\")\n",
    "    for chunk in custom_chunks:\n",
    "        print(f\"  - {chunk.page_content}\")\n",
    "\n",
    "    return para_chunks\n",
    "character_text_splitter_example()"
   ],
   "id": "e757e63d91a4e4fc",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Token文本分割器示例",
   "id": "6041fe05918b6c54"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T04:39:44.020099Z",
     "start_time": "2025-07-23T04:39:44.010096Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "def token_text_splitter_example():\n",
    "    \"\"\"Token文本分割器示例\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"3. TokenTextSplitter\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    doc = create_sample_documents()\n",
    "\n",
    "    # 3.1 基础Token分割\n",
    "    print(\"\\n3.1 基础Token分割\")\n",
    "    token_splitter = TokenTextSplitter(\n",
    "        chunk_size=100,      # Token数量\n",
    "        chunk_overlap=20,    # 重叠Token数\n",
    "        model_name=\"gpt-3.5-turbo\"  # 指定tokenizer模型\n",
    "    )\n",
    "\n",
    "    token_chunks = token_splitter.split_documents([doc])\n",
    "    print(f\"Token分割块数: {len(token_chunks)}\")\n",
    "\n",
    "    # 显示每块的实际token数\n",
    "    for i, chunk in enumerate(token_chunks[:3]):\n",
    "        token_count = token_splitter._tokenizer.encode(chunk.page_content)\n",
    "        print(f\"块 {i+1} Token数: {len(token_count)}, 内容: {chunk.page_content[:60]}...\")\n",
    "\n",
    "    # 3.2 不同模型的Token分割\n",
    "    print(\"\\n3.2 不同模型Token分割对比\")\n",
    "    models = [\"gpt-3.5-turbo\", \"text-davinci-003\", \"gpt-4\"]\n",
    "\n",
    "    for model in models:\n",
    "        try:\n",
    "            model_splitter = TokenTextSplitter(\n",
    "                chunk_size=50,\n",
    "                chunk_overlap=10,\n",
    "                model_name=model\n",
    "            )\n",
    "            model_chunks = model_splitter.split_documents([doc])\n",
    "            print(f\"{model}: {len(model_chunks)} 块\")\n",
    "        except Exception as e:\n",
    "            print(f\"{model}: 不支持 ({e})\")\n",
    "\n",
    "    return token_chunks\n",
    "token_text_splitter_example()"
   ],
   "id": "6f3d4d1dc5ea686e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "3. TokenTextSplitter\n",
      "============================================================\n",
      "\n",
      "3.1 基础Token分割\n",
      "Token分割块数: 6\n",
      "块 1 Token数: 100, 内容: 人工智能的发展历程可以追溯到20世纪50年代。当时，计算机科学家开始探索让机器模拟人类智能的可能性。\n",
      "\n",
      "在1956年的达...\n",
      "块 2 Token数: 100, 内容: 次被正式提出。这标志着AI作为一个独立学科的诞生。\n",
      "\n",
      "随后的几十年里，AI经历了多次起伏。60-70年代是第一个AI春天...\n",
      "块 3 Token数: 100, 内容: 年代初，由于技术限制和过高期望，AI进入了所谓的\"AI冬天\"。\n",
      "\n",
      "21世纪以来，随着大数据、云计算和深度学习的发展，AI...\n",
      "\n",
      "3.2 不同模型Token分割对比\n",
      "gpt-3.5-turbo: 11 块\n",
      "text-davinci-003: 21 块\n",
      "gpt-4: 11 块\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'ai_history'}, page_content='人工智能的发展历程可以追溯到20世纪50年代。当时，计算机科学家开始探索让机器模拟人类智能的可能性。\\n\\n在1956年的达特茅斯会议上，人工智能这个术语首次被正式提出。这标志着AI作为一个独'),\n",
       " Document(metadata={'source': 'ai_history'}, page_content='次被正式提出。这标志着AI作为一个独立学科的诞生。\\n\\n随后的几十年里，AI经历了多次起伏。60-70年代是第一个AI春天，专家系统得到了广泛应用。\\n\\n80年代末到90年代初，由于技术限制和过高期望，AI进'),\n",
       " Document(metadata={'source': 'ai_history'}, page_content='年代初，由于技术限制和过高期望，AI进入了所谓的\"AI冬天\"。\\n\\n21世纪以来，随着大数据、云计算和深度学习的发展，AI迎来了新的春天。\\n\\n今天，AI已经在图像识别、自然语言处理、推荐系统等领'),\n",
       " Document(metadata={'source': 'ai_history'}, page_content='在图像识别、自然语言处理、推荐系统等领域取得了突破性进展。\\n\\n机器学习作为AI的核心技术，包括监督学习、无监督学习和强化学习三大类。深度学习则是机器学习的一个重要分支。\\n\\n自然语'),\n",
       " Document(metadata={'source': 'ai_history'}, page_content='学习则是机器学习的一个重要分支。\\n\\n自然语言处理（NLP）让计算机能够理解和生成人类语言。计算机视觉让机器能够\"看见\"和理解图像。\\n\\n未来，AI将在更多领域发挥重要作用，包括医疗、教育、'),\n",
       " Document(metadata={'source': 'ai_history'}, page_content='�重要作用，包括医疗、教育、交通、金融等。同时，AI的伦理和安全问题也需要得到重视。')]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Markdown标题分割器示例",
   "id": "aad363dd25cc64f7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T04:39:47.529630Z",
     "start_time": "2025-07-23T04:39:47.521482Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "def markdown_header_splitter_example():\n",
    "    \"\"\"Markdown标题分割器示例\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"4. MarkdownHeaderTextSplitter\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # 创建Markdown文档\n",
    "    markdown_text = \"\"\"\n",
    "# 人工智能技术指南\n",
    "\n",
    "## 1. 机器学习基础\n",
    "\n",
    "### 1.1 监督学习\n",
    "监督学习是机器学习的一个重要分支，使用标记的训练数据来学习输入到输出的映射。\n",
    "\n",
    "常见算法包括：\n",
    "- 线性回归\n",
    "- 逻辑回归\n",
    "- 决策树\n",
    "- 随机森林\n",
    "\n",
    "### 1.2 无监督学习\n",
    "无监督学习从未标记的数据中发现隐藏的模式。\n",
    "\n",
    "主要方法：\n",
    "- 聚类分析\n",
    "- 降维技术\n",
    "- 关联规则挖掘\n",
    "\n",
    "## 2. 深度学习\n",
    "\n",
    "### 2.1 神经网络基础\n",
    "神经网络是深度学习的基础，模拟人脑神经元的工作方式。\n",
    "\n",
    "### 2.2 卷积神经网络\n",
    "CNN主要用于图像处理和计算机视觉任务。\n",
    "\n",
    "### 2.3 循环神经网络\n",
    "RNN适合处理序列数据，如文本和时间序列。\n",
    "\n",
    "## 3. 自然语言处理\n",
    "\n",
    "### 3.1 文本预处理\n",
    "包括分词、词性标注、命名实体识别等步骤。\n",
    "\n",
    "### 3.2 语言模型\n",
    "从统计语言模型到现代的Transformer模型。\n",
    "\n",
    "# 总结\n",
    "\n",
    "人工智能技术正在快速发展，各个领域都有重要突破。\n",
    "\"\"\"\n",
    "\n",
    "    # 4.1 基础标题分割\n",
    "    print(\"\\n4.1 基础标题分割\")\n",
    "    md_splitter = MarkdownHeaderTextSplitter(\n",
    "        headers_to_split_on=[\n",
    "            (\"#\", \"Header 1\"),\n",
    "            (\"##\", \"Header 2\"),\n",
    "            (\"###\", \"Header 3\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    md_chunks = md_splitter.split_text(markdown_text)\n",
    "    print(f\"Markdown分割块数: {len(md_chunks)}\")\n",
    "\n",
    "    for i, chunk in enumerate(md_chunks[:5]):\n",
    "        print(f\"\\n块 {i+1}:\")\n",
    "        print(f\"内容: {chunk.page_content[:100]}...\")\n",
    "        print(f\"元数据: {chunk.metadata}\")\n",
    "\n",
    "    # 4.2 结合递归分割器\n",
    "    print(\"\\n4.2 结合递归分割器进行二次分割\")\n",
    "\n",
    "    # 先按标题分割\n",
    "    md_docs = md_splitter.split_text(markdown_text)\n",
    "\n",
    "    # 再对长块进行递归分割\n",
    "    recursive_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=200,\n",
    "        chunk_overlap=50\n",
    "    )\n",
    "\n",
    "    final_chunks = []\n",
    "    for doc in md_docs:\n",
    "        if len(doc.page_content) > 200:\n",
    "            sub_chunks = recursive_splitter.split_documents([doc])\n",
    "            final_chunks.extend(sub_chunks)\n",
    "        else:\n",
    "            final_chunks.append(doc)\n",
    "\n",
    "    print(f\"二次分割后块数: {len(final_chunks)}\")\n",
    "\n",
    "    return md_chunks\n",
    "markdown_header_splitter_example()"
   ],
   "id": "16cc12ea512e6025",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "4. MarkdownHeaderTextSplitter\n",
      "============================================================\n",
      "\n",
      "4.1 基础标题分割\n",
      "Markdown分割块数: 8\n",
      "\n",
      "块 1:\n",
      "内容: 监督学习是机器学习的一个重要分支，使用标记的训练数据来学习输入到输出的映射。  \n",
      "常见算法包括：\n",
      "- 线性回归\n",
      "- 逻辑回归\n",
      "- 决策树\n",
      "- 随机森林...\n",
      "元数据: {'Header 1': '人工智能技术指南', 'Header 2': '1. 机器学习基础', 'Header 3': '1.1 监督学习'}\n",
      "\n",
      "块 2:\n",
      "内容: 无监督学习从未标记的数据中发现隐藏的模式。  \n",
      "主要方法：\n",
      "- 聚类分析\n",
      "- 降维技术\n",
      "- 关联规则挖掘...\n",
      "元数据: {'Header 1': '人工智能技术指南', 'Header 2': '1. 机器学习基础', 'Header 3': '1.2 无监督学习'}\n",
      "\n",
      "块 3:\n",
      "内容: 神经网络是深度学习的基础，模拟人脑神经元的工作方式。...\n",
      "元数据: {'Header 1': '人工智能技术指南', 'Header 2': '2. 深度学习', 'Header 3': '2.1 神经网络基础'}\n",
      "\n",
      "块 4:\n",
      "内容: CNN主要用于图像处理和计算机视觉任务。...\n",
      "元数据: {'Header 1': '人工智能技术指南', 'Header 2': '2. 深度学习', 'Header 3': '2.2 卷积神经网络'}\n",
      "\n",
      "块 5:\n",
      "内容: RNN适合处理序列数据，如文本和时间序列。...\n",
      "元数据: {'Header 1': '人工智能技术指南', 'Header 2': '2. 深度学习', 'Header 3': '2.3 循环神经网络'}\n",
      "\n",
      "4.2 结合递归分割器进行二次分割\n",
      "二次分割后块数: 8\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'Header 1': '人工智能技术指南', 'Header 2': '1. 机器学习基础', 'Header 3': '1.1 监督学习'}, page_content='监督学习是机器学习的一个重要分支，使用标记的训练数据来学习输入到输出的映射。  \\n常见算法包括：\\n- 线性回归\\n- 逻辑回归\\n- 决策树\\n- 随机森林'),\n",
       " Document(metadata={'Header 1': '人工智能技术指南', 'Header 2': '1. 机器学习基础', 'Header 3': '1.2 无监督学习'}, page_content='无监督学习从未标记的数据中发现隐藏的模式。  \\n主要方法：\\n- 聚类分析\\n- 降维技术\\n- 关联规则挖掘'),\n",
       " Document(metadata={'Header 1': '人工智能技术指南', 'Header 2': '2. 深度学习', 'Header 3': '2.1 神经网络基础'}, page_content='神经网络是深度学习的基础，模拟人脑神经元的工作方式。'),\n",
       " Document(metadata={'Header 1': '人工智能技术指南', 'Header 2': '2. 深度学习', 'Header 3': '2.2 卷积神经网络'}, page_content='CNN主要用于图像处理和计算机视觉任务。'),\n",
       " Document(metadata={'Header 1': '人工智能技术指南', 'Header 2': '2. 深度学习', 'Header 3': '2.3 循环神经网络'}, page_content='RNN适合处理序列数据，如文本和时间序列。'),\n",
       " Document(metadata={'Header 1': '人工智能技术指南', 'Header 2': '3. 自然语言处理', 'Header 3': '3.1 文本预处理'}, page_content='包括分词、词性标注、命名实体识别等步骤。'),\n",
       " Document(metadata={'Header 1': '人工智能技术指南', 'Header 2': '3. 自然语言处理', 'Header 3': '3.2 语言模型'}, page_content='从统计语言模型到现代的Transformer模型。'),\n",
       " Document(metadata={'Header 1': '总结'}, page_content='人工智能技术正在快速发展，各个领域都有重要突破。')]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### HTML标题分割器示例",
   "id": "40d65927c234df09"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T04:40:33.308628Z",
     "start_time": "2025-07-23T04:40:33.300629Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "def html_header_splitter_example():\n",
    "    \"\"\"HTML标题分割器示例\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"5. HTMLHeaderTextSplitter\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    html_text = \"\"\"\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "<head>\n",
    "    <title>AI技术文档</title>\n",
    "</head>\n",
    "<body>\n",
    "    <h1>人工智能概述</h1>\n",
    "    <p>人工智能是计算机科学的一个分支。</p>\n",
    "\n",
    "    <h2>机器学习</h2>\n",
    "    <p>机器学习是AI的重要组成部分。</p>\n",
    "\n",
    "    <h3>监督学习</h3>\n",
    "    <p>使用标记数据进行训练。</p>\n",
    "\n",
    "    <h3>无监督学习</h3>\n",
    "    <p>从未标记数据中发现模式。</p>\n",
    "\n",
    "    <h2>深度学习</h2>\n",
    "    <p>基于神经网络的学习方法。</p>\n",
    "\n",
    "    <h3>CNN</h3>\n",
    "    <p>卷积神经网络用于图像处理。</p>\n",
    "\n",
    "    <h3>RNN</h3>\n",
    "    <p>循环神经网络处理序列数据。</p>\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "    html_splitter = HTMLHeaderTextSplitter(\n",
    "        headers_to_split_on=[\n",
    "            (\"h1\", \"Header 1\"),\n",
    "            (\"h2\", \"Header 2\"),\n",
    "            (\"h3\", \"Header 3\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    html_chunks = html_splitter.split_text(html_text)\n",
    "    print(f\"HTML分割块数: {len(html_chunks)}\")\n",
    "\n",
    "    for i, chunk in enumerate(html_chunks[:3]):\n",
    "        print(f\"\\n块 {i+1}:\")\n",
    "        print(f\"内容: {chunk.page_content[:80]}...\")\n",
    "        print(f\"元数据: {chunk.metadata}\")\n",
    "\n",
    "    return html_chunks\n",
    "html_header_splitter_example()"
   ],
   "id": "4aa23e32db7c52d0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "5. HTMLHeaderTextSplitter\n",
      "============================================================\n",
      "HTML分割块数: 14\n",
      "\n",
      "块 1:\n",
      "内容: 人工智能概述...\n",
      "元数据: {'Header 1': '人工智能概述'}\n",
      "\n",
      "块 2:\n",
      "内容: 人工智能是计算机科学的一个分支。...\n",
      "元数据: {'Header 1': '人工智能概述'}\n",
      "\n",
      "块 3:\n",
      "内容: 机器学习...\n",
      "元数据: {'Header 1': '人工智能概述', 'Header 2': '机器学习'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'Header 1': '人工智能概述'}, page_content='人工智能概述'),\n",
       " Document(metadata={'Header 1': '人工智能概述'}, page_content='人工智能是计算机科学的一个分支。'),\n",
       " Document(metadata={'Header 1': '人工智能概述', 'Header 2': '机器学习'}, page_content='机器学习'),\n",
       " Document(metadata={'Header 1': '人工智能概述', 'Header 2': '机器学习'}, page_content='机器学习是AI的重要组成部分。'),\n",
       " Document(metadata={'Header 1': '人工智能概述', 'Header 2': '机器学习', 'Header 3': '监督学习'}, page_content='监督学习'),\n",
       " Document(metadata={'Header 1': '人工智能概述', 'Header 2': '机器学习', 'Header 3': '监督学习'}, page_content='使用标记数据进行训练。'),\n",
       " Document(metadata={'Header 1': '人工智能概述', 'Header 2': '机器学习', 'Header 3': '无监督学习'}, page_content='无监督学习'),\n",
       " Document(metadata={'Header 1': '人工智能概述', 'Header 2': '机器学习', 'Header 3': '无监督学习'}, page_content='从未标记数据中发现模式。'),\n",
       " Document(metadata={'Header 1': '人工智能概述', 'Header 2': '深度学习'}, page_content='深度学习'),\n",
       " Document(metadata={'Header 1': '人工智能概述', 'Header 2': '深度学习'}, page_content='基于神经网络的学习方法。'),\n",
       " Document(metadata={'Header 1': '人工智能概述', 'Header 2': '深度学习', 'Header 3': 'CNN'}, page_content='CNN'),\n",
       " Document(metadata={'Header 1': '人工智能概述', 'Header 2': '深度学习', 'Header 3': 'CNN'}, page_content='卷积神经网络用于图像处理。'),\n",
       " Document(metadata={'Header 1': '人工智能概述', 'Header 2': '深度学习', 'Header 3': 'RNN'}, page_content='RNN'),\n",
       " Document(metadata={'Header 1': '人工智能概述', 'Header 2': '深度学习', 'Header 3': 'RNN'}, page_content='循环神经网络处理序列数据。')]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 代码分割器示例",
   "id": "9f6db3d961b5606a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T04:40:54.517430Z",
     "start_time": "2025-07-23T04:40:54.508407Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "def code_splitter_example():\n",
    "    \"\"\"代码分割器示例\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"6. PythonCodeTextSplitter\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    python_code = '''\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "class DataProcessor:\n",
    "    \"\"\"数据处理类\"\"\"\n",
    "\n",
    "    def __init__(self, data_path):\n",
    "        self.data_path = data_path\n",
    "        self.data = None\n",
    "\n",
    "    def load_data(self):\n",
    "        \"\"\"加载数据\"\"\"\n",
    "        self.data = pd.read_csv(self.data_path)\n",
    "        return self.data\n",
    "\n",
    "    def preprocess(self):\n",
    "        \"\"\"数据预处理\"\"\"\n",
    "        # 处理缺失值\n",
    "        self.data = self.data.dropna()\n",
    "\n",
    "        # 特征缩放\n",
    "        from sklearn.preprocessing import StandardScaler\n",
    "        scaler = StandardScaler()\n",
    "        numeric_columns = self.data.select_dtypes(include=[np.number]).columns\n",
    "        self.data[numeric_columns] = scaler.fit_transform(self.data[numeric_columns])\n",
    "\n",
    "        return self.data\n",
    "\n",
    "def train_model(X, y):\n",
    "    \"\"\"训练模型\"\"\"\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "    print(f\"Mean Squared Error: {mse}\")\n",
    "    return model\n",
    "\n",
    "def main():\n",
    "    \"\"\"主函数\"\"\"\n",
    "    processor = DataProcessor(\"data.csv\")\n",
    "    data = processor.load_data()\n",
    "    processed_data = processor.preprocess()\n",
    "\n",
    "    X = processed_data.drop('target', axis=1)\n",
    "    y = processed_data['target']\n",
    "\n",
    "    model = train_model(X, y)\n",
    "    print(\"模型训练完成\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "'''\n",
    "\n",
    "    # Python代码分割\n",
    "    python_splitter = PythonCodeTextSplitter(\n",
    "        chunk_size=500,\n",
    "        chunk_overlap=50\n",
    "    )\n",
    "\n",
    "    code_chunks = python_splitter.split_text(python_code)\n",
    "    print(f\"Python代码分割块数: {len(code_chunks)}\")\n",
    "\n",
    "    for i, chunk in enumerate(code_chunks[:3]):\n",
    "        print(f\"\\n代码块 {i+1} (长度: {len(chunk)}):\")\n",
    "        print(chunk[:200] + \"...\" if len(chunk) > 200 else chunk)\n",
    "\n",
    "    return code_chunks\n",
    "code_splitter_example()"
   ],
   "id": "16e9cd00aaf06ba0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "6. PythonCodeTextSplitter\n",
      "============================================================\n",
      "Python代码分割块数: 5\n",
      "\n",
      "代码块 1 (长度: 188):\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.linear_model import LinearRegression\n",
      "from sklearn.metrics import mean_squared_error\n",
      "\n",
      "代码块 2 (长度: 352):\n",
      "class DataProcessor:\n",
      "    \"\"\"数据处理类\"\"\"\n",
      "\n",
      "    def __init__(self, data_path):\n",
      "        self.data_path = data_path\n",
      "        self.data = None\n",
      "\n",
      "    def load_data(self):\n",
      "        \"\"\"加载数据\"\"\"\n",
      "        self.data = pd...\n",
      "\n",
      "代码块 3 (长度: 288):\n",
      "# 特征缩放\n",
      "        from sklearn.preprocessing import StandardScaler\n",
      "        scaler = StandardScaler()\n",
      "        numeric_columns = self.data.select_dtypes(include=[np.number]).columns\n",
      "        self.data[numer...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['import numpy as np\\nimport pandas as pd\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.linear_model import LinearRegression\\nfrom sklearn.metrics import mean_squared_error',\n",
       " 'class DataProcessor:\\n    \"\"\"数据处理类\"\"\"\\n\\n    def __init__(self, data_path):\\n        self.data_path = data_path\\n        self.data = None\\n\\n    def load_data(self):\\n        \"\"\"加载数据\"\"\"\\n        self.data = pd.read_csv(self.data_path)\\n        return self.data\\n\\n    def preprocess(self):\\n        \"\"\"数据预处理\"\"\"\\n        # 处理缺失值\\n        self.data = self.data.dropna()',\n",
       " '# 特征缩放\\n        from sklearn.preprocessing import StandardScaler\\n        scaler = StandardScaler()\\n        numeric_columns = self.data.select_dtypes(include=[np.number]).columns\\n        self.data[numeric_columns] = scaler.fit_transform(self.data[numeric_columns])\\n\\n        return self.data',\n",
       " 'def train_model(X, y):\\n    \"\"\"训练模型\"\"\"\\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\\n\\n    model = LinearRegression()\\n    model.fit(X_train, y_train)\\n\\n    y_pred = model.predict(X_test)\\n    mse = mean_squared_error(y_test, y_pred)\\n\\n    print(f\"Mean Squared Error: {mse}\")\\n    return model',\n",
       " 'def main():\\n    \"\"\"主函数\"\"\"\\n    processor = DataProcessor(\"data.csv\")\\n    data = processor.load_data()\\n    processed_data = processor.preprocess()\\n\\n    X = processed_data.drop(\\'target\\', axis=1)\\n    y = processed_data[\\'target\\']\\n\\n    model = train_model(X, y)\\n    print(\"模型训练完成\")\\n\\nif __name__ == \"__main__\":\\n    main()']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### LaTeX分割器示例",
   "id": "1c2a67cb0547d28"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T04:41:10.518491Z",
     "start_time": "2025-07-23T04:41:10.512148Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "def latex_splitter_example():\n",
    "    \"\"\"LaTeX分割器示例\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"7. LatexTextSplitter\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    latex_text = r\"\"\"\n",
    "\\documentclass{article}\n",
    "\\usepackage{amsmath}\n",
    "\\title{机器学习数学基础}\n",
    "\\author{AI研究团队}\n",
    "\n",
    "\\begin{document}\n",
    "\\maketitle\n",
    "\n",
    "\\section{线性代数}\n",
    "线性代数是机器学习的数学基础之一。\n",
    "\n",
    "\\subsection{向量}\n",
    "向量是具有大小和方向的量。在机器学习中，特征通常表示为向量。\n",
    "\n",
    "设向量 $\\mathbf{v} = [v_1, v_2, \\ldots, v_n]^T$，其中 $v_i$ 是第 $i$ 个分量。\n",
    "\n",
    "\\subsection{矩阵}\n",
    "矩阵是二维数组，用于表示线性变换。\n",
    "\n",
    "矩阵乘法定义为：\n",
    "\\begin{equation}\n",
    "(\\mathbf{AB})_{ij} = \\sum_{k=1}^{n} a_{ik}b_{kj}\n",
    "\\end{equation}\n",
    "\n",
    "\\section{概率论}\n",
    "概率论为机器学习提供了不确定性建模的工具。\n",
    "\n",
    "\\subsection{贝叶斯定理}\n",
    "贝叶斯定理是概率论的核心：\n",
    "\\begin{equation}\n",
    "P(A|B) = \\frac{P(B|A)P(A)}{P(B)}\n",
    "\\end{equation}\n",
    "\n",
    "\\section{优化理论}\n",
    "优化理论用于寻找模型的最优参数。\n",
    "\n",
    "\\subsection{梯度下降}\n",
    "梯度下降是最常用的优化算法：\n",
    "\\begin{equation}\n",
    "\\theta_{t+1} = \\theta_t - \\alpha \\nabla_\\theta J(\\theta)\n",
    "\\end{equation}\n",
    "\n",
    "其中 $\\alpha$ 是学习率，$J(\\theta)$ 是损失函数。\n",
    "\n",
    "\\end{document}\n",
    "\"\"\"\n",
    "\n",
    "    latex_splitter = LatexTextSplitter(\n",
    "        chunk_size=300,\n",
    "        chunk_overlap=50\n",
    "    )\n",
    "\n",
    "    latex_chunks = latex_splitter.split_text(latex_text)\n",
    "    print(f\"LaTeX分割块数: {len(latex_chunks)}\")\n",
    "\n",
    "    for i, chunk in enumerate(latex_chunks[:3]):\n",
    "        print(f\"\\nLaTeX块 {i+1}:\")\n",
    "        print(chunk[:150] + \"...\" if len(chunk) > 150 else chunk)\n",
    "\n",
    "    return latex_chunks\n",
    "latex_splitter_example()"
   ],
   "id": "4b7e76a91110b69b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "7. LatexTextSplitter\n",
      "============================================================\n",
      "LaTeX分割块数: 4\n",
      "\n",
      "LaTeX块 1:\n",
      "\\documentclass{article}\n",
      "\\usepackage{amsmath}\n",
      "\\title{机器学习数学基础}\n",
      "\\author{AI研究团队}\n",
      "\n",
      "\\begin{document}\n",
      "\\maketitle\n",
      "\n",
      "\\section{线性代数}\n",
      "线性代数是机器学习的数学基础之一。\n",
      "\n",
      "\\subsect...\n",
      "\n",
      "LaTeX块 2:\n",
      "$ 个分量。\n",
      "\n",
      "\\subsection{矩阵}\n",
      "矩阵是二维数组，用于表示线性变换。\n",
      "\n",
      "矩阵乘法定义为：\n",
      "\\begin{equation}\n",
      "(\\mathbf{AB})_{ij} = \\sum_{k=1}^{n} a_{ik}b_{kj}\n",
      "\\end{equation}\n",
      "\n",
      "\\section{概率论}\n",
      "概率...\n",
      "\n",
      "LaTeX块 3:\n",
      "= \\frac{P(B|A)P(A)}{P(B)}\n",
      "\\end{equation}\n",
      "\n",
      "\\section{优化理论}\n",
      "优化理论用于寻找模型的最优参数。\n",
      "\n",
      "\\subsection{梯度下降}\n",
      "梯度下降是最常用的优化算法：\n",
      "\\begin{equation}\n",
      "\\theta_{t+1} = \\theta_t -...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['\\\\documentclass{article}\\n\\\\usepackage{amsmath}\\n\\\\title{机器学习数学基础}\\n\\\\author{AI研究团队}\\n\\n\\\\begin{document}\\n\\\\maketitle\\n\\n\\\\section{线性代数}\\n线性代数是机器学习的数学基础之一。\\n\\n\\\\subsection{向量}\\n向量是具有大小和方向的量。在机器学习中，特征通常表示为向量。\\n\\n设向量 $\\\\mathbf{v} = [v_1, v_2, \\\\ldots, v_n]^T$，其中 $v_i$ 是第 $i',\n",
       " '$ 个分量。\\n\\n\\\\subsection{矩阵}\\n矩阵是二维数组，用于表示线性变换。\\n\\n矩阵乘法定义为：\\n\\\\begin{equation}\\n(\\\\mathbf{AB})_{ij} = \\\\sum_{k=1}^{n} a_{ik}b_{kj}\\n\\\\end{equation}\\n\\n\\\\section{概率论}\\n概率论为机器学习提供了不确定性建模的工具。\\n\\n\\\\subsection{贝叶斯定理}\\n贝叶斯定理是概率论的核心：\\n\\\\begin{equation}\\nP(A|B) =',\n",
       " '= \\\\frac{P(B|A)P(A)}{P(B)}\\n\\\\end{equation}\\n\\n\\\\section{优化理论}\\n优化理论用于寻找模型的最优参数。\\n\\n\\\\subsection{梯度下降}\\n梯度下降是最常用的优化算法：\\n\\\\begin{equation}\\n\\\\theta_{t+1} = \\\\theta_t - \\\\alpha \\\\nabla_\\\\theta J(\\\\theta)\\n\\\\end{equation}\\n\\n其中',\n",
       " '$\\\\alpha$ 是学习率，$J(\\\\theta)$ 是损失函数。\\n\\n\\\\end{document}']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 高级分割技术",
   "id": "529c08d21ed984fb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T04:41:51.146275Z",
     "start_time": "2025-07-23T04:41:51.136142Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "def advanced_splitting_techniques():\n",
    "    \"\"\"高级分割技术\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"8. 高级分割技术\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    doc = create_sample_documents()\n",
    "\n",
    "    # 8.1 动态块大小\n",
    "    print(\"\\n8.1 动态块大小调整\")\n",
    "\n",
    "    def adaptive_chunk_size(text: str) -> int:\n",
    "        \"\"\"根据文本复杂度动态调整块大小\"\"\"\n",
    "        sentences = text.split('。')\n",
    "        avg_sentence_length = sum(len(s) for s in sentences) / len(sentences) if sentences else 100\n",
    "\n",
    "        if avg_sentence_length > 50:\n",
    "            return 300  # 长句子用大块\n",
    "        elif avg_sentence_length > 30:\n",
    "            return 200  # 中等句子用中块\n",
    "        else:\n",
    "            return 150  # 短句子用小块\n",
    "\n",
    "    adaptive_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=adaptive_chunk_size(doc.page_content),\n",
    "        chunk_overlap=50\n",
    "    )\n",
    "\n",
    "    adaptive_chunks = adaptive_splitter.split_documents([doc])\n",
    "    print(f\"自适应分割块数: {len(adaptive_chunks)}\")\n",
    "\n",
    "    # 8.2 语义感知分割\n",
    "    print(\"\\n8.2 语义感知分割\")\n",
    "\n",
    "    def semantic_splitter(text: str, max_chunk_size: int = 200) -> List[str]:\n",
    "        \"\"\"基于语义的分割（简化版）\"\"\"\n",
    "        sentences = text.split('。')\n",
    "        chunks = []\n",
    "        current_chunk = \"\"\n",
    "\n",
    "        for sentence in sentences:\n",
    "            sentence = sentence.strip()\n",
    "            if not sentence:\n",
    "                continue\n",
    "\n",
    "            # 检查是否包含关键词（新主题开始）\n",
    "            topic_keywords = ['然而', '另外', '此外', '同时', '因此', '总之']\n",
    "            is_new_topic = any(keyword in sentence for keyword in topic_keywords)\n",
    "\n",
    "            if (len(current_chunk) + len(sentence) > max_chunk_size) or is_new_topic:\n",
    "                if current_chunk:\n",
    "                    chunks.append(current_chunk.strip())\n",
    "                current_chunk = sentence + '。'\n",
    "            else:\n",
    "                current_chunk += sentence + '。'\n",
    "\n",
    "        if current_chunk:\n",
    "            chunks.append(current_chunk.strip())\n",
    "\n",
    "        return chunks\n",
    "\n",
    "    semantic_chunks = semantic_splitter(doc.page_content)\n",
    "    print(f\"语义分割块数: {len(semantic_chunks)}\")\n",
    "\n",
    "    # 8.3 重叠策略优化\n",
    "    print(\"\\n8.3 智能重叠策略\")\n",
    "\n",
    "    def smart_overlap_splitter(text: str, chunk_size: int = 200) -> List[Document]:\n",
    "        \"\"\"智能重叠分割\"\"\"\n",
    "        sentences = text.split('。')\n",
    "        chunks = []\n",
    "\n",
    "        for i in range(0, len(sentences), 3):  # 每3句为一块\n",
    "            chunk_sentences = sentences[i:i+4]  # 取4句（包含1句重叠）\n",
    "            chunk_text = '。'.join(chunk_sentences).strip()\n",
    "\n",
    "            if chunk_text:\n",
    "                chunks.append(Document(\n",
    "                    page_content=chunk_text,\n",
    "                    metadata={\"chunk_id\": len(chunks), \"overlap_strategy\": \"sentence_based\"}\n",
    "                ))\n",
    "\n",
    "        return chunks\n",
    "\n",
    "    smart_chunks = smart_overlap_splitter(doc.page_content)\n",
    "    print(f\"智能重叠分割块数: {len(smart_chunks)}\")\n",
    "\n",
    "    return adaptive_chunks\n",
    "advanced_splitting_techniques()"
   ],
   "id": "eb577b3c88979ea0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "8. 高级分割技术\n",
      "============================================================\n",
      "\n",
      "8.1 动态块大小调整\n",
      "自适应分割块数: 4\n",
      "\n",
      "8.2 语义感知分割\n",
      "语义分割块数: 4\n",
      "\n",
      "8.3 智能重叠策略\n",
      "智能重叠分割块数: 5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'ai_history'}, page_content='人工智能的发展历程可以追溯到20世纪50年代。当时，计算机科学家开始探索让机器模拟人类智能的可能性。\\n\\n在1956年的达特茅斯会议上，人工智能这个术语首次被正式提出。这标志着AI作为一个独立学科的诞生。\\n\\n随后的几十年里，AI经历了多次起伏。60-70年代是第一个AI春天，专家系统得到了广泛应用。'),\n",
       " Document(metadata={'source': 'ai_history'}, page_content='随后的几十年里，AI经历了多次起伏。60-70年代是第一个AI春天，专家系统得到了广泛应用。\\n\\n80年代末到90年代初，由于技术限制和过高期望，AI进入了所谓的\"AI冬天\"。\\n\\n21世纪以来，随着大数据、云计算和深度学习的发展，AI迎来了新的春天。'),\n",
       " Document(metadata={'source': 'ai_history'}, page_content='21世纪以来，随着大数据、云计算和深度学习的发展，AI迎来了新的春天。\\n\\n今天，AI已经在图像识别、自然语言处理、推荐系统等领域取得了突破性进展。\\n\\n机器学习作为AI的核心技术，包括监督学习、无监督学习和强化学习三大类。深度学习则是机器学习的一个重要分支。'),\n",
       " Document(metadata={'source': 'ai_history'}, page_content='自然语言处理（NLP）让计算机能够理解和生成人类语言。计算机视觉让机器能够\"看见\"和理解图像。\\n\\n未来，AI将在更多领域发挥重要作用，包括医疗、教育、交通、金融等。同时，AI的伦理和安全问题也需要得到重视。')]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 分割器性能对比",
   "id": "d496c39856552c96"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T04:42:17.512714Z",
     "start_time": "2025-07-23T04:42:17.504253Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "def splitting_performance_comparison():\n",
    "    \"\"\"分割器性能对比\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"9. 分割器性能对比\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    doc = create_sample_documents()\n",
    "\n",
    "    splitters = {\n",
    "        \"RecursiveCharacter\": RecursiveCharacterTextSplitter(chunk_size=200, chunk_overlap=50),\n",
    "        \"Character\": CharacterTextSplitter(chunk_size=200, chunk_overlap=50, separator=\"\\n\\n\"),\n",
    "        \"Token\": TokenTextSplitter(chunk_size=50, chunk_overlap=10)\n",
    "    }\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    for name, splitter in splitters.items():\n",
    "        try:\n",
    "            import time\n",
    "            start_time = time.time()\n",
    "            chunks = splitter.split_documents([doc])\n",
    "            end_time = time.time()\n",
    "\n",
    "            results[name] = {\n",
    "                \"chunks\": len(chunks),\n",
    "                \"time\": end_time - start_time,\n",
    "                \"avg_length\": sum(len(c.page_content) for c in chunks) / len(chunks),\n",
    "                \"min_length\": min(len(c.page_content) for c in chunks),\n",
    "                \"max_length\": max(len(c.page_content) for c in chunks)\n",
    "            }\n",
    "        except Exception as e:\n",
    "            results[name] = {\"error\": str(e)}\n",
    "\n",
    "    print(\"\\n性能对比结果:\")\n",
    "    print(f\"{'分割器':<20} {'块数':<8} {'时间(s)':<10} {'平均长度':<10} {'最小长度':<10} {'最大长度':<10}\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "    for name, result in results.items():\n",
    "        if \"error\" not in result:\n",
    "            print(f\"{name:<20} {result['chunks']:<8} {result['time']:<10.4f} \"\n",
    "                  f\"{result['avg_length']:<10.1f} {result['min_length']:<10} {result['max_length']:<10}\")\n",
    "        else:\n",
    "            print(f\"{name:<20} 错误: {result['error']}\")\n",
    "splitting_performance_comparison()"
   ],
   "id": "f956334c9f412a6b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "9. 分割器性能对比\n",
      "============================================================\n",
      "\n",
      "性能对比结果:\n",
      "分割器                  块数       时间(s)      平均长度       最小长度       最大长度      \n",
      "--------------------------------------------------------------------------------\n",
      "RecursiveCharacter   3        0.0000     154.0      103        190       \n",
      "Character            3        0.0000     154.0      103        190       \n",
      "Token                21       0.0010     25.9       10         32        \n"
     ]
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 运行所有文本分割器示例",
   "id": "1f989884bffaaeb8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T04:42:52.546641Z",
     "start_time": "2025-07-23T04:42:52.537224Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "def main():\n",
    "    \"\"\"运行所有文本分割器示例\"\"\"\n",
    "    print(\"🚀 LangChain 0.3 Text Splitters 完整示例\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    # 运行所有示例\n",
    "    recursive_character_splitter_example()\n",
    "    character_text_splitter_example()\n",
    "    token_text_splitter_example()\n",
    "    markdown_header_splitter_example()\n",
    "    html_header_splitter_example()\n",
    "    code_splitter_example()\n",
    "    latex_splitter_example()\n",
    "    advanced_splitting_techniques()\n",
    "    splitting_performance_comparison()\n",
    "\n",
    "    print(\"\\n🎉 所有文本分割器示例运行完成！\")\n",
    "\n",
    "    # 最佳实践建议\n",
    "    print(\"\\n📋 最佳实践建议:\")\n",
    "    print(\"1. 通用文本：使用 RecursiveCharacterTextSplitter\")\n",
    "    print(\"2. 结构化文档：使用对应的Header分割器\")\n",
    "    print(\"3. 代码文档：使用 PythonCodeTextSplitter\")\n",
    "    print(\"4. Token限制：使用 TokenTextSplitter\")\n",
    "    print(\"5. 简单需求：使用 CharacterTextSplitter\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "id": "9194c102d056b703",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 LangChain 0.3 Text Splitters 完整示例\n",
      "================================================================================\n",
      "============================================================\n",
      "1. RecursiveCharacterTextSplitter（推荐）\n",
      "============================================================\n",
      "\n",
      "1.1 基础递归分割\n",
      "基础分割块数: 3\n",
      "块 1 (长度: 190): 人工智能的发展历程可以追溯到20世纪50年代。当时，计算机科学家开始探索让机器模拟人类智能的可能性。\n",
      "\n",
      "在1956年的达特茅斯会议上，人工智能这个术语首次被正式...\n",
      "块 2 (长度: 169): 80年代末到90年代初，由于技术限制和过高期望，AI进入了所谓的\"AI冬天\"。\n",
      "\n",
      "21世纪以来，随着大数据、云计算和深度学习的发展，AI迎来了新的春天。\n",
      "\n",
      "今天...\n",
      "块 3 (长度: 103): 自然语言处理（NLP）让计算机能够理解和生成人类语言。计算机视觉让机器能够\"看见\"和理解图像。\n",
      "\n",
      "未来，AI将在更多领域发挥重要作用，包括医疗、教育、交通、金融...\n",
      "\n",
      "1.2 自定义分隔符优先级\n",
      "自定义分割块数: 4\n",
      "\n",
      "1.3 段落优先分割\n",
      "段落分割块数: 2\n",
      "\n",
      "============================================================\n",
      "2. CharacterTextSplitter\n",
      "============================================================\n",
      "\n",
      "2.1 按段落分割\n",
      "段落分割块数: 2\n",
      "\n",
      "2.2 按句子分割\n",
      "句子分割块数: 6\n",
      "\n",
      "2.3 自定义分隔符\n",
      "自定义分割块数: 2\n",
      "  - 项目A|项目B|项目C|项目D\n",
      "  - 项目E的详细描述和分析报告\n",
      "\n",
      "============================================================\n",
      "3. TokenTextSplitter\n",
      "============================================================\n",
      "\n",
      "3.1 基础Token分割\n",
      "Token分割块数: 6\n",
      "块 1 Token数: 100, 内容: 人工智能的发展历程可以追溯到20世纪50年代。当时，计算机科学家开始探索让机器模拟人类智能的可能性。\n",
      "\n",
      "在1956年的达...\n",
      "块 2 Token数: 100, 内容: 次被正式提出。这标志着AI作为一个独立学科的诞生。\n",
      "\n",
      "随后的几十年里，AI经历了多次起伏。60-70年代是第一个AI春天...\n",
      "块 3 Token数: 100, 内容: 年代初，由于技术限制和过高期望，AI进入了所谓的\"AI冬天\"。\n",
      "\n",
      "21世纪以来，随着大数据、云计算和深度学习的发展，AI...\n",
      "\n",
      "3.2 不同模型Token分割对比\n",
      "gpt-3.5-turbo: 11 块\n",
      "text-davinci-003: 21 块\n",
      "gpt-4: 11 块\n",
      "\n",
      "============================================================\n",
      "4. MarkdownHeaderTextSplitter\n",
      "============================================================\n",
      "\n",
      "4.1 基础标题分割\n",
      "Markdown分割块数: 8\n",
      "\n",
      "块 1:\n",
      "内容: 监督学习是机器学习的一个重要分支，使用标记的训练数据来学习输入到输出的映射。  \n",
      "常见算法包括：\n",
      "- 线性回归\n",
      "- 逻辑回归\n",
      "- 决策树\n",
      "- 随机森林...\n",
      "元数据: {'Header 1': '人工智能技术指南', 'Header 2': '1. 机器学习基础', 'Header 3': '1.1 监督学习'}\n",
      "\n",
      "块 2:\n",
      "内容: 无监督学习从未标记的数据中发现隐藏的模式。  \n",
      "主要方法：\n",
      "- 聚类分析\n",
      "- 降维技术\n",
      "- 关联规则挖掘...\n",
      "元数据: {'Header 1': '人工智能技术指南', 'Header 2': '1. 机器学习基础', 'Header 3': '1.2 无监督学习'}\n",
      "\n",
      "块 3:\n",
      "内容: 神经网络是深度学习的基础，模拟人脑神经元的工作方式。...\n",
      "元数据: {'Header 1': '人工智能技术指南', 'Header 2': '2. 深度学习', 'Header 3': '2.1 神经网络基础'}\n",
      "\n",
      "块 4:\n",
      "内容: CNN主要用于图像处理和计算机视觉任务。...\n",
      "元数据: {'Header 1': '人工智能技术指南', 'Header 2': '2. 深度学习', 'Header 3': '2.2 卷积神经网络'}\n",
      "\n",
      "块 5:\n",
      "内容: RNN适合处理序列数据，如文本和时间序列。...\n",
      "元数据: {'Header 1': '人工智能技术指南', 'Header 2': '2. 深度学习', 'Header 3': '2.3 循环神经网络'}\n",
      "\n",
      "4.2 结合递归分割器进行二次分割\n",
      "二次分割后块数: 8\n",
      "\n",
      "============================================================\n",
      "5. HTMLHeaderTextSplitter\n",
      "============================================================\n",
      "HTML分割块数: 14\n",
      "\n",
      "块 1:\n",
      "内容: 人工智能概述...\n",
      "元数据: {'Header 1': '人工智能概述'}\n",
      "\n",
      "块 2:\n",
      "内容: 人工智能是计算机科学的一个分支。...\n",
      "元数据: {'Header 1': '人工智能概述'}\n",
      "\n",
      "块 3:\n",
      "内容: 机器学习...\n",
      "元数据: {'Header 1': '人工智能概述', 'Header 2': '机器学习'}\n",
      "\n",
      "============================================================\n",
      "6. PythonCodeTextSplitter\n",
      "============================================================\n",
      "Python代码分割块数: 5\n",
      "\n",
      "代码块 1 (长度: 188):\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.linear_model import LinearRegression\n",
      "from sklearn.metrics import mean_squared_error\n",
      "\n",
      "代码块 2 (长度: 352):\n",
      "class DataProcessor:\n",
      "    \"\"\"数据处理类\"\"\"\n",
      "\n",
      "    def __init__(self, data_path):\n",
      "        self.data_path = data_path\n",
      "        self.data = None\n",
      "\n",
      "    def load_data(self):\n",
      "        \"\"\"加载数据\"\"\"\n",
      "        self.data = pd...\n",
      "\n",
      "代码块 3 (长度: 288):\n",
      "# 特征缩放\n",
      "        from sklearn.preprocessing import StandardScaler\n",
      "        scaler = StandardScaler()\n",
      "        numeric_columns = self.data.select_dtypes(include=[np.number]).columns\n",
      "        self.data[numer...\n",
      "\n",
      "============================================================\n",
      "7. LatexTextSplitter\n",
      "============================================================\n",
      "LaTeX分割块数: 4\n",
      "\n",
      "LaTeX块 1:\n",
      "\\documentclass{article}\n",
      "\\usepackage{amsmath}\n",
      "\\title{机器学习数学基础}\n",
      "\\author{AI研究团队}\n",
      "\n",
      "\\begin{document}\n",
      "\\maketitle\n",
      "\n",
      "\\section{线性代数}\n",
      "线性代数是机器学习的数学基础之一。\n",
      "\n",
      "\\subsect...\n",
      "\n",
      "LaTeX块 2:\n",
      "$ 个分量。\n",
      "\n",
      "\\subsection{矩阵}\n",
      "矩阵是二维数组，用于表示线性变换。\n",
      "\n",
      "矩阵乘法定义为：\n",
      "\\begin{equation}\n",
      "(\\mathbf{AB})_{ij} = \\sum_{k=1}^{n} a_{ik}b_{kj}\n",
      "\\end{equation}\n",
      "\n",
      "\\section{概率论}\n",
      "概率...\n",
      "\n",
      "LaTeX块 3:\n",
      "= \\frac{P(B|A)P(A)}{P(B)}\n",
      "\\end{equation}\n",
      "\n",
      "\\section{优化理论}\n",
      "优化理论用于寻找模型的最优参数。\n",
      "\n",
      "\\subsection{梯度下降}\n",
      "梯度下降是最常用的优化算法：\n",
      "\\begin{equation}\n",
      "\\theta_{t+1} = \\theta_t -...\n",
      "\n",
      "============================================================\n",
      "8. 高级分割技术\n",
      "============================================================\n",
      "\n",
      "8.1 动态块大小调整\n",
      "自适应分割块数: 4\n",
      "\n",
      "8.2 语义感知分割\n",
      "语义分割块数: 4\n",
      "\n",
      "8.3 智能重叠策略\n",
      "智能重叠分割块数: 5\n",
      "\n",
      "============================================================\n",
      "9. 分割器性能对比\n",
      "============================================================\n",
      "\n",
      "性能对比结果:\n",
      "分割器                  块数       时间(s)      平均长度       最小长度       最大长度      \n",
      "--------------------------------------------------------------------------------\n",
      "RecursiveCharacter   3        0.0000     154.0      103        190       \n",
      "Character            3        0.0000     154.0      103        190       \n",
      "Token                21       0.0000     25.9       10         32        \n",
      "\n",
      "🎉 所有文本分割器示例运行完成！\n",
      "\n",
      "📋 最佳实践建议:\n",
      "1. 通用文本：使用 RecursiveCharacterTextSplitter\n",
      "2. 结构化文档：使用对应的Header分割器\n",
      "3. 代码文档：使用 PythonCodeTextSplitter\n",
      "4. Token限制：使用 TokenTextSplitter\n",
      "5. 简单需求：使用 CharacterTextSplitter\n"
     ]
    }
   ],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T03:08:17.593085Z",
     "start_time": "2025-07-23T03:08:17.414855Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 2. Text Splitters 示例\n",
    "def text_splitters_example(documents: List[Document]):\n",
    "    \"\"\"文本分割器示例\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"2. Text Splitters 文本分割器示例\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # 创建长文本用于分割\n",
    "    long_text = \"\"\"\n",
    "    人工智能的发展历程可以追溯到20世纪50年代。当时，计算机科学家开始探索让机器模拟人类智能的可能性。\n",
    "\n",
    "    在1956年的达特茅斯会议上，人工智能这个术语首次被正式提出。这标志着AI作为一个独立学科的诞生。\n",
    "\n",
    "    随后的几十年里，AI经历了多次起伏。60-70年代是第一个AI春天，专家系统得到了广泛应用。\n",
    "\n",
    "    80年代末到90年代初，由于技术限制和过高期望，AI进入了所谓的\"AI冬天\"。\n",
    "\n",
    "    21世纪以来，随着大数据、云计算和深度学习的发展，AI迎来了新的春天。\n",
    "\n",
    "    今天，AI已经在图像识别、自然语言处理、推荐系统等领域取得了突破性进展。\n",
    "    \"\"\"\n",
    "\n",
    "    long_doc = Document(page_content=long_text, metadata={\"source\": \"ai_history\"})\n",
    "\n",
    "    # 2.1 递归字符分割器（推荐）\n",
    "    print(\"\\n2.1 RecursiveCharacterTextSplitter\")\n",
    "    recursive_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=200,\n",
    "        chunk_overlap=50,\n",
    "        length_function=len,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \"。\", \"，\", \" \", \"\"]\n",
    "    )\n",
    "\n",
    "    recursive_chunks = recursive_splitter.split_documents([long_doc])\n",
    "    print(f\"递归分割块数: {len(recursive_chunks)}\")\n",
    "    for i, chunk in enumerate(recursive_chunks[:2]):\n",
    "        print(f\"块 {i + 1}: {chunk.page_content[:100]}...\")\n",
    "\n",
    "    # 2.2 字符分割器\n",
    "    print(\"\\n2.2 CharacterTextSplitter\")\n",
    "    char_splitter = CharacterTextSplitter(\n",
    "        chunk_size=300,\n",
    "        chunk_overlap=50,\n",
    "        separator=\"\\n\\n\"\n",
    "    )\n",
    "\n",
    "    char_chunks = char_splitter.split_documents([long_doc])\n",
    "    print(f\"字符分割块数: {len(char_chunks)}\")\n",
    "\n",
    "    # 2.3 Token分割器\n",
    "    print(\"\\n2.3 TokenTextSplitter\")\n",
    "    token_splitter = TokenTextSplitter(\n",
    "        chunk_size=100,\n",
    "        chunk_overlap=20\n",
    "    )\n",
    "\n",
    "    token_chunks = token_splitter.split_documents([long_doc])\n",
    "    print(f\"Token分割块数: {len(token_chunks)}\")\n",
    "\n",
    "    # 2.4 Markdown分割器\n",
    "    print(\"\\n2.4 MarkdownHeaderTextSplitter\")\n",
    "    markdown_text = \"\"\"\n",
    "# 人工智能概述\n",
    "\n",
    "## 什么是人工智能\n",
    "人工智能是计算机科学的一个分支。\n",
    "\n",
    "## AI的应用领域\n",
    "\n",
    "### 自然语言处理\n",
    "NLP是AI的重要分支。\n",
    "\n",
    "### 计算机视觉\n",
    "计算机视觉让机器能够\"看见\"。\n",
    "\n",
    "## 未来发展\n",
    "AI将继续快速发展。\n",
    "\"\"\"\n",
    "\n",
    "    markdown_splitter = MarkdownHeaderTextSplitter(\n",
    "        headers_to_split_on=[\n",
    "            (\"#\", \"Header 1\"),\n",
    "            (\"##\", \"Header 2\"),\n",
    "            (\"###\", \"Header 3\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    md_doc = Document(page_content=markdown_text)\n",
    "    md_chunks = markdown_splitter.split_text(markdown_text)\n",
    "    print(f\"Markdown分割块数: {len(md_chunks)}\")\n",
    "\n",
    "    return recursive_chunks\n",
    "# 2. 文本分割\n",
    "chunks = text_splitters_example(documents)"
   ],
   "id": "1279205a511e600e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "2. Text Splitters 文本分割器示例\n",
      "============================================================\n",
      "\n",
      "2.1 RecursiveCharacterTextSplitter\n",
      "递归分割块数: 2\n",
      "块 1: 人工智能的发展历程可以追溯到20世纪50年代。当时，计算机科学家开始探索让机器模拟人类智能的可能性。\n",
      "\n",
      "    在1956年的达特茅斯会议上，人工智能这个术语首次被正式提出。这标志着AI作为一个独立学...\n",
      "块 2: 80年代末到90年代初，由于技术限制和过高期望，AI进入了所谓的\"AI冬天\"。\n",
      "\n",
      "    21世纪以来，随着大数据、云计算和深度学习的发展，AI迎来了新的春天。\n",
      "\n",
      "    今天，AI已经在图像识别、自...\n",
      "\n",
      "2.2 CharacterTextSplitter\n",
      "字符分割块数: 1\n",
      "\n",
      "2.3 TokenTextSplitter\n",
      "Token分割块数: 7\n",
      "\n",
      "2.4 MarkdownHeaderTextSplitter\n",
      "Markdown分割块数: 4\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 3. Embedding Models 示例",
   "id": "88b30c524eb356d8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T04:52:14.218680Z",
     "start_time": "2025-07-23T04:52:13.636641Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "LangChain 0.3 Embedding Models 完整示例\n",
    "包含所有主要嵌入模型和高级用法\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import asyncio\n",
    "from typing import List, Dict, Any, Optional\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import time\n",
    "\n",
    "# 核心嵌入模型导入\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.embeddings import (\n",
    "    HuggingFaceEmbeddings,\n",
    "    HuggingFaceInstructEmbeddings,\n",
    "    SentenceTransformerEmbeddings,\n",
    "    CohereEmbeddings,\n",
    "    BedrockEmbeddings\n",
    ")\n",
    "from langchain_core.documents import Document"
   ],
   "id": "355d73806f13e63",
   "outputs": [],
   "execution_count": 34
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Ollama嵌入模型示例",
   "id": "2cdaefec8bb544b2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T04:57:51.681456Z",
     "start_time": "2025-07-23T04:57:51.478789Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "def ollama_embeddings_example():\n",
    "    \"\"\"Ollama嵌入模型示例 - 本地部署\"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"1. Ollama嵌入模型示例\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    try:\n",
    "        # 1.1 基础Ollama嵌入\n",
    "        print(\"\\n1.1 基础Ollama嵌入模型\")\n",
    "        embeddings = OllamaEmbeddings(\n",
    "            base_url=\"http://localhost:11434\",\n",
    "            model=\"nomic-embed-text:latest\",  # 推荐的嵌入模型\n",
    "        )\n",
    "\n",
    "        # 测试文本\n",
    "        texts = [\n",
    "            \"人工智能是计算机科学的分支\",\n",
    "            \"机器学习是AI的重要组成部分\",\n",
    "            \"深度学习使用神经网络进行学习\",\n",
    "            \"自然语言处理让计算机理解人类语言\",\n",
    "            \"今天天气很好，适合出门散步\"\n",
    "        ]\n",
    "\n",
    "        # 生成文档嵌入\n",
    "        print(\"生成文档嵌入...\")\n",
    "        doc_embeddings = embeddings.embed_documents(texts)\n",
    "        print(f\"文档嵌入数量: {len(doc_embeddings)}\")\n",
    "        print(f\"嵌入向量维度: {len(doc_embeddings[0])}\")\n",
    "\n",
    "        # 生成查询嵌入\n",
    "        query = \"什么是人工智能技术？\"\n",
    "        query_embedding = embeddings.embed_query(query)\n",
    "        print(f\"查询嵌入维度: {len(query_embedding)}\")\n",
    "\n",
    "        # 1.2 计算相似度\n",
    "        print(\"\\n1.2 语义相似度计算\")\n",
    "\n",
    "        def cosine_similarity(a: List[float], b: List[float]) -> float:\n",
    "            \"\"\"计算余弦相似度\"\"\"\n",
    "            a_np = np.array(a)\n",
    "            b_np = np.array(b)\n",
    "            return np.dot(a_np, b_np) / (np.linalg.norm(a_np) * np.linalg.norm(b_np))\n",
    "\n",
    "        print(f\"查询: '{query}'\")\n",
    "        print(\"与各文档的相似度:\")\n",
    "        similarities = []\n",
    "        for i, text in enumerate(texts):\n",
    "            similarity = cosine_similarity(query_embedding, doc_embeddings[i])\n",
    "            similarities.append((text, similarity))\n",
    "            print(f\"{i+1}. {similarity:.4f} - {text}\")\n",
    "\n",
    "        # 排序显示最相似的文档\n",
    "        similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "        print(f\"\\n最相似文档: {similarities[0][0]} (相似度: {similarities[0][1]:.4f})\")\n",
    "\n",
    "        # # 1.3 不同Ollama模型对比\n",
    "        # print(\"\\n1.3 不同Ollama嵌入模型对比\")\n",
    "        # ollama_models = [\n",
    "        #     \"nomic-embed-text\",\n",
    "        #     \"mxbai-embed-large\",\n",
    "        #     \"all-minilm\"\n",
    "        # ]\n",
    "        #\n",
    "        # for model_name in ollama_models:\n",
    "        #     try:\n",
    "        #         model_embeddings = OllamaEmbeddings(\n",
    "        #             base_url=\"http://localhost:11434\",\n",
    "        #             model=model_name\n",
    "        #         )\n",
    "        #         test_embedding = model_embeddings.embed_query(\"测试文本\")\n",
    "        #         print(f\"{model_name}: 维度 {len(test_embedding)}\")\n",
    "        #     except Exception as e:\n",
    "        #         print(f\"{model_name}: 不可用 ({str(e)[:50]}...)\")\n",
    "\n",
    "        return embeddings\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Ollama嵌入模型初始化失败: {e}\")\n",
    "        print(\"请确保Ollama服务正在运行并安装了嵌入模型\")\n",
    "        print(\"安装命令: ollama pull nomic-embed-text\")\n",
    "        return None\n",
    "ollama_embeddings_example()"
   ],
   "id": "f4693db3bc495453",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "1. Ollama嵌入模型示例\n",
      "============================================================\n",
      "\n",
      "1.1 基础Ollama嵌入模型\n",
      "生成文档嵌入...\n",
      "文档嵌入数量: 5\n",
      "嵌入向量维度: 768\n",
      "查询嵌入维度: 768\n",
      "\n",
      "1.2 语义相似度计算\n",
      "查询: '什么是人工智能技术？'\n",
      "与各文档的相似度:\n",
      "1. 0.8601 - 人工智能是计算机科学的分支\n",
      "2. 0.5262 - 机器学习是AI的重要组成部分\n",
      "3. 0.5862 - 深度学习使用神经网络进行学习\n",
      "4. 0.7732 - 自然语言处理让计算机理解人类语言\n",
      "5. 0.5400 - 今天天气很好，适合出门散步\n",
      "\n",
      "最相似文档: 人工智能是计算机科学的分支 (相似度: 0.8601)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "OllamaEmbeddings(model='nomic-embed-text:latest', validate_model_on_init=False, base_url='http://localhost:11434', client_kwargs={}, async_client_kwargs={}, sync_client_kwargs={}, mirostat=None, mirostat_eta=None, mirostat_tau=None, num_ctx=None, num_gpu=None, keep_alive=None, num_thread=None, repeat_last_n=None, repeat_penalty=None, temperature=None, stop=None, tfs_z=None, top_k=None, top_p=None)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 41
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### OpenAI嵌入模型示例",
   "id": "8681326b3e89e9d4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "def openai_embeddings_example():\n",
    "    \"\"\"OpenAI嵌入模型示例\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"2. OpenAI嵌入模型示例\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    try:\n",
    "        # 2.1 基础OpenAI嵌入\n",
    "        print(\"\\n2.1 基础OpenAI嵌入\")\n",
    "        embeddings = OpenAIEmbeddings(\n",
    "            model=\"text-embedding-3-small\",  # 新版本模型\n",
    "            api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "            dimensions=1536  # 可选：指定维度\n",
    "        )\n",
    "\n",
    "        texts = [\n",
    "            \"Artificial intelligence is a branch of computer science\",\n",
    "            \"Machine learning is a subset of AI\",\n",
    "            \"Deep learning uses neural networks\",\n",
    "            \"Natural language processing enables computers to understand human language\"\n",
    "        ]\n",
    "\n",
    "        doc_embeddings = embeddings.embed_documents(texts)\n",
    "        query_embedding = embeddings.embed_query(\"What is artificial intelligence?\")\n",
    "\n",
    "        print(f\"OpenAI嵌入维度: {len(doc_embeddings[0])}\")\n",
    "\n",
    "        # 2.2 不同OpenAI模型对比\n",
    "        print(\"\\n2.2 OpenAI模型对比\")\n",
    "        openai_models = [\n",
    "            (\"text-embedding-3-small\", 1536),\n",
    "            (\"text-embedding-3-large\", 3072),\n",
    "            (\"text-embedding-ada-002\", 1536)\n",
    "        ]\n",
    "\n",
    "        for model_name, default_dim in openai_models:\n",
    "            try:\n",
    "                model_embeddings = OpenAIEmbeddings(\n",
    "                    model=model_name,\n",
    "                    api_key=os.getenv(\"OPENAI_API_KEY\")\n",
    "                )\n",
    "                test_embedding = model_embeddings.embed_query(\"test\")\n",
    "                print(f\"{model_name}: 维度 {len(test_embedding)}\")\n",
    "            except Exception as e:\n",
    "                print(f\"{model_name}: 不可用 ({str(e)[:50]}...)\")\n",
    "\n",
    "        # 2.3 自定义维度（仅支持text-embedding-3系列）\n",
    "        print(\"\\n2.3 自定义嵌入维度\")\n",
    "        try:\n",
    "            custom_embeddings = OpenAIEmbeddings(\n",
    "                model=\"text-embedding-3-large\",\n",
    "                dimensions=1024,  # 自定义维度\n",
    "                api_key=os.getenv(\"OPENAI_API_KEY\")\n",
    "            )\n",
    "            custom_embedding = custom_embeddings.embed_query(\"自定义维度测试\")\n",
    "            print(f\"自定义维度嵌入: {len(custom_embedding)}\")\n",
    "        except Exception as e:\n",
    "            print(f\"自定义维度失败: {e}\")\n",
    "\n",
    "        return embeddings\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"OpenAI嵌入模型失败: {e}\")\n",
    "        print(\"请设置OPENAI_API_KEY环境变量\")\n",
    "        return None"
   ],
   "id": "9eea52b0b752046a",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### HuggingFace嵌入模型示例",
   "id": "842aaf54fead0da4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "def huggingface_embeddings_example():\n",
    "    \"\"\"HuggingFace嵌入模型示例\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"3. HuggingFace嵌入模型示例\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # 3.1 基础HuggingFace嵌入\n",
    "    print(\"\\n3.1 基础HuggingFace嵌入\")\n",
    "    try:\n",
    "        # 使用预训练的sentence-transformers模型\n",
    "        embeddings = HuggingFaceEmbeddings(\n",
    "            model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "            model_kwargs={'device': 'cpu'},  # 或 'cuda' 如果有GPU\n",
    "            encode_kwargs={'normalize_embeddings': True}  # 标准化嵌入\n",
    "        )\n",
    "\n",
    "        texts = [\n",
    "            \"这是一个测试文档\",\n",
    "            \"人工智能技术发展迅速\",\n",
    "            \"机器学习算法很重要\"\n",
    "        ]\n",
    "\n",
    "        doc_embeddings = embeddings.embed_documents(texts)\n",
    "        query_embedding = embeddings.embed_query(\"AI技术\")\n",
    "\n",
    "        print(f\"HuggingFace嵌入维度: {len(doc_embeddings[0])}\")\n",
    "\n",
    "        # 3.2 中文优化模型\n",
    "        print(\"\\n3.2 中文优化嵌入模型\")\n",
    "        chinese_models = [\n",
    "            \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\",\n",
    "            \"sentence-transformers/distiluse-base-multilingual-cased\",\n",
    "            \"BAAI/bge-small-zh-v1.5\"  # 中文优化模型\n",
    "        ]\n",
    "\n",
    "        for model_name in chinese_models:\n",
    "            try:\n",
    "                chinese_embeddings = HuggingFaceEmbeddings(\n",
    "                    model_name=model_name,\n",
    "                    model_kwargs={'device': 'cpu'}\n",
    "                )\n",
    "                test_embedding = chinese_embeddings.embed_query(\"中文测试\")\n",
    "                print(f\"{model_name}: 维度 {len(test_embedding)}\")\n",
    "            except Exception as e:\n",
    "                print(f\"{model_name}: 加载失败 ({str(e)[:50]}...)\")\n",
    "\n",
    "        # 3.3 指令优化嵌入\n",
    "        print(\"\\n3.3 指令优化嵌入模型\")\n",
    "        try:\n",
    "            instruct_embeddings = HuggingFaceInstructEmbeddings(\n",
    "                model_name=\"hkunlp/instructor-xl\",\n",
    "                model_kwargs={'device': 'cpu'}\n",
    "            )\n",
    "\n",
    "            # 使用指令前缀\n",
    "            query_instruction = \"为这个查询找到最相关的文档: \"\n",
    "            doc_instruction = \"这是一个关于技术的文档: \"\n",
    "\n",
    "            instruct_query = instruct_embeddings.embed_query(\n",
    "                query_instruction + \"人工智能应用\"\n",
    "            )\n",
    "            print(f\"指令嵌入维度: {len(instruct_query)}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"指令嵌入模型加载失败: {e}\")\n",
    "\n",
    "        return embeddings\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"HuggingFace嵌入模型失败: {e}\")\n",
    "        return None"
   ],
   "id": "25563679504a09f7",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### SentenceTransformers嵌入模型示例",
   "id": "fa655f44fae250e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "def sentence_transformers_example():\n",
    "    \"\"\"SentenceTransformers嵌入模型示例\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"4. SentenceTransformers嵌入模型示例\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    try:\n",
    "        # 4.1 多语言模型\n",
    "        print(\"\\n4.1 多语言SentenceTransformers\")\n",
    "        multilingual_embeddings = SentenceTransformerEmbeddings(\n",
    "            model_name=\"sentence-transformers/paraphrase-multilingual-mpnet-base-v2\"\n",
    "        )\n",
    "\n",
    "        # 多语言测试\n",
    "        multilingual_texts = [\n",
    "            \"Hello, how are you?\",\n",
    "            \"你好，你好吗？\",\n",
    "            \"Hola, ¿cómo estás?\",\n",
    "            \"Bonjour, comment allez-vous?\"\n",
    "        ]\n",
    "\n",
    "        multi_embeddings = multilingual_embeddings.embed_documents(multilingual_texts)\n",
    "        print(f\"多语言嵌入维度: {len(multi_embeddings[0])}\")\n",
    "\n",
    "        # 计算跨语言相似度\n",
    "        english_query = multilingual_embeddings.embed_query(\"greeting\")\n",
    "        chinese_query = multilingual_embeddings.embed_query(\"问候\")\n",
    "\n",
    "        def cosine_similarity(a, b):\n",
    "            return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "\n",
    "        cross_lang_similarity = cosine_similarity(english_query, chinese_query)\n",
    "        print(f\"跨语言相似度 (greeting vs 问候): {cross_lang_similarity:.4f}\")\n",
    "\n",
    "        # 4.2 专业领域模型\n",
    "        print(\"\\n4.2 专业领域嵌入模型\")\n",
    "        domain_models = [\n",
    "            \"sentence-transformers/all-mpnet-base-v2\",  # 通用\n",
    "            \"sentence-transformers/msmarco-distilbert-base-v4\",  # 搜索优化\n",
    "            \"sentence-transformers/nli-mpnet-base-v2\"  # 自然语言推理\n",
    "        ]\n",
    "\n",
    "        for model_name in domain_models:\n",
    "            try:\n",
    "                domain_embeddings = SentenceTransformerEmbeddings(model_name=model_name)\n",
    "                test_embedding = domain_embeddings.embed_query(\"domain test\")\n",
    "                print(f\"{model_name.split('/')[-1]}: 维度 {len(test_embedding)}\")\n",
    "            except Exception as e:\n",
    "                print(f\"{model_name}: 不可用\")\n",
    "\n",
    "        return multilingual_embeddings\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"SentenceTransformers失败: {e}\")\n",
    "        return None"
   ],
   "id": "adcd5cf10c88c30d",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 云端嵌入模型示例",
   "id": "649ec6c4cb0d4a91"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "def cloud_embeddings_example():\n",
    "    \"\"\"云端嵌入模型示例\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"5. 云端嵌入模型示例\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # 5.1 Cohere嵌入\n",
    "    print(\"\\n5.1 Cohere嵌入模型\")\n",
    "    try:\n",
    "        cohere_embeddings = CohereEmbeddings(\n",
    "            cohere_api_key=os.getenv(\"COHERE_API_KEY\"),\n",
    "            model=\"embed-english-v3.0\"  # 或 embed-multilingual-v3.0\n",
    "        )\n",
    "\n",
    "        cohere_texts = [\"AI technology\", \"Machine learning algorithms\"]\n",
    "        cohere_embeds = cohere_embeddings.embed_documents(cohere_texts)\n",
    "        print(f\"Cohere嵌入维度: {len(cohere_embeds[0])}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Cohere嵌入失败: {e}\")\n",
    "\n",
    "    # 5.2 AWS Bedrock嵌入\n",
    "    print(\"\\n5.2 AWS Bedrock嵌入模型\")\n",
    "    try:\n",
    "        bedrock_embeddings = BedrockEmbeddings(\n",
    "            credentials_profile_name=\"default\",\n",
    "            region_name=\"us-east-1\",\n",
    "            model_id=\"amazon.titan-embed-text-v1\"\n",
    "        )\n",
    "\n",
    "        bedrock_texts = [\"Cloud computing\", \"Serverless architecture\"]\n",
    "        bedrock_embeds = bedrock_embeddings.embed_documents(bedrock_texts)\n",
    "        print(f\"Bedrock嵌入维度: {len(bedrock_embeds[0])}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Bedrock嵌入失败: {e}\")"
   ],
   "id": "c659a74a35c242d7",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 嵌入模型性能对比",
   "id": "cf847864f2143d27"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "def embedding_performance_comparison():\n",
    "    \"\"\"嵌入模型性能对比\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"6. 嵌入模型性能对比\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # 测试文本\n",
    "    test_texts = [\n",
    "        \"人工智能技术正在快速发展\",\n",
    "        \"机器学习算法在各个领域都有应用\",\n",
    "        \"深度学习模型需要大量的训练数据\",\n",
    "        \"自然语言处理让计算机理解人类语言\",\n",
    "        \"计算机视觉技术可以识别图像中的物体\"\n",
    "    ]\n",
    "\n",
    "    test_query = \"AI技术的应用领域\"\n",
    "\n",
    "    # 定义要测试的模型\n",
    "    models_to_test = []\n",
    "\n",
    "    # Ollama模型\n",
    "    try:\n",
    "        ollama_model = OllamaEmbeddings(\n",
    "            base_url=\"http://localhost:11434\",\n",
    "            model=\"nomic-embed-text\"\n",
    "        )\n",
    "        models_to_test.append((\"Ollama-nomic\", ollama_model))\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # HuggingFace模型\n",
    "    try:\n",
    "        hf_model = HuggingFaceEmbeddings(\n",
    "            model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "        )\n",
    "        models_to_test.append((\"HF-MiniLM\", hf_model))\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # 性能测试\n",
    "    results = {}\n",
    "\n",
    "    for model_name, model in models_to_test:\n",
    "        try:\n",
    "            print(f\"\\n测试 {model_name}...\")\n",
    "\n",
    "            # 测试文档嵌入时间\n",
    "            start_time = time.time()\n",
    "            doc_embeddings = model.embed_documents(test_texts)\n",
    "            doc_time = time.time() - start_time\n",
    "\n",
    "            # 测试查询嵌入时间\n",
    "            start_time = time.time()\n",
    "            query_embedding = model.embed_query(test_query)\n",
    "            query_time = time.time() - start_time\n",
    "\n",
    "            # 计算相似度\n",
    "            similarities = []\n",
    "            for doc_emb in doc_embeddings:\n",
    "                sim = np.dot(query_embedding, doc_emb) / (\n",
    "                    np.linalg.norm(query_embedding) * np.linalg.norm(doc_emb)\n",
    "                )\n",
    "                similarities.append(sim)\n",
    "\n",
    "            results[model_name] = {\n",
    "                \"dimension\": len(doc_embeddings[0]),\n",
    "                \"doc_time\": doc_time,\n",
    "                \"query_time\": query_time,\n",
    "                \"avg_similarity\": np.mean(similarities),\n",
    "                \"max_similarity\": np.max(similarities)\n",
    "            }\n",
    "\n",
    "        except Exception as e:\n",
    "            results[model_name] = {\"error\": str(e)}\n",
    "\n",
    "    # 显示结果\n",
    "    print(\"\\n性能对比结果:\")\n",
    "    print(f\"{'模型':<15} {'维度':<8} {'文档时间(s)':<12} {'查询时间(s)':<12} {'平均相似度':<12} {'最高相似度':<12}\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "    for model_name, result in results.items():\n",
    "        if \"error\" not in result:\n",
    "            print(f\"{model_name:<15} {result['dimension']:<8} {result['doc_time']:<12.4f} \"\n",
    "                  f\"{result['query_time']:<12.4f} {result['avg_similarity']:<12.4f} \"\n",
    "                  f\"{result['max_similarity']:<12.4f}\")\n",
    "        else:\n",
    "            print(f\"{model_name:<15} 错误: {result['error'][:50]}...\")"
   ],
   "id": "6a3420fb4bd3fe3c",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 高级嵌入技术",
   "id": "9e7696d32b7f474b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "def advanced_embedding_techniques():\n",
    "    \"\"\"高级嵌入技术\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"7. 高级嵌入技术\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # 7.1 嵌入缓存\n",
    "    print(\"\\n7.1 嵌入缓存机制\")\n",
    "\n",
    "    class CachedEmbeddings:\n",
    "        \"\"\"带缓存的嵌入模型\"\"\"\n",
    "\n",
    "        def __init__(self, base_embeddings):\n",
    "            self.base_embeddings = base_embeddings\n",
    "            self.cache = {}\n",
    "\n",
    "        def embed_query(self, text: str) -> List[float]:\n",
    "            if text in self.cache:\n",
    "                print(f\"缓存命中: {text[:30]}...\")\n",
    "                return self.cache[text]\n",
    "\n",
    "            embedding = self.base_embeddings.embed_query(text)\n",
    "            self.cache[text] = embedding\n",
    "            print(f\"新计算: {text[:30]}...\")\n",
    "            return embedding\n",
    "\n",
    "        def embed_documents(self, texts: List[str]) -> List[List[float]]:\n",
    "            embeddings = []\n",
    "            for text in texts:\n",
    "                embeddings.append(self.embed_query(text))\n",
    "            return embeddings\n",
    "\n",
    "    # 使用缓存嵌入\n",
    "    try:\n",
    "        base_model = HuggingFaceEmbeddings(\n",
    "            model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "        )\n",
    "        cached_model = CachedEmbeddings(base_model)\n",
    "\n",
    "        # 第一次计算\n",
    "        test_texts = [\"AI技术\", \"机器学习\", \"AI技术\"]  # 重复文本\n",
    "        embeddings1 = cached_model.embed_documents(test_texts)\n",
    "\n",
    "        # 第二次计算（应该使用缓存）\n",
    "        embeddings2 = cached_model.embed_documents(test_texts)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"缓存嵌入示例失败: {e}\")\n",
    "\n",
    "    # 7.2 批量处理优化\n",
    "    print(\"\\n7.2 批量处理优化\")\n",
    "\n",
    "    def batch_embed_documents(embeddings_model, texts: List[str], batch_size: int = 32):\n",
    "        \"\"\"批量处理嵌入\"\"\"\n",
    "        all_embeddings = []\n",
    "\n",
    "        for i in range(0, len(texts), batch_size):\n",
    "            batch = texts[i:i + batch_size]\n",
    "            print(f\"处理批次 {i//batch_size + 1}: {len(batch)} 个文档\")\n",
    "\n",
    "            batch_embeddings = embeddings_model.embed_documents(batch)\n",
    "            all_embeddings.extend(batch_embeddings)\n",
    "\n",
    "        return all_embeddings\n",
    "\n",
    "    # 7.3 异步嵌入处理\n",
    "    print(\"\\n7.3 异步嵌入处理\")\n",
    "\n",
    "    async def async_embed_documents(embeddings_model, texts: List[str]):\n",
    "        \"\"\"异步处理嵌入\"\"\"\n",
    "        loop = asyncio.get_event_loop()\n",
    "\n",
    "        # 将文本分组\n",
    "        chunk_size = len(texts) // 4 + 1\n",
    "        tasks = []\n",
    "\n",
    "        for i in range(0, len(texts), chunk_size):\n",
    "            chunk = texts[i:i + chunk_size]\n",
    "            task = loop.run_in_executor(\n",
    "                None,\n",
    "                embeddings_model.embed_documents,\n",
    "                chunk\n",
    "            )\n",
    "            tasks.append(task)\n",
    "\n",
    "        # 等待所有任务完成\n",
    "        results = await asyncio.gather(*tasks)\n",
    "\n",
    "        # 合并结果\n",
    "        all_embeddings = []\n",
    "        for result in results:\n",
    "            all_embeddings.extend(result)\n",
    "\n",
    "        return all_embeddings"
   ],
   "id": "d6105b1dabd48ae7",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 嵌入质量评估",
   "id": "c6cdfe35150caa2b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "def embedding_quality_evaluation():\n",
    "    \"\"\"嵌入质量评估\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"8. 嵌入质量评估\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # 8.1 语义相似度测试\n",
    "    print(\"\\n8.1 语义相似度测试\")\n",
    "\n",
    "    # 定义测试用例\n",
    "    similarity_tests = [\n",
    "        (\"人工智能\", \"AI技术\", \"高相似度\"),\n",
    "        (\"机器学习\", \"深度学习\", \"中等相似度\"),\n",
    "        (\"计算机\", \"苹果\", \"低相似度\"),\n",
    "        (\"狗\", \"猫\", \"中等相似度\"),\n",
    "        (\"汽车\", \"飞机\", \"低相似度\")\n",
    "    ]\n",
    "\n",
    "    try:\n",
    "        embeddings = HuggingFaceEmbeddings(\n",
    "            model_name=\"sentence-transformers/paraphrase-multilingual-mpnet-base-v2\"\n",
    "        )\n",
    "\n",
    "        print(\"语义相似度测试结果:\")\n",
    "        for text1, text2, expected in similarity_tests:\n",
    "            emb1 = embeddings.embed_query(text1)\n",
    "            emb2 = embeddings.embed_query(text2)\n",
    "\n",
    "            similarity = np.dot(emb1, emb2) / (np.linalg.norm(emb1) * np.linalg.norm(emb2))\n",
    "            print(f\"{text1} vs {text2}: {similarity:.4f} ({expected})\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"语义相似度测试失败: {e}\")\n",
    "\n",
    "    # 8.2 聚类质量评估\n",
    "    print(\"\\n8.2 聚类质量评估\")\n",
    "\n",
    "    def evaluate_clustering_quality(embeddings_model, texts: List[str], labels: List[str]):\n",
    "        \"\"\"评估聚类质量\"\"\"\n",
    "        try:\n",
    "            from sklearn.cluster import KMeans\n",
    "            from sklearn.metrics import adjusted_rand_score\n",
    "\n",
    "            # 生成嵌入\n",
    "            embeddings = embeddings_model.embed_documents(texts)\n",
    "            embeddings_array = np.array(embeddings)\n",
    "\n",
    "            # 执行聚类\n",
    "            n_clusters = len(set(labels))\n",
    "            kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "            predicted_labels = kmeans.fit_predict(embeddings_array)\n",
    "\n",
    "            # 计算调整兰德指数\n",
    "            ari_score = adjusted_rand_score(labels, predicted_labels)\n",
    "            print(f\"聚类质量 (ARI): {ari_score:.4f}\")\n",
    "\n",
    "            return ari_score\n",
    "\n",
    "        except ImportError:\n",
    "            print(\"需要安装scikit-learn: pip install scikit-learn\")\n",
    "        except Exception as e:\n",
    "            print(f\"聚类评估失败: {e}\")"
   ],
   "id": "2cb39f33305df0a",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 自定义嵌入包装器",
   "id": "2bda2842fef09cab"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "def custom_embedding_wrapper():\n",
    "    \"\"\"自定义嵌入包装器\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"9. 自定义嵌入包装器\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    from langchain_core.embeddings import Embeddings\n",
    "\n",
    "    class MultiModelEmbeddings(Embeddings):\n",
    "        \"\"\"多模型集成嵌入\"\"\"\n",
    "\n",
    "        def __init__(self, models: List[Embeddings], weights: Optional[List[float]] = None):\n",
    "            self.models = models\n",
    "            self.weights = weights or [1.0] * len(models)\n",
    "\n",
    "            # 标准化权重\n",
    "            total_weight = sum(self.weights)\n",
    "            self.weights = [w / total_weight for w in self.weights]\n",
    "\n",
    "        def embed_documents(self, texts: List[str]) -> List[List[float]]:\n",
    "            \"\"\"集成多个模型的文档嵌入\"\"\"\n",
    "            all_embeddings = []\n",
    "\n",
    "            # 获取每个模型的嵌入\n",
    "            model_embeddings = []\n",
    "            for model in self.models:\n",
    "                embeddings = model.embed_documents(texts)\n",
    "                model_embeddings.append(embeddings)\n",
    "\n",
    "            # 加权平均\n",
    "            for i in range(len(texts)):\n",
    "                combined_embedding = np.zeros(len(model_embeddings[0][i]))\n",
    "\n",
    "                for j, (embeddings, weight) in enumerate(zip(model_embeddings, self.weights)):\n",
    "                    combined_embedding += np.array(embeddings[i]) * weight\n",
    "\n",
    "                all_embeddings.append(combined_embedding.tolist())\n",
    "\n",
    "            return all_embeddings\n",
    "\n",
    "        def embed_query(self, text: str) -> List[float]:\n",
    "            \"\"\"集成多个模型的查询嵌入\"\"\"\n",
    "            embeddings = self.embed_documents([text])\n",
    "            return embeddings[0]\n",
    "\n",
    "    # 使用示例\n",
    "    try:\n",
    "        # 创建多个基础模型\n",
    "        model1 = HuggingFaceEmbeddings(\n",
    "            model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "        )\n",
    "\n",
    "        # 如果有多个模型可用\n",
    "        models = [model1]  # 可以添加更多模型\n",
    "        weights = [1.0]    # 对应的权重\n",
    "\n",
    "        multi_embeddings = MultiModelEmbeddings(models, weights)\n",
    "\n",
    "        test_text = \"多模型嵌入测试\"\n",
    "        result = multi_embeddings.embed_query(test_text)\n",
    "        print(f\"多模型嵌入维度: {len(result)}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"多模型嵌入失败: {e}\")"
   ],
   "id": "d8d2f141a929d7d",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "def main():\n",
    "    \"\"\"运行所有嵌入模型示例\"\"\"\n",
    "    print(\"🚀 LangChain 0.3 Embedding Models 完整示例\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    # 运行所有示例\n",
    "    ollama_embeddings = ollama_embeddings_example()\n",
    "    openai_embeddings = openai_embeddings_example()\n",
    "    hf_embeddings = huggingface_embeddings_example()\n",
    "    st_embeddings = sentence_transformers_example()\n",
    "    cloud_embeddings_example()\n",
    "    embedding_performance_comparison()\n",
    "    advanced_embedding_techniques()\n",
    "    embedding_quality_evaluation()\n",
    "    custom_embedding_wrapper()\n",
    "\n",
    "    print(\"\\n🎉 所有嵌入模型示例运行完成！\")\n",
    "\n",
    "    # 最佳实践建议\n",
    "    print(\"\\n📋 嵌入模型选择建议:\")\n",
    "    print(\"1. 本地部署：Ollama + nomic-embed-text\")\n",
    "    print(\"2. 云端服务：OpenAI text-embedding-3-small\")\n",
    "    print(\"3. 开源方案：HuggingFace sentence-transformers\")\n",
    "    print(\"4. 中文优化：BAAI/bge-small-zh-v1.5\")\n",
    "    print(\"5. 多语言：paraphrase-multilingual-mpnet-base-v2\")\n",
    "    print(\"6. 高性能：text-embedding-3-large\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "id": "9fa80966af27d1d4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T06:54:26.718459Z",
     "start_time": "2025-07-23T06:54:26.199599Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 3. Embedding Models 示例\n",
    "def embedding_models_example():\n",
    "    \"\"\"嵌入模型示例\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"3. Embedding Models 嵌入模型示例\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # 3.1 Ollama嵌入模型\n",
    "    print(\"\\n3.1 Ollama嵌入模型\")\n",
    "    try:\n",
    "        embeddings = OllamaEmbeddings(\n",
    "            base_url=\"http://localhost:11434\",\n",
    "            model=\"nomic-embed-text\"  # 或使用其他嵌入模型\n",
    "        )\n",
    "\n",
    "        # 测试文本\n",
    "        texts = [\n",
    "            \"人工智能是计算机科学的分支\",\n",
    "            \"机器学习是AI的子集\",\n",
    "            \"深度学习使用神经网络\",\n",
    "            \"今天天气很好\"\n",
    "        ]\n",
    "\n",
    "        # 生成嵌入向量\n",
    "        text_embeddings = embeddings.embed_documents(texts)\n",
    "        query_embedding = embeddings.embed_query(\"什么是人工智能？\")\n",
    "\n",
    "        print(f\"文档嵌入数量: {len(text_embeddings)}\")\n",
    "        print(f\"嵌入向量维度: {len(text_embeddings[0])}\")\n",
    "        print(f\"查询嵌入维度: {len(query_embedding)}\")\n",
    "\n",
    "        # 计算相似度\n",
    "        import numpy as np\n",
    "\n",
    "        def cosine_similarity(a, b):\n",
    "            return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "\n",
    "        print(\"\\n相似度计算:\")\n",
    "        for i, text in enumerate(texts):\n",
    "            similarity = cosine_similarity(query_embedding, text_embeddings[i])\n",
    "            print(f\"'{text}' 相似度: {similarity:.4f}\")\n",
    "\n",
    "        return embeddings\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Ollama嵌入模型初始化失败: {e}\")\n",
    "        print(\"请确保Ollama服务正在运行并安装了嵌入模型\")\n",
    "        return None\n",
    "# 3. 嵌入模型\n",
    "embeddings = embedding_models_example()"
   ],
   "id": "95925b2d3a7b8cd2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "3. Embedding Models 嵌入模型示例\n",
      "============================================================\n",
      "\n",
      "3.1 Ollama嵌入模型\n",
      "文档嵌入数量: 4\n",
      "嵌入向量维度: 768\n",
      "查询嵌入维度: 768\n",
      "\n",
      "相似度计算:\n",
      "'人工智能是计算机科学的分支' 相似度: 0.8551\n",
      "'机器学习是AI的子集' 相似度: 0.6135\n",
      "'深度学习使用神经网络' 相似度: 0.5818\n",
      "'今天天气很好' 相似度: 0.5851\n"
     ]
    }
   ],
   "execution_count": 42
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 4. Vector Stores 示例",
   "id": "8127affda365bc76"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### FAISS向量存储",
   "id": "7bc71c31bc626a09"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T07:26:37.066513Z",
     "start_time": "2025-07-23T07:26:37.061484Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "LangChain 0.3 Vector Stores 完整示例\n",
    "包含所有主要向量存储和高级用法\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import asyncio\n",
    "from typing import List, Dict, Any, Optional, Tuple\n",
    "import time\n",
    "import json\n",
    "\n",
    "# 核心导入\n",
    "from langchain_core.documents import Document\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "# 向量存储导入\n",
    "from langchain_community.vectorstores import (\n",
    "    FAISS,\n",
    "    Chroma,\n",
    "    Qdrant,\n",
    "    Pinecone,\n",
    "    Weaviate,\n",
    "    Milvus,\n",
    "    ElasticsearchStore,\n",
    "    Redis,\n",
    "    PGVector,\n",
    "    SupabaseVectorStore\n",
    ")"
   ],
   "id": "e7cc29d4cd56791",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T07:27:19.412142Z",
     "start_time": "2025-07-23T07:27:19.291241Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 初始化嵌入模型\n",
    "embeddings = OllamaEmbeddings(\n",
    "    base_url=\"http://localhost:11434\",\n",
    "    model=\"nomic-embed-text:latest\"\n",
    ")\n",
    "\n",
    "# 准备文档\n",
    "documents = [\n",
    "    Document(\n",
    "        page_content=\"人工智能是计算机科学的一个分支\",\n",
    "        metadata={\"source\": \"ai_intro.txt\", \"category\": \"technology\"}\n",
    "    )\n",
    "    # ... 更多文档\n",
    "]\n",
    "\n",
    "# 创建FAISS向量存储\n",
    "faiss_vectorstore = FAISS.from_documents(documents, embeddings)\n",
    "\n",
    "# 保存索引到本地\n",
    "index_path = \"faiss_index\"\n",
    "faiss_vectorstore.save_local(index_path)\n",
    "\n",
    "# 加载已保存的索引\n",
    "loaded_vectorstore = FAISS.load_local(\n",
    "    index_path,\n",
    "    embeddings,\n",
    "    allow_dangerous_deserialization=True\n",
    ")\n",
    "\n",
    "# 基础相似性搜索\n",
    "query = \"什么是人工智能技术？\"\n",
    "similar_docs = faiss_vectorstore.similarity_search(query, k=3)\n",
    "print(similar_docs)\n",
    "# # 带分数的相似性搜索\n",
    "# similar_docs_with_scores = faiss_vectorstore.similarity_search_with_score(query, k=3)\n",
    "#\n",
    "# # # 基于阈值的搜索\n",
    "# # threshold = 0.8\n",
    "# # similar_docs_threshold = faiss_vectorstore.similarity_search_with_score_threshold(\n",
    "# #     query,\n",
    "# #     score_threshold=threshold\n",
    "# # )\n",
    "#\n",
    "# # 添加新文档\n",
    "# new_documents = [\n",
    "#     Document(\n",
    "#         page_content=\"量子计算是一种利用量子力学原理的计算方式\",\n",
    "#         metadata={\"source\": \"quantum.txt\", \"category\": \"technology\"}\n",
    "#     )\n",
    "# ]\n",
    "#\n",
    "# faiss_vectorstore.add_documents(new_documents)\n",
    "\n",
    "# # 向量存储合并\n",
    "# store1.merge_from(store2)\n",
    "#\n",
    "# # 不同索引类型（适合大规模数据）\n",
    "# faiss_ivf = FAISS.from_documents(\n",
    "#     documents,\n",
    "#     embeddings,\n",
    "#     index_type=\"IVF\",\n",
    "#     nlist=10  # 聚类中心数量\n",
    "# )"
   ],
   "id": "3feab481e112dd99",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(id='6ad706ea-41d7-43a2-ab98-6798c1322bda', metadata={'source': 'ai_intro.txt', 'category': 'technology'}, page_content='人工智能是计算机科学的一个分支')]\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T07:03:56.273154Z",
     "start_time": "2025-07-23T07:03:53.944736Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "def faiss_vectorstore_example():\n",
    "    \"\"\"FAISS向量存储详细示例\"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"1. FAISS向量存储示例\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    try:\n",
    "        # 初始化嵌入模型\n",
    "        # embeddings = HuggingFaceEmbeddings(\n",
    "        #     model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "        # )\n",
    "        embeddings = OllamaEmbeddings(\n",
    "        base_url=\"http://localhost:11434\",\n",
    "        model=\"nomic-embed-text:latest\",  # 推荐的嵌入模型\n",
    "        )\n",
    "\n",
    "        # 准备文档\n",
    "        documents = [\n",
    "            Document(\n",
    "                page_content=\"人工智能是计算机科学的一个分支，致力于创建智能机器\",\n",
    "                metadata={\"source\": \"ai_intro.txt\", \"category\": \"technology\", \"date\": \"2024-01-01\"}\n",
    "            ),\n",
    "            Document(\n",
    "                page_content=\"机器学习是人工智能的子集，使计算机能够从数据中学习\",\n",
    "                metadata={\"source\": \"ml_basics.txt\", \"category\": \"technology\", \"date\": \"2024-01-02\"}\n",
    "            ),\n",
    "            Document(\n",
    "                page_content=\"深度学习使用神经网络来模拟人脑的工作方式\",\n",
    "                metadata={\"source\": \"dl_guide.txt\", \"category\": \"technology\", \"date\": \"2024-01-03\"}\n",
    "            ),\n",
    "            Document(\n",
    "                page_content=\"自然语言处理让计算机理解和生成人类语言\",\n",
    "                metadata={\"source\": \"nlp_overview.txt\", \"category\": \"technology\", \"date\": \"2024-01-04\"}\n",
    "            ),\n",
    "            Document(\n",
    "                page_content=\"计算机视觉技术可以识别和分析图像中的内容\",\n",
    "                metadata={\"source\": \"cv_intro.txt\", \"category\": \"technology\", \"date\": \"2024-01-05\"}\n",
    "            ),\n",
    "            Document(\n",
    "                page_content=\"今天天气很好，适合出门散步和运动\",\n",
    "                metadata={\"source\": \"weather.txt\", \"category\": \"daily\", \"date\": \"2024-01-06\"}\n",
    "            ),\n",
    "            Document(\n",
    "                page_content=\"股票市场今天表现良好，科技股领涨\",\n",
    "                metadata={\"source\": \"market.txt\", \"category\": \"finance\", \"date\": \"2024-01-07\"}\n",
    "            )\n",
    "        ]\n",
    "\n",
    "        # 1.1 基础FAISS创建\n",
    "        print(\"\\n1.1 基础FAISS向量存储创建\")\n",
    "        faiss_vectorstore = FAISS.from_documents(documents, embeddings)\n",
    "        print(f\"✅ FAISS向量存储创建成功，包含 {len(documents)} 个文档\")\n",
    "\n",
    "        # 1.2 保存和加载\n",
    "        print(\"\\n1.2 FAISS索引保存和加载\")\n",
    "        index_path = \"faiss_index\"\n",
    "        faiss_vectorstore.save_local(index_path)\n",
    "        print(f\"✅ FAISS索引已保存到 {index_path}\")\n",
    "\n",
    "        # 加载索引\n",
    "        loaded_vectorstore = FAISS.load_local(\n",
    "            index_path,\n",
    "            embeddings,\n",
    "            allow_dangerous_deserialization=True\n",
    "        )\n",
    "        print(\"✅ FAISS索引加载成功\")\n",
    "\n",
    "        # 1.3 基础相似性搜索\n",
    "        print(\"\\n1.3 基础相似性搜索\")\n",
    "        query = \"什么是人工智能技术？\"\n",
    "        similar_docs = faiss_vectorstore.similarity_search(query, k=3)\n",
    "\n",
    "        print(f\"查询: '{query}'\")\n",
    "        print(\"最相似的文档:\")\n",
    "        for i, doc in enumerate(similar_docs):\n",
    "            print(f\"{i+1}. {doc.page_content}\")\n",
    "            print(f\"   元数据: {doc.metadata}\")\n",
    "            print()\n",
    "\n",
    "        # 1.4 带分数的相似性搜索\n",
    "        print(\"\\n1.4 带分数的相似性搜索\")\n",
    "        similar_docs_with_scores = faiss_vectorstore.similarity_search_with_score(query, k=3)\n",
    "\n",
    "        print(\"带分数的搜索结果:\")\n",
    "        for i, (doc, score) in enumerate(similar_docs_with_scores):\n",
    "            print(f\"{i+1}. 分数: {score:.4f}\")\n",
    "            print(f\"   内容: {doc.page_content[:80]}...\")\n",
    "            print(f\"   来源: {doc.metadata.get('source', 'unknown')}\")\n",
    "            print()\n",
    "\n",
    "        # 1.5 基于阈值的搜索\n",
    "        print(\"\\n1.5 基于阈值的相似性搜索\")\n",
    "        threshold = 0.8\n",
    "        similar_docs_threshold = faiss_vectorstore.similarity_search_with_score_threshold(\n",
    "            query,\n",
    "            score_threshold=threshold\n",
    "        )\n",
    "\n",
    "        print(f\"阈值 {threshold} 以上的文档:\")\n",
    "        for doc, score in similar_docs_threshold:\n",
    "            print(f\"分数: {score:.4f} - {doc.page_content[:60]}...\")\n",
    "\n",
    "        # 1.6 添加新文档\n",
    "        print(\"\\n1.6 添加新文档\")\n",
    "        new_documents = [\n",
    "            Document(\n",
    "                page_content=\"量子计算是一种利用量子力学原理的计算方式\",\n",
    "                metadata={\"source\": \"quantum.txt\", \"category\": \"technology\", \"date\": \"2024-01-08\"}\n",
    "            ),\n",
    "            Document(\n",
    "                page_content=\"区块链技术提供了去中心化的数据存储方案\",\n",
    "                metadata={\"source\": \"blockchain.txt\", \"category\": \"technology\", \"date\": \"2024-01-09\"}\n",
    "            )\n",
    "        ]\n",
    "\n",
    "        # 添加文档\n",
    "        faiss_vectorstore.add_documents(new_documents)\n",
    "        print(f\"✅ 已添加 {len(new_documents)} 个新文档\")\n",
    "\n",
    "        # 验证添加结果\n",
    "        new_query = \"量子计算的原理\"\n",
    "        new_results = faiss_vectorstore.similarity_search(new_query, k=2)\n",
    "        print(f\"新查询 '{new_query}' 的结果:\")\n",
    "        for doc in new_results:\n",
    "            print(f\"- {doc.page_content[:50]}...\")\n",
    "\n",
    "        # 1.7 不同搜索算法\n",
    "        print(\"\\n1.7 不同FAISS搜索算法\")\n",
    "\n",
    "        # 使用不同的索引类型\n",
    "        try:\n",
    "            # 创建IVF索引（适合大规模数据）\n",
    "            faiss_ivf = FAISS.from_documents(\n",
    "                documents,\n",
    "                embeddings,\n",
    "                index_type=\"IVF\",\n",
    "                nlist=10  # 聚类中心数量\n",
    "            )\n",
    "\n",
    "            ivf_results = faiss_ivf.similarity_search(\"机器学习算法\", k=2)\n",
    "            print(\"IVF索引搜索结果:\")\n",
    "            for doc in ivf_results:\n",
    "                print(f\"- {doc.page_content[:50]}...\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"IVF索引创建失败: {e}\")\n",
    "\n",
    "        return faiss_vectorstore\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"FAISS示例失败: {e}\")\n",
    "        return None\n",
    "faiss_vectorstore_example()"
   ],
   "id": "d902c11974c014e9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "1. FAISS向量存储示例\n",
      "============================================================\n",
      "\n",
      "1.1 基础FAISS向量存储创建\n",
      "✅ FAISS向量存储创建成功，包含 7 个文档\n",
      "\n",
      "1.2 FAISS索引保存和加载\n",
      "✅ FAISS索引已保存到 faiss_index\n",
      "✅ FAISS索引加载成功\n",
      "\n",
      "1.3 基础相似性搜索\n",
      "查询: '什么是人工智能技术？'\n",
      "最相似的文档:\n",
      "1. 人工智能是计算机科学的一个分支，致力于创建智能机器\n",
      "   元数据: {'source': 'ai_intro.txt', 'category': 'technology', 'date': '2024-01-01'}\n",
      "\n",
      "2. 机器学习是人工智能的子集，使计算机能够从数据中学习\n",
      "   元数据: {'source': 'ml_basics.txt', 'category': 'technology', 'date': '2024-01-02'}\n",
      "\n",
      "3. 自然语言处理让计算机理解和生成人类语言\n",
      "   元数据: {'source': 'nlp_overview.txt', 'category': 'technology', 'date': '2024-01-04'}\n",
      "\n",
      "\n",
      "1.4 带分数的相似性搜索\n",
      "带分数的搜索结果:\n",
      "1. 分数: 0.3241\n",
      "   内容: 人工智能是计算机科学的一个分支，致力于创建智能机器...\n",
      "   来源: ai_intro.txt\n",
      "\n",
      "2. 分数: 0.3984\n",
      "   内容: 机器学习是人工智能的子集，使计算机能够从数据中学习...\n",
      "   来源: ml_basics.txt\n",
      "\n",
      "3. 分数: 0.6391\n",
      "   内容: 自然语言处理让计算机理解和生成人类语言...\n",
      "   来源: nlp_overview.txt\n",
      "\n",
      "\n",
      "1.5 基于阈值的相似性搜索\n",
      "FAISS示例失败: 'FAISS' object has no attribute 'similarity_search_with_score_threshold'\n"
     ]
    }
   ],
   "execution_count": 48
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "def chroma_vectorstore_example():\n",
    "    \"\"\"Chroma向量存储详细示例\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"2. Chroma向量存储示例\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    try:\n",
    "        # 初始化嵌入模型\n",
    "        embeddings = HuggingFaceEmbeddings(\n",
    "            model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "        )\n",
    "\n",
    "        # 准备文档\n",
    "        documents = [\n",
    "            Document(\n",
    "                page_content=\"Python是一种高级编程语言，语法简洁易读\",\n",
    "                metadata={\"language\": \"python\", \"difficulty\": \"beginner\", \"topic\": \"programming\"}\n",
    "            ),\n",
    "            Document(\n",
    "                page_content=\"JavaScript是Web开发的核心语言之一\",\n",
    "                metadata={\"language\": \"javascript\", \"difficulty\": \"intermediate\", \"topic\": \"web\"}\n",
    "            ),\n",
    "            Document(\n",
    "                page_content=\"Java是一种面向对象的编程语言，广泛用于企业开发\",\n",
    "                metadata={\"language\": \"java\", \"difficulty\": \"intermediate\", \"topic\": \"enterprise\"}\n",
    "            ),\n",
    "            Document(\n",
    "                page_content=\"C++是一种高性能的系统编程语言\",\n",
    "                metadata={\"language\": \"cpp\", \"difficulty\": \"advanced\", \"topic\": \"system\"}\n",
    "            ),\n",
    "            Document(\n",
    "                page_content=\"Go语言是Google开发的现代编程语言，适合并发编程\",\n",
    "                metadata={\"language\": \"go\", \"difficulty\": \"intermediate\", \"topic\": \"concurrent\"}\n",
    "            )\n",
    "        ]\n",
    "\n",
    "        # 2.1 基础Chroma创建\n",
    "        print(\"\\n2.1 基础Chroma向量存储创建\")\n",
    "        persist_directory = \"./chroma_db\"\n",
    "\n",
    "        chroma_vectorstore = Chroma.from_documents(\n",
    "            documents=documents,\n",
    "            embedding=embeddings,\n",
    "            persist_directory=persist_directory,\n",
    "            collection_name=\"programming_languages\"\n",
    "        )\n",
    "\n",
    "        print(f\"✅ Chroma向量存储创建成功，持久化目录: {persist_directory}\")\n",
    "\n",
    "        # 2.2 持久化\n",
    "        print(\"\\n2.2 Chroma数据持久化\")\n",
    "        chroma_vectorstore.persist()\n",
    "        print(\"✅ Chroma数据已持久化\")\n",
    "\n",
    "        # 2.3 基础搜索\n",
    "        print(\"\\n2.3 基础相似性搜索\")\n",
    "        query = \"适合初学者的编程语言\"\n",
    "        results = chroma_vectorstore.similarity_search(query, k=3)\n",
    "\n",
    "        print(f\"查询: '{query}'\")\n",
    "        for i, doc in enumerate(results):\n",
    "            print(f\"{i+1}. {doc.page_content}\")\n",
    "            print(f\"   元数据: {doc.metadata}\")\n",
    "            print()\n",
    "\n",
    "        # 2.4 元数据过滤搜索\n",
    "        print(\"\\n2.4 基于元数据的过滤搜索\")\n",
    "\n",
    "        # 搜索初学者难度的语言\n",
    "        beginner_results = chroma_vectorstore.similarity_search(\n",
    "            query=\"编程语言\",\n",
    "            k=5,\n",
    "            filter={\"difficulty\": \"beginner\"}\n",
    "        )\n",
    "\n",
    "        print(\"初学者难度的编程语言:\")\n",
    "        for doc in beginner_results:\n",
    "            print(f\"- {doc.page_content}\")\n",
    "            print(f\"  难度: {doc.metadata['difficulty']}\")\n",
    "\n",
    "        # 搜索Web相关的语言\n",
    "        web_results = chroma_vectorstore.similarity_search(\n",
    "            query=\"开发语言\",\n",
    "            k=5,\n",
    "            filter={\"topic\": \"web\"}\n",
    "        )\n",
    "\n",
    "        print(\"\\nWeb开发相关的语言:\")\n",
    "        for doc in web_results:\n",
    "            print(f\"- {doc.page_content}\")\n",
    "            print(f\"  主题: {doc.metadata['topic']}\")\n",
    "\n",
    "        # 2.5 复杂过滤条件\n",
    "        print(\"\\n2.5 复杂过滤条件\")\n",
    "\n",
    "        # 使用$in操作符\n",
    "        intermediate_results = chroma_vectorstore.similarity_search(\n",
    "            query=\"编程\",\n",
    "            k=5,\n",
    "            filter={\"difficulty\": {\"$in\": [\"intermediate\", \"advanced\"]}}\n",
    "        )\n",
    "\n",
    "        print(\"中级和高级难度的语言:\")\n",
    "        for doc in intermediate_results:\n",
    "            print(f\"- {doc.page_content}\")\n",
    "            print(f\"  难度: {doc.metadata['difficulty']}\")\n",
    "\n",
    "        # 2.6 获取集合信息\n",
    "        print(\"\\n2.6 Chroma集合信息\")\n",
    "        collection = chroma_vectorstore._collection\n",
    "        print(f\"集合名称: {collection.name}\")\n",
    "        print(f\"文档数量: {collection.count()}\")\n",
    "\n",
    "        # 2.7 删除文档\n",
    "        print(\"\\n2.7 删除文档操作\")\n",
    "\n",
    "        # 添加一个临时文档用于删除演示\n",
    "        temp_doc = Document(\n",
    "            page_content=\"临时测试文档，将被删除\",\n",
    "            metadata={\"temp\": True, \"id\": \"temp_doc_1\"}\n",
    "        )\n",
    "\n",
    "        doc_ids = chroma_vectorstore.add_documents([temp_doc])\n",
    "        print(f\"添加临时文档，ID: {doc_ids}\")\n",
    "\n",
    "        # 删除文档\n",
    "        if doc_ids:\n",
    "            chroma_vectorstore.delete(doc_ids)\n",
    "            print(\"✅ 临时文档已删除\")\n",
    "\n",
    "        # 2.8 从现有集合加载\n",
    "        print(\"\\n2.8 从现有集合加载\")\n",
    "\n",
    "        # 创建新的Chroma实例连接到现有集合\n",
    "        existing_chroma = Chroma(\n",
    "            persist_directory=persist_directory,\n",
    "            embedding_function=embeddings,\n",
    "            collection_name=\"programming_languages\"\n",
    "        )\n",
    "\n",
    "        # 验证加载成功\n",
    "        test_results = existing_chroma.similarity_search(\"Python\", k=1)\n",
    "        print(f\"从现有集合加载成功，测试搜索结果: {len(test_results)} 个文档\")\n",
    "\n",
    "        return chroma_vectorstore\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Chroma示例失败: {e}\")\n",
    "        return None\n",
    "\n",
    "def qdrant_vectorstore_example():\n",
    "    \"\"\"Qdrant向量存储示例\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"3. Qdrant向量存储示例\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    try:\n",
    "        from qdrant_client import QdrantClient\n",
    "        from qdrant_client.models import Distance, VectorParams\n",
    "\n",
    "        # 初始化嵌入模型\n",
    "        embeddings = HuggingFaceEmbeddings(\n",
    "            model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "        )\n",
    "\n",
    "        # 3.1 本地Qdrant设置\n",
    "        print(\"\\n3.1 本地Qdrant向量存储\")\n",
    "\n",
    "        # 创建Qdrant客户端（内存模式）\n",
    "        client = QdrantClient(\":memory:\")\n",
    "\n",
    "        # 准备文档\n",
    "        documents = [\n",
    "            Document(\n",
    "                page_content=\"机器学习算法可以分为监督学习、无监督学习和强化学习\",\n",
    "                metadata={\"category\": \"ml\", \"type\": \"algorithm\", \"level\": \"intermediate\"}\n",
    "            ),\n",
    "            Document(\n",
    "                page_content=\"监督学习使用标记数据训练模型进行预测\",\n",
    "                metadata={\"category\": \"ml\", \"type\": \"supervised\", \"level\": \"beginner\"}\n",
    "            ),\n",
    "            Document(\n",
    "                page_content=\"无监督学习从未标记数据中发现隐藏模式\",\n",
    "                metadata={\"category\": \"ml\", \"type\": \"unsupervised\", \"level\": \"intermediate\"}\n",
    "            ),\n",
    "            Document(\n",
    "                page_content=\"强化学习通过与环境交互来学习最优策略\",\n",
    "                metadata={\"category\": \"ml\", \"type\": \"reinforcement\", \"level\": \"advanced\"}\n",
    "            ),\n",
    "            Document(\n",
    "                page_content=\"神经网络是深度学习的基础架构\",\n",
    "                metadata={\"category\": \"dl\", \"type\": \"architecture\", \"level\": \"intermediate\"}\n",
    "            )\n",
    "        ]\n",
    "\n",
    "        # 创建Qdrant向量存储\n",
    "        collection_name = \"ml_knowledge\"\n",
    "        qdrant_vectorstore = Qdrant.from_documents(\n",
    "            documents,\n",
    "            embeddings,\n",
    "            client=client,\n",
    "            collection_name=collection_name,\n",
    "            force_recreate=True\n",
    "        )\n",
    "\n",
    "        print(f\"✅ Qdrant向量存储创建成功，集合: {collection_name}\")\n",
    "\n",
    "        # 3.2 基础搜索\n",
    "        print(\"\\n3.2 Qdrant相似性搜索\")\n",
    "        query = \"什么是监督学习？\"\n",
    "        results = qdrant_vectorstore.similarity_search(query, k=3)\n",
    "\n",
    "        print(f\"查询: '{query}'\")\n",
    "        for i, doc in enumerate(results):\n",
    "            print(f\"{i+1}. {doc.page_content}\")\n",
    "            print(f\"   类别: {doc.metadata.get('category')}\")\n",
    "            print(f\"   类型: {doc.metadata.get('type')}\")\n",
    "            print()\n",
    "\n",
    "        # 3.3 带分数搜索\n",
    "        print(\"\\n3.3 Qdrant带分数搜索\")\n",
    "        results_with_scores = qdrant_vectorstore.similarity_search_with_score(query, k=3)\n",
    "\n",
    "        for i, (doc, score) in enumerate(results_with_scores):\n",
    "            print(f\"{i+1}. 分数: {score:.4f}\")\n",
    "            print(f\"   内容: {doc.page_content[:60]}...\")\n",
    "            print()\n",
    "\n",
    "        # 3.4 过滤搜索\n",
    "        print(\"\\n3.4 Qdrant过滤搜索\")\n",
    "\n",
    "        # 搜索特定类别\n",
    "        ml_results = qdrant_vectorstore.similarity_search(\n",
    "            query=\"学习算法\",\n",
    "            k=5,\n",
    "            filter={\"category\": \"ml\"}\n",
    "        )\n",
    "\n",
    "        print(\"机器学习类别的文档:\")\n",
    "        for doc in ml_results:\n",
    "            print(f\"- {doc.page_content[:50]}...\")\n",
    "            print(f\"  类型: {doc.metadata.get('type')}\")\n",
    "\n",
    "        # 3.5 添加新文档\n",
    "        print(\"\\n3.5 添加新文档到Qdrant\")\n",
    "        new_docs = [\n",
    "            Document(\n",
    "                page_content=\"卷积神经网络特别适合图像处理任务\",\n",
    "                metadata={\"category\": \"dl\", \"type\": \"cnn\", \"level\": \"intermediate\"}\n",
    "            ),\n",
    "            Document(\n",
    "                page_content=\"循环神经网络擅长处理序列数据\",\n",
    "                metadata={\"category\": \"dl\", \"type\": \"rnn\", \"level\": \"intermediate\"}\n",
    "            )\n",
    "        ]\n",
    "\n",
    "        qdrant_vectorstore.add_documents(new_docs)\n",
    "        print(f\"✅ 已添加 {len(new_docs)} 个新文档\")\n",
    "\n",
    "        # 验证添加\n",
    "        cnn_results = qdrant_vectorstore.similarity_search(\"卷积神经网络\", k=1)\n",
    "        print(f\"验证添加: {cnn_results[0].page_content[:40]}...\")\n",
    "\n",
    "        return qdrant_vectorstore\n",
    "\n",
    "    except ImportError:\n",
    "        print(\"需要安装qdrant-client: pip install qdrant-client\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Qdrant示例失败: {e}\")\n",
    "        return None\n",
    "\n",
    "def pinecone_vectorstore_example():\n",
    "    \"\"\"Pinecone向量存储示例\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"4. Pinecone向量存储示例\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    try:\n",
    "        import pinecone\n",
    "\n",
    "        # 4.1 Pinecone初始化\n",
    "        print(\"\\n4.1 Pinecone初始化\")\n",
    "\n",
    "        api_key = os.getenv(\"PINECONE_API_KEY\")\n",
    "        if not api_key:\n",
    "            print(\"请设置PINECONE_API_KEY环境变量\")\n",
    "            return None\n",
    "\n",
    "        # 初始化Pinecone\n",
    "        pinecone.init(\n",
    "            api_key=api_key,\n",
    "            environment=os.getenv(\"PINECONE_ENV\", \"us-west1-gcp\")\n",
    "        )\n",
    "\n",
    "        # 初始化嵌入模型\n",
    "        embeddings = OpenAIEmbeddings(\n",
    "            api_key=os.getenv(\"OPENAI_API_KEY\")\n",
    "        )\n",
    "\n",
    "        # 准备文档\n",
    "        documents = [\n",
    "            Document(\n",
    "                page_content=\"云计算提供按需访问的计算资源\",\n",
    "                metadata={\"service\": \"cloud\", \"provider\": \"general\", \"type\": \"infrastructure\"}\n",
    "            ),\n",
    "            Document(\n",
    "                page_content=\"AWS是亚马逊提供的云计算平台\",\n",
    "                metadata={\"service\": \"cloud\", \"provider\": \"aws\", \"type\": \"platform\"}\n",
    "            ),\n",
    "            Document(\n",
    "                page_content=\"Azure是微软的云计算服务\",\n",
    "                metadata={\"service\": \"cloud\", \"provider\": \"microsoft\", \"type\": \"platform\"}\n",
    "            ),\n",
    "            Document(\n",
    "                page_content=\"Google Cloud Platform提供各种云服务\",\n",
    "                metadata={\"service\": \"cloud\", \"provider\": \"google\", \"type\": \"platform\"}\n",
    "            )\n",
    "        ]\n",
    "\n",
    "        # 4.2 创建Pinecone索引\n",
    "        print(\"\\n4.2 创建Pinecone索引\")\n",
    "\n",
    "        index_name = \"langchain-demo\"\n",
    "        dimension = 1536  # OpenAI嵌入维度\n",
    "\n",
    "        # 检查索引是否存在\n",
    "        if index_name not in pinecone.list_indexes():\n",
    "            pinecone.create_index(\n",
    "                name=index_name,\n",
    "                dimension=dimension,\n",
    "                metric=\"cosine\"\n",
    "            )\n",
    "            print(f\"✅ 创建Pinecone索引: {index_name}\")\n",
    "        else:\n",
    "            print(f\"✅ 使用现有Pinecone索引: {index_name}\")\n",
    "\n",
    "        # 4.3 创建Pinecone向量存储\n",
    "        print(\"\\n4.3 创建Pinecone向量存储\")\n",
    "\n",
    "        pinecone_vectorstore = Pinecone.from_documents(\n",
    "            documents,\n",
    "            embeddings,\n",
    "            index_name=index_name\n",
    "        )\n",
    "\n",
    "        print(\"✅ Pinecone向量存储创建成功\")\n",
    "\n",
    "        # 4.4 搜索测试\n",
    "        print(\"\\n4.4 Pinecone搜索测试\")\n",
    "\n",
    "        query = \"什么是云计算平台？\"\n",
    "        results = pinecone_vectorstore.similarity_search(query, k=3)\n",
    "\n",
    "        print(f\"查询: '{query}'\")\n",
    "        for i, doc in enumerate(results):\n",
    "            print(f\"{i+1}. {doc.page_content}\")\n",
    "            print(f\"   提供商: {doc.metadata.get('provider')}\")\n",
    "            print()\n",
    "\n",
    "        # 4.5 命名空间使用\n",
    "        print(\"\\n4.5 Pinecone命名空间\")\n",
    "\n",
    "        # 使用命名空间分离不同类型的数据\n",
    "        namespace_vectorstore = Pinecone.from_existing_index(\n",
    "            index_name=index_name,\n",
    "            embedding=embeddings,\n",
    "            namespace=\"cloud_services\"\n",
    "        )\n",
    "\n",
    "        # 添加文档到特定命名空间\n",
    "        namespace_docs = [\n",
    "            Document(\n",
    "                page_content=\"Kubernetes是容器编排平台\",\n",
    "                metadata={\"type\": \"orchestration\", \"category\": \"devops\"}\n",
    "            )\n",
    "        ]\n",
    "\n",
    "        namespace_vectorstore.add_documents(namespace_docs)\n",
    "        print(\"✅ 文档已添加到命名空间\")\n",
    "\n",
    "        return pinecone_vectorstore\n",
    "\n",
    "    except ImportError:\n",
    "        print(\"需要安装pinecone-client: pip install pinecone-client\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Pinecone示例失败: {e}\")\n",
    "        return None\n",
    "\n",
    "def elasticsearch_vectorstore_example():\n",
    "    \"\"\"Elasticsearch向量存储示例\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"5. Elasticsearch向量存储示例\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    try:\n",
    "        from elasticsearch import Elasticsearch\n",
    "\n",
    "        # 5.1 Elasticsearch连接\n",
    "        print(\"\\n5.1 Elasticsearch连接\")\n",
    "\n",
    "        # 连接到本地Elasticsearch\n",
    "        es_client = Elasticsearch(\n",
    "            [{\"host\": \"localhost\", \"port\": 9200}],\n",
    "            # 如果有认证，添加以下配置\n",
    "            # http_auth=(\"username\", \"password\"),\n",
    "            # use_ssl=True,\n",
    "            # verify_certs=True\n",
    "        )\n",
    "\n",
    "        # 检查连接\n",
    "        if not es_client.ping():\n",
    "            print(\"无法连接到Elasticsearch，请确保服务正在运行\")\n",
    "            return None\n",
    "\n",
    "        print(\"✅ Elasticsearch连接成功\")\n",
    "\n",
    "        # 初始化嵌入模型\n",
    "        embeddings = HuggingFaceEmbeddings(\n",
    "            model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "        )\n",
    "\n",
    "        # 准备文档\n",
    "        documents = [\n",
    "            Document(\n",
    "                page_content=\"搜索引擎技术是信息检索的核心\",\n",
    "                metadata={\"domain\": \"search\", \"complexity\": \"medium\", \"year\": 2024}\n",
    "            ),\n",
    "            Document(\n",
    "                page_content=\"全文搜索可以在大量文档中快速找到相关内容\",\n",
    "                metadata={\"domain\": \"search\", \"complexity\": \"low\", \"year\": 2024}\n",
    "            ),\n",
    "            Document(\n",
    "                page_content=\"倒排索引是搜索引擎的基础数据结构\",\n",
    "                metadata={\"domain\": \"search\", \"complexity\": \"high\", \"year\": 2024}\n",
    "            ),\n",
    "            Document(\n",
    "                page_content=\"分布式搜索可以处理大规模数据集\",\n",
    "                metadata={\"domain\": \"distributed\", \"complexity\": \"high\", \"year\": 2024}\n",
    "            )\n",
    "        ]\n",
    "\n",
    "        # 5.2 创建Elasticsearch向量存储\n",
    "        print(\"\\n5.2 创建Elasticsearch向量存储\")\n",
    "\n",
    "        index_name = \"langchain_demo\"\n",
    "\n",
    "        es_vectorstore = ElasticsearchStore.from_documents(\n",
    "            documents,\n",
    "            embeddings,\n",
    "            es_connection=es_client,\n",
    "            index_name=index_name,\n",
    "            distance_strategy=\"COSINE\"\n",
    "        )\n",
    "\n",
    "        print(f\"✅ Elasticsearch向量存储创建成功，索引: {index_name}\")\n",
    "\n",
    "        # 5.3 基础搜索\n",
    "        print(\"\\n5.3 Elasticsearch搜索\")\n",
    "\n",
    "        query = \"如何实现快速搜索？\"\n",
    "        results = es_vectorstore.similarity_search(query, k=3)\n",
    "\n",
    "        print(f\"查询: '{query}'\")\n",
    "        for i, doc in enumerate(results):\n",
    "            print(f\"{i+1}. {doc.page_content}\")\n",
    "            print(f\"   领域: {doc.metadata.get('domain')}\")\n",
    "            print(f\"   复杂度: {doc.metadata.get('complexity')}\")\n",
    "            print()\n",
    "\n",
    "        # 5.4 混合搜索（向量+关键词）\n",
    "        print(\"\\n5.4 Elasticsearch混合搜索\")\n",
    "\n",
    "        # 结合向量搜索和关键词搜索\n",
    "        hybrid_results = es_vectorstore.similarity_search(\n",
    "            query=\"搜索引擎\",\n",
    "            k=3,\n",
    "            filter={\"term\": {\"metadata.domain.keyword\": \"search\"}}\n",
    "        )\n",
    "\n",
    "        print(\"混合搜索结果:\")\n",
    "        for doc in hybrid_results:\n",
    "            print(f\"- {doc.page_content}\")\n",
    "            print(f\"  复杂度: {doc.metadata.get('complexity')}\")\n",
    "\n",
    "        # 5.5 聚合查询\n",
    "        print(\"\\n5.5 Elasticsearch聚合查询\")\n",
    "\n",
    "        # 获取不同复杂度的文档数量\n",
    "        agg_query = {\n",
    "            \"aggs\": {\n",
    "                \"complexity_count\": {\n",
    "                    \"terms\": {\n",
    "                        \"field\": \"metadata.complexity.keyword\"\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "\n",
    "        # 执行聚合查询\n",
    "        agg_results = es_client.search(\n",
    "            index=index_name,\n",
    "            body=agg_query,\n",
    "            size=0\n",
    "        )\n",
    "\n",
    "        print(\"复杂度分布:\")\n",
    "        for bucket in agg_results[\"aggregations\"][\"complexity_count\"][\"buckets\"]:\n",
    "            print(f\"- {bucket['key']}: {bucket['doc_count']} 个文档\")\n",
    "\n",
    "        return es_vectorstore\n",
    "\n",
    "    except ImportError:\n",
    "        print(\"需要安装elasticsearch: pip install elasticsearch\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Elasticsearch示例失败: {e}\")\n",
    "        return None\n",
    "\n",
    "def redis_vectorstore_example():\n",
    "    \"\"\"Redis向量存储示例\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"6. Redis向量存储示例\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    try:\n",
    "        import redis\n",
    "\n",
    "        # 6.1 Redis连接\n",
    "        print(\"\\n6.1 Redis连接\")\n",
    "\n",
    "        redis_client = redis.Redis(\n",
    "            host=\"localhost\",\n",
    "            port=6379,\n",
    "            db=0,\n",
    "            decode_responses=True\n",
    "        )\n",
    "\n",
    "        # 检查连接\n",
    "        redis_client.ping()\n",
    "        print(\"✅ Redis连接成功\")\n",
    "\n",
    "        # 初始化嵌入模型\n",
    "        embeddings = HuggingFaceEmbeddings(\n",
    "            model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "        )\n",
    "\n",
    "        # 准备文档\n",
    "        documents = [\n",
    "            Document(\n",
    "                page_content=\"Redis是一个高性能的键值存储数据库\",\n",
    "                metadata={\"type\": \"database\", \"performance\": \"high\", \"category\": \"nosql\"}\n",
    "            ),\n",
    "            Document(\n",
    "                page_content=\"Redis支持多种数据结构，如字符串、列表、集合等\",\n",
    "                metadata={\"type\": \"database\", \"feature\": \"data_structures\", \"category\": \"nosql\"}\n",
    "            ),\n",
    "            Document(\n",
    "                page_content=\"Redis可以用作缓存、消息队列和会话存储\",\n",
    "                metadata={\"type\": \"database\", \"use_case\": \"cache\", \"category\": \"nosql\"}\n",
    "            ),\n",
    "            Document(\n",
    "                page_content=\"Redis集群提供了高可用性和水平扩展能力\",\n",
    "                metadata={\"type\": \"database\", \"feature\": \"clustering\", \"category\": \"nosql\"}\n",
    "            )\n",
    "        ]\n",
    "\n",
    "        # 6.2 创建Redis向量存储\n",
    "        print(\"\\n6.2 创建Redis向量存储\")\n",
    "\n",
    "        index_name = \"redis_langchain_demo\"\n",
    "\n",
    "        redis_vectorstore = Redis.from_documents(\n",
    "            documents,\n",
    "            embeddings,\n",
    "            redis_url=\"redis://localhost:6379\",\n",
    "            index_name=index_name\n",
    "        )\n",
    "\n",
    "        print(f\"✅ Redis向量存储创建成功，索引: {index_name}\")\n",
    "\n",
    "        # 6.3 搜索测试\n",
    "        print(\"\\n6.3 Redis搜索测试\")\n",
    "\n",
    "        query = \"什么是高性能数据库？\"\n",
    "        results = redis_vectorstore.similarity_search(query, k=3)\n",
    "\n",
    "        print(f\"查询: '{query}'\")\n",
    "        for i, doc in enumerate(results):\n",
    "            print(f\"{i+1}. {doc.page_content}\")\n",
    "            print(f\"   类型: {doc.metadata.get('type')}\")\n",
    "            print(f\"   特性: {doc.metadata.get('feature', doc.metadata.get('performance', 'N/A'))}\")\n",
    "            print()\n",
    "\n",
    "        # 6.4 过滤搜索\n",
    "        print(\"\\n6.4 Redis过滤搜索\")\n",
    "\n",
    "        # 搜索特定特性的文档\n",
    "        feature_results = redis_vectorstore.similarity_search(\n",
    "            query=\"Redis功能\",\n",
    "            k=5,\n",
    "            filter={\"feature\": \"data_structures\"}\n",
    "        )\n",
    "\n",
    "        print(\"数据结构相关的文档:\")\n",
    "        for doc in feature_results:\n",
    "            print(f\"- {doc.page_content}\")\n",
    "\n",
    "        return redis_vectorstore\n",
    "\n",
    "    except ImportError:\n",
    "        print(\"需要安装redis: pip install redis\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Redis示例失败: {e}\")\n",
    "        return None\n",
    "\n",
    "def vector_store_comparison():\n",
    "    \"\"\"向量存储性能对比\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"7. 向量存储性能对比\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # 准备测试数据\n",
    "    embeddings = HuggingFaceEmbeddings(\n",
    "        model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "    )\n",
    "\n",
    "    # 生成测试文档\n",
    "    test_documents = []\n",
    "    for i in range(100):\n",
    "        doc = Document(\n",
    "            page_content=f\"这是第{i}个测试文档，包含一些示例内容用于性能测试。文档编号：{i}\",\n",
    "            metadata={\"doc_id\": i, \"category\": f\"cat_{i % 5}\", \"priority\": i % 3}\n",
    "        )\n",
    "        test_documents.append(doc)\n",
    "\n",
    "    test_query = \"测试文档内容\"\n",
    "\n",
    "    # 测试不同向量存储的性能\n",
    "    stores_to_test = []\n",
    "\n",
    "    # FAISS测试\n",
    "    try:\n",
    "        print(\"\\n7.1 FAISS性能测试\")\n",
    "        start_time = time.time()\n",
    "        faiss_store = FAISS.from_documents(test_documents, embeddings)\n",
    "        creation_time = time.time() - start_time\n",
    "\n",
    "        start_time = time.time()\n",
    "        faiss_results = faiss_store.similarity_search(test_query, k=5)\n",
    "        search_time = time.time() - start_time\n",
    "\n",
    "        stores_to_test.append({\n",
    "            \"name\": \"FAISS\",\n",
    "            \"creation_time\": creation_time,\n",
    "            \"search_time\": search_time,\n",
    "            \"results_count\": len(faiss_results)\n",
    "        })\n",
    "\n",
    "        print(f\"FAISS - 创建时间: {creation_time:.4f}s, 搜索时间: {search_time:.4f}s\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"FAISS测试失败: {e}\")\n",
    "\n",
    "    # Chroma测试\n",
    "    try:\n",
    "        print(\"\\n7.2 Chroma性能测试\")\n",
    "        start_time = time.time()\n",
    "        chroma_store = Chroma.from_documents(\n",
    "            test_documents,\n",
    "            embeddings,\n",
    "            persist_directory=\"./test_chroma\",\n",
    "            collection_name=\"performance_test\"\n",
    "        )\n",
    "        creation_time = time.time() - start_time\n",
    "\n",
    "        start_time = time.time()\n",
    "        chroma_results = chroma_store.similarity_search(test_query, k=5)\n",
    "        search_time = time.time() - start_time\n",
    "\n",
    "        stores_to_test.append({\n",
    "            \"name\": \"Chroma\",\n",
    "            \"creation_time\": creation_time,\n",
    "            \"search_time\": search_time,\n",
    "            \"results_count\": len(chroma_results)\n",
    "        })\n",
    "\n",
    "        print(f\"Chroma - 创建时间: {creation_time:.4f}s, 搜索时间: {search_time:.4f}s\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Chroma测试失败: {e}\")\n",
    "\n",
    "    # 性能对比总结\n",
    "    print(\"\\n7.3 性能对比总结\")\n",
    "    print(f\"{'存储类型':<15} {'创建时间(s)':<12} {'搜索时间(s)':<12} {'结果数量':<10}\")\n",
    "    print(\"-\" * 55)\n",
    "\n",
    "    for store in stores_to_test:\n",
    "        print(f\"{store['name']:<15} {store['creation_time']:<12.4f} \"\n",
    "              f\"{store['search_time']:<12.4f} {store['results_count']:<10}\")\n",
    "\n",
    "    # 推荐建议\n",
    "    print(\"\\n7.4 选择建议\")\n",
    "    print(\"📋 向量存储选择指南:\")\n",
    "    print(\"1. 小规模本地应用: FAISS\")\n",
    "    print(\"2. 需要持久化: Chroma\")\n",
    "    print(\"3. 生产环境大规模: Pinecone/Qdrant\")\n",
    "    print(\"4. 已有Elasticsearch: ElasticsearchStore\")\n",
    "    print(\"5. 需要高性能缓存: Redis\")\n",
    "    print(\"6. 企业级应用: Weaviate/Milvus\")\n",
    "\n",
    "def advanced_vector_operations():\n",
    "    \"\"\"高级向量操作示例\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"8. 高级向量操作示例\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    try:\n",
    "        embeddings = HuggingFaceEmbeddings(\n",
    "            model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "        )\n",
    "\n",
    "        # 8.1 向量存储合并\n",
    "        print(\"\\n8.1 向量存储合并\")\n",
    "\n",
    "        # 创建两个不同的向量存储\n",
    "        docs1 = [\n",
    "            Document(page_content=\"第一个存储的文档1\", metadata={\"store\": \"store1\"}),\n",
    "            Document(page_content=\"第一个存储的文档2\", metadata={\"store\": \"store1\"})\n",
    "        ]\n",
    "\n",
    "        docs2 = [\n",
    "            Document(page_content=\"第二个存储的文档1\", metadata={\"store\": \"store2\"}),\n",
    "            Document(page_content=\"第二个存储的文档2\", metadata={\"store\": \"store2\"})\n",
    "        ]\n",
    "\n",
    "        store1 = FAISS.from_documents(docs1, embeddings)\n",
    "        store2 = FAISS.from_documents(docs2, embeddings)\n",
    "\n",
    "        # 合并向量存储\n",
    "        store1.merge_from(store2)\n",
    "        print(\"✅ 向量存储合并完成\")\n",
    "\n",
    "        # 验证合并结果\n",
    "        all_results = store1.similarity_search(\"文档\", k=4)\n",
    "        print(f\"合并后总文档数: {len(all_results)}\")\n",
    "        for doc in all_results:\n",
    "            print(f\"- {doc.page_content} (来源: {doc.metadata['store']})\")\n",
    "\n",
    "        # 8.2 批量操作\n",
    "        print(\"\\n8.2 批量向量操作\")\n",
    "\n",
    "        # 批量添加文档\n",
    "        batch_docs = [\n",
    "            Document(page_content=f\"批量文档{i}\", metadata={\"batch\": True, \"id\": i})\n",
    "            for i in range(10)\n",
    "        ]\n",
    "\n",
    "        start_time = time.time()\n",
    "        store1.add_documents(batch_docs)\n",
    "        batch_time = time.time() - start_time\n",
    "\n",
    "        print(f\"批量添加10个文档耗时: {batch_time:.4f}s\")\n",
    "\n",
    "        # 8.3 向量存储统计\n",
    "        print(\"\\n8.3 向量存储统计信息\")\n",
    "\n",
    "        # 获取所有文档进行统计\n",
    "        all_docs = store1.similarity_search(\"\", k=100)  # 获取更多文档\n",
    "\n",
    "        # 统计不同来源的文档数量\n",
    "        store_counts = {}\n",
    "        batch_count = 0\n",
    "\n",
    "        for doc in all_docs:\n",
    "            if doc.metadata.get(\"batch\"):\n",
    "                batch_count += 1\n",
    "            else:\n",
    "                store = doc.metadata.get(\"store\", \"unknown\")\n",
    "                store_counts[store] = store_counts.get(store, 0) + 1\n",
    "\n",
    "        print(\"文档统计:\")\n",
    "        for store, count in store_counts.items():\n",
    "            print(f\"- {store}: {count} 个文档\")\n",
    "        print(f\"- 批量文档: {batch_count} 个文档\")\n",
    "\n",
    "        # 8.4 相似度阈值过滤\n",
    "        print(\"\\n8.4 相似度阈值过滤\")\n",
    "\n",
    "        query = \"批量文档\"\n",
    "        threshold = 0.5\n",
    "\n",
    "        # 获取所有结果并手动过滤\n",
    "        all_results_with_scores = store1.similarity_search_with_score(query, k=20)\n",
    "\n",
    "        filtered_results = [\n",
    "            (doc, score) for doc, score in all_results_with_scores\n",
    "            if score <= threshold  # FAISS使用距离，越小越相似\n",
    "        ]\n",
    "\n",
    "        print(f\"阈值 {threshold} 以下的结果:\")\n",
    "        for doc, score in filtered_results[:5]:\n",
    "            print(f\"- 分数: {score:.4f} - {doc.page_content}\")\n",
    "\n",
    "        return store1\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"高级向量操作失败: {e}\")\n",
    "        return None\n",
    "\n",
    "def main():\n",
    "    \"\"\"运行所有向量存储示例\"\"\"\n",
    "    print(\"🚀 LangChain 0.3 Vector Stores 完整示例\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    # 运行所有示例\n",
    "    faiss_store = faiss_vectorstore_example()\n",
    "    chroma_store = chroma_vectorstore_example()\n",
    "    qdrant_store = qdrant_vectorstore_example()\n",
    "    pinecone_store = pinecone_vectorstore_example()\n",
    "    es_store = elasticsearch_vectorstore_example()\n",
    "    redis_store = redis_vectorstore_example()\n",
    "\n",
    "    # 性能对比\n",
    "    vector_store_comparison()\n",
    "\n",
    "    # 高级操作\n",
    "    advanced_vector_operations()\n",
    "\n",
    "    print(\"\\n🎉 所有向量存储示例运行完成！\")\n",
    "\n",
    "    # 清理临时文件\n",
    "    import shutil\n",
    "    temp_dirs = [\"faiss_index\", \"chroma_db\", \"test_chroma\"]\n",
    "    for temp_dir in temp_dirs:\n",
    "        if os.path.exists(temp_dir):\n",
    "            try:\n",
    "                shutil.rmtree(temp_dir)\n",
    "                print(f\"🧹 已清理临时目录: {temp_dir}\")\n",
    "            except Exception as e:\n",
    "                print(f\"清理 {temp_dir} 失败: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "id": "b94579b900fba73c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T06:59:58.792957Z",
     "start_time": "2025-07-23T06:59:55.770318Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "# 4. Vector Stores 示例\n",
    "def vector_stores_example(chunks: List[Document], embeddings):\n",
    "    \"\"\"向量存储示例\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"4. Vector Stores 向量存储示例\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    if embeddings is None:\n",
    "        print(\"跳过向量存储示例（嵌入模型不可用）\")\n",
    "        return None, None\n",
    "\n",
    "    # 4.1 FAISS向量存储\n",
    "    print(\"\\n4.1 FAISS向量存储\")\n",
    "    try:\n",
    "        # 创建FAISS向量存储\n",
    "        faiss_vectorstore = FAISS.from_documents(chunks, embeddings)\n",
    "\n",
    "        # 保存到本地\n",
    "        faiss_vectorstore.save_local(\"faiss_index\")\n",
    "        print(\"✅ FAISS索引已保存\")\n",
    "\n",
    "        # 相似性搜索\n",
    "        query = \"人工智能的发展\"\n",
    "        similar_docs = faiss_vectorstore.similarity_search(query, k=3)\n",
    "\n",
    "        print(f\"\\n查询: '{query}'\")\n",
    "        print(\"相似文档:\")\n",
    "        for i, doc in enumerate(similar_docs):\n",
    "            print(f\"{i + 1}. {doc.page_content[:100]}...\")\n",
    "\n",
    "        # 带分数的相似性搜索\n",
    "        similar_docs_with_scores = faiss_vectorstore.similarity_search_with_score(query, k=3)\n",
    "        print(\"\\n带分数的搜索结果:\")\n",
    "        for i, (doc, score) in enumerate(similar_docs_with_scores):\n",
    "            print(f\"{i + 1}. 分数: {score:.4f} - {doc.page_content[:80]}...\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"FAISS创建失败: {e}\")\n",
    "        faiss_vectorstore = None\n",
    "\n",
    "    # 4.2 Chroma向量存储\n",
    "    print(\"\\n4.2 Chroma向量存储\")\n",
    "    try:\n",
    "        chroma_vectorstore = Chroma.from_documents(\n",
    "            chunks,\n",
    "            embeddings,\n",
    "            persist_directory=\"./chroma_db\"\n",
    "        )\n",
    "\n",
    "        # 持久化\n",
    "        chroma_vectorstore.persist()\n",
    "        print(\"✅ Chroma数据库已持久化\")\n",
    "\n",
    "        # 搜索测试\n",
    "        chroma_results = chroma_vectorstore.similarity_search(\"机器学习\", k=2)\n",
    "        print(f\"Chroma搜索结果数量: {len(chroma_results)}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Chroma创建失败: {e}\")\n",
    "        chroma_vectorstore = None\n",
    "\n",
    "    return faiss_vectorstore, chroma_vectorstore\n",
    "\n",
    "# 4. 向量存储\n",
    "faiss_store, chroma_store = vector_stores_example(chunks, embeddings)"
   ],
   "id": "6e01b3deffa2493e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "4. Vector Stores 向量存储示例\n",
      "============================================================\n",
      "\n",
      "4.1 FAISS向量存储\n",
      "✅ FAISS索引已保存\n",
      "\n",
      "查询: '人工智能的发展'\n",
      "相似文档:\n",
      "1. 人工智能的发展历程可以追溯到20世纪50年代。当时，计算机科学家开始探索让机器模拟人类智能的可能性。\n",
      "\n",
      "    在1956年的达特茅斯会议上，人工智能这个术语首次被正式提出。这标志着AI作为一个独立学...\n",
      "2. 80年代末到90年代初，由于技术限制和过高期望，AI进入了所谓的\"AI冬天\"。\n",
      "\n",
      "    21世纪以来，随着大数据、云计算和深度学习的发展，AI迎来了新的春天。\n",
      "\n",
      "    今天，AI已经在图像识别、自...\n",
      "\n",
      "带分数的搜索结果:\n",
      "1. 分数: 0.4337 - 人工智能的发展历程可以追溯到20世纪50年代。当时，计算机科学家开始探索让机器模拟人类智能的可能性。\n",
      "\n",
      "    在1956年的达特茅斯会议上，人工智能这个术语首...\n",
      "2. 分数: 0.7495 - 80年代末到90年代初，由于技术限制和过高期望，AI进入了所谓的\"AI冬天\"。\n",
      "\n",
      "    21世纪以来，随着大数据、云计算和深度学习的发展，AI迎来了新的春天。...\n",
      "\n",
      "4.2 Chroma向量存储\n",
      "✅ Chroma数据库已持久化\n",
      "Chroma搜索结果数量: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\34769\\AppData\\Local\\Temp\\ipykernel_12904\\3854282456.py:51: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
      "  chroma_vectorstore.persist()\n"
     ]
    }
   ],
   "execution_count": 43
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 5. Retrievers 示例",
   "id": "ad86b09d47e42ea4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T03:08:21.798555Z",
     "start_time": "2025-07-23T03:08:21.776446Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "# 5. Retrievers 示例\n",
    "def retrievers_example(vectorstore, chunks: List[Document]):\n",
    "    \"\"\"检索器示例\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"5. Retrievers 检索器示例\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # 5.1 向量存储检索器\n",
    "    print(\"\\n5.1 向量存储检索器\")\n",
    "    if vectorstore:\n",
    "        vector_retriever = vectorstore.as_retriever(\n",
    "            search_type=\"similarity\",\n",
    "            search_kwargs={\"k\": 3}\n",
    "        )\n",
    "\n",
    "        results = vector_retriever.invoke(\"人工智能的应用\")\n",
    "        print(f\"向量检索结果数量: {len(results)}\")\n",
    "        for i, doc in enumerate(results):\n",
    "            print(f\"{i + 1}. {doc.page_content[:100]}...\")\n",
    "\n",
    "    # 5.2 BM25检索器\n",
    "    print(\"\\n5.2 BM25检索器\")\n",
    "    try:\n",
    "        bm25_retriever = BM25Retriever.from_documents(chunks)\n",
    "        bm25_retriever.k = 3\n",
    "\n",
    "        bm25_results = bm25_retriever.invoke(\"人工智能发展\")\n",
    "        print(f\"BM25检索结果数量: {len(bm25_results)}\")\n",
    "        for i, doc in enumerate(bm25_results):\n",
    "            print(f\"{i + 1}. {doc.page_content[:100]}...\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"BM25检索器创建失败: {e}\")\n",
    "        bm25_retriever = None\n",
    "\n",
    "    # 5.3 集成检索器\n",
    "    print(\"\\n5.3 集成检索器\")\n",
    "    if vectorstore and bm25_retriever:\n",
    "        try:\n",
    "            ensemble_retriever = EnsembleRetriever(\n",
    "                retrievers=[vector_retriever, bm25_retriever],\n",
    "                weights=[0.7, 0.3]  # 向量搜索权重0.7，BM25权重0.3\n",
    "            )\n",
    "\n",
    "            ensemble_results = ensemble_retriever.invoke(\"机器学习技术\")\n",
    "            print(f\"集成检索结果数量: {len(ensemble_results)}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"集成检索器创建失败: {e}\")\n",
    "\n",
    "    # 5.4 多查询检索器\n",
    "    print(\"\\n5.4 多查询检索器\")\n",
    "    if vectorstore:\n",
    "        try:\n",
    "            llm = ChatOllama(\n",
    "                base_url=\"http://localhost:11434\",\n",
    "                model=\"gemma3:4b\"\n",
    "            )\n",
    "\n",
    "            multi_query_retriever = MultiQueryRetriever.from_llm(\n",
    "                retriever=vector_retriever,\n",
    "                llm=llm\n",
    "            )\n",
    "\n",
    "            multi_results = multi_query_retriever.invoke(\"AI的未来发展趋势\")\n",
    "            print(f\"多查询检索结果数量: {len(multi_results)}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"多查询检索器创建失败: {e}\")\n",
    "# 5. 检索器\n",
    "retrievers_example(faiss_store, chunks)"
   ],
   "id": "f14c88e8a32df855",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "5. Retrievers 检索器示例\n",
      "============================================================\n",
      "\n",
      "5.1 向量存储检索器\n",
      "\n",
      "5.2 BM25检索器\n",
      "BM25检索结果数量: 2\n",
      "1. 80年代末到90年代初，由于技术限制和过高期望，AI进入了所谓的\"AI冬天\"。\n",
      "\n",
      "    21世纪以来，随着大数据、云计算和深度学习的发展，AI迎来了新的春天。\n",
      "\n",
      "    今天，AI已经在图像识别、自...\n",
      "2. 人工智能的发展历程可以追溯到20世纪50年代。当时，计算机科学家开始探索让机器模拟人类智能的可能性。\n",
      "\n",
      "    在1956年的达特茅斯会议上，人工智能这个术语首次被正式提出。这标志着AI作为一个独立学...\n",
      "\n",
      "5.3 集成检索器\n",
      "\n",
      "5.4 多查询检索器\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 6. 完整RAG流程示例",
   "id": "f90a79c3cfcb7331"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T08:40:23.593705Z",
     "start_time": "2025-07-23T08:40:11.224941Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# 6. 完整RAG流程示例\n",
    "def complete_rag_example():\n",
    "    \"\"\"完整的RAG流程示例\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"6. 完整RAG流程示例\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    try:\n",
    "        # 6.1 准备数据\n",
    "        documents = [\n",
    "            Document(page_content=\"LangChain是一个用于构建LLM应用的框架\", metadata={\"source\": \"doc1\"}),\n",
    "            Document(page_content=\"向量数据库可以存储和检索高维向量\", metadata={\"source\": \"doc2\"}),\n",
    "            Document(page_content=\"RAG结合了检索和生成，提高了AI回答的准确性\", metadata={\"source\": \"doc3\"}),\n",
    "            Document(page_content=\"嵌入模型将文本转换为数值向量表示\", metadata={\"source\": \"doc4\"})\n",
    "        ]\n",
    "\n",
    "        # 6.2 文本分割\n",
    "        splitter = RecursiveCharacterTextSplitter(chunk_size=100, chunk_overlap=20)\n",
    "        chunks = splitter.split_documents(documents)\n",
    "\n",
    "        # 6.3 创建嵌入和向量存储\n",
    "        embeddings = OllamaEmbeddings(\n",
    "            base_url=\"http://localhost:11434\",\n",
    "            model=\"nomic-embed-text\"\n",
    "        )\n",
    "\n",
    "        vectorstore = FAISS.from_documents(chunks, embeddings)\n",
    "        retriever = vectorstore.as_retriever(k=2)\n",
    "\n",
    "        # 6.4 创建RAG链\n",
    "        from langchain_core.prompts import ChatPromptTemplate\n",
    "        from langchain_core.output_parsers import StrOutputParser\n",
    "        from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "        llm = ChatOllama(\n",
    "            base_url=\"http://localhost:11434\",\n",
    "            model=\"gemma3:4b\"\n",
    "        )\n",
    "\n",
    "        prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "        基于以下上下文回答问题：\n",
    "\n",
    "        上下文：{context}\n",
    "\n",
    "        问题：{question}\n",
    "\n",
    "        请提供准确、简洁的回答：\n",
    "        \"\"\")\n",
    "\n",
    "        def format_docs(docs):\n",
    "            return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "        rag_chain = (\n",
    "                {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "                | prompt\n",
    "                | llm\n",
    "                | StrOutputParser()\n",
    "        )\n",
    "\n",
    "        # 6.5 测试RAG系统\n",
    "        questions = [\n",
    "            \"什么是LangChain？\",\n",
    "            \"向量数据库的作用是什么？\",\n",
    "            \"RAG技术有什么优势？\"\n",
    "        ]\n",
    "\n",
    "        for question in questions:\n",
    "            print(f\"\\n问题: {question}\")\n",
    "            answer = rag_chain.invoke(question)\n",
    "            print(f\"回答: {answer}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"RAG流程执行失败: {e}\")\n",
    "complete_rag_example()"
   ],
   "id": "e78b5eb261c8ae41",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "6. 完整RAG流程示例\n",
      "============================================================\n",
      "\n",
      "问题: 什么是LangChain？\n",
      "回答: LangChain 是一个用于构建LLM应用的框架。\n",
      "\n",
      "\n",
      "问题: 向量数据库的作用是什么？\n",
      "回答: 向量数据库的作用是存储和检索高维向量。\n",
      "\n",
      "\n",
      "问题: RAG技术有什么优势？\n",
      "回答: RAG技术提高了AI回答的准确性。\n",
      "\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 7. 高级功能示例",
   "id": "7af8f010e7932d07"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T03:08:25.967216Z",
     "start_time": "2025-07-23T03:08:25.960219Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# 7. 高级功能示例\n",
    "def advanced_features_example():\n",
    "    \"\"\"高级功能示例\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"7. 高级功能示例\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # 7.1 自定义文档加载器\n",
    "    print(\"\\n7.1 自定义文档加载器\")\n",
    "\n",
    "    class CustomLoader:\n",
    "        def __init__(self, data_source):\n",
    "            self.data_source = data_source\n",
    "\n",
    "        def load(self):\n",
    "            # 模拟从API或数据库加载数据\n",
    "            documents = []\n",
    "            for i, item in enumerate(self.data_source):\n",
    "                doc = Document(\n",
    "                    page_content=item[\"content\"],\n",
    "                    metadata={\"id\": i, \"type\": item[\"type\"]}\n",
    "                )\n",
    "                documents.append(doc)\n",
    "            return documents\n",
    "\n",
    "    # 使用自定义加载器\n",
    "    custom_data = [\n",
    "        {\"content\": \"Python是一种编程语言\", \"type\": \"技术\"},\n",
    "        {\"content\": \"数据科学需要统计知识\", \"type\": \"科学\"},\n",
    "        {\"content\": \"机器学习算法很重要\", \"type\": \"AI\"}\n",
    "    ]\n",
    "\n",
    "    custom_loader = CustomLoader(custom_data)\n",
    "    custom_docs = custom_loader.load()\n",
    "    print(f\"自定义加载器文档数量: {len(custom_docs)}\")\n",
    "\n",
    "    # 7.2 文档过滤和预处理\n",
    "    print(\"\\n7.2 文档过滤和预处理\")\n",
    "\n",
    "    def preprocess_documents(documents):\n",
    "        \"\"\"文档预处理函数\"\"\"\n",
    "        processed_docs = []\n",
    "        for doc in documents:\n",
    "            # 清理文本\n",
    "            content = doc.page_content.strip()\n",
    "            content = content.replace(\"\\n\", \" \").replace(\"\\t\", \" \")\n",
    "\n",
    "            # 过滤短文档\n",
    "            if len(content) > 10:\n",
    "                doc.page_content = content\n",
    "                processed_docs.append(doc)\n",
    "\n",
    "        return processed_docs\n",
    "\n",
    "    processed_docs = preprocess_documents(custom_docs)\n",
    "    print(f\"预处理后文档数量: {len(processed_docs)}\")\n",
    "advanced_features_example()"
   ],
   "id": "1fd34ff050068f21",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "7. 高级功能示例\n",
      "============================================================\n",
      "\n",
      "7.1 自定义文档加载器\n",
      "自定义加载器文档数量: 3\n",
      "\n",
      "7.2 文档过滤和预处理\n",
      "预处理后文档数量: 1\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 8. 性能优化示例",
   "id": "7bddd0a43c7a49f4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T03:08:25.998972Z",
     "start_time": "2025-07-23T03:08:25.990972Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 8. 性能优化示例\n",
    "async def performance_optimization_example():\n",
    "    \"\"\"性能优化示例\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"8. 性能优化示例\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # 8.1 批量处理\n",
    "    print(\"\\n8.1 批量嵌入处理\")\n",
    "\n",
    "    try:\n",
    "        embeddings = OllamaEmbeddings(\n",
    "            base_url=\"http://localhost:11434\",\n",
    "            model=\"nomic-embed-text\"\n",
    "        )\n",
    "\n",
    "        # 大量文本\n",
    "        texts = [f\"这是第{i}个文档的内容\" for i in range(10)]\n",
    "\n",
    "        # 批量生成嵌入\n",
    "        batch_embeddings = embeddings.embed_documents(texts)\n",
    "        print(f\"批量处理文档数量: {len(batch_embeddings)}\")\n",
    "\n",
    "        # 8.2 异步处理\n",
    "        print(\"\\n8.2 异步处理示例\")\n",
    "\n",
    "        async def async_embed_text(text):\n",
    "            # 模拟异步嵌入\n",
    "            await asyncio.sleep(0.1)\n",
    "            return embeddings.embed_query(text)\n",
    "\n",
    "        # 并发处理\n",
    "        tasks = [async_embed_text(f\"异步文本{i}\") for i in range(5)]\n",
    "        results = await asyncio.gather(*tasks)\n",
    "        print(f\"异步处理结果数量: {len(results)}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"性能优化示例失败: {e}\")\n",
    "performance_optimization_example()\n"
   ],
   "id": "35a7d46e00e601f",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<coroutine object performance_optimization_example at 0x000002B59BA34260>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 主函数",
   "id": "2e5e75e1f7ea7e1f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T03:08:26.043412Z",
     "start_time": "2025-07-23T03:08:26.038469Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# 主函数\n",
    "def main():\n",
    "    \"\"\"运行所有示例\"\"\"\n",
    "    print(\"🚀 LangChain 0.3 Data Connection 完整示例\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    # 1. 文档加载\n",
    "    documents = document_loaders_example()\n",
    "\n",
    "    # 2. 文本分割\n",
    "    chunks = text_splitters_example(documents)\n",
    "\n",
    "    # 3. 嵌入模型\n",
    "    embeddings = embedding_models_example()\n",
    "\n",
    "    # 4. 向量存储\n",
    "    faiss_store, chroma_store = vector_stores_example(chunks, embeddings)\n",
    "\n",
    "    # 5. 检索器\n",
    "    retrievers_example(faiss_store, chunks)\n",
    "\n",
    "    # 6. 完整RAG流程\n",
    "    complete_rag_example()\n",
    "\n",
    "    # 7. 高级功能\n",
    "    advanced_features_example()\n",
    "\n",
    "    print(\"\\n🎉 所有示例运行完成！\")\n",
    "\n",
    "    # 清理临时文件\n",
    "    cleanup_files()\n",
    "\n",
    "\n",
    "def cleanup_files():\n",
    "    \"\"\"清理临时文件\"\"\"\n",
    "    import shutil\n",
    "\n",
    "    files_to_remove = [\"sample.txt\", \"sample.csv\", \"sample.json\"]\n",
    "    dirs_to_remove = [\"docs\", \"faiss_index\", \"chroma_db\"]\n",
    "\n",
    "    for file in files_to_remove:\n",
    "        if os.path.exists(file):\n",
    "            os.remove(file)\n",
    "\n",
    "    for dir in dirs_to_remove:\n",
    "        if os.path.exists(dir):\n",
    "            shutil.rmtree(dir)\n",
    "\n",
    "    print(\"🧹 临时文件已清理\")\n"
   ],
   "id": "7f6e6f544e1599ac",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T03:08:34.297186Z",
     "start_time": "2025-07-23T03:08:26.078641Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 运行同步示例\n",
    "    main()\n",
    "\n",
    "    # 运行异步示例\n",
    "    # asyncio.run(performance_optimization_example())"
   ],
   "id": "1588268b0cb2662",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 LangChain 0.3 Data Connection 完整示例\n",
      "================================================================================\n",
      "\n",
      "============================================================\n",
      "1. Document Loaders 文档加载器示例\n",
      "============================================================\n",
      "\n",
      "1.1 文本文件加载\n",
      "加载的文档数量: 1\n",
      "文档内容预览: \n",
      "        人工智能（AI）是计算机科学的一个分支，致力于创建能够执行通常需要人类智能的任务的系统。\n",
      "        机器学习是AI的一个子集，它使计算机能够从数据中学习而无需明确编程。\n",
      "   ...\n",
      "\n",
      "1.2 CSV文件加载\n",
      "CSV文档数量: 3\n",
      "CSV文档示例: name: 张三\n",
      "age: 25\n",
      "city: 北京\n",
      "description: 软件工程师\n",
      "\n",
      "1.3 JSON文件加载\n",
      "JSON文档数量: 2\n",
      "JSON文档示例: Python是一种高级编程语言\n",
      "\n",
      "1.4 目录批量加载\n",
      "目录文档数量: 3\n",
      "\n",
      "============================================================\n",
      "2. Text Splitters 文本分割器示例\n",
      "============================================================\n",
      "\n",
      "2.1 RecursiveCharacterTextSplitter\n",
      "递归分割块数: 2\n",
      "块 1: 人工智能的发展历程可以追溯到20世纪50年代。当时，计算机科学家开始探索让机器模拟人类智能的可能性。\n",
      "\n",
      "    在1956年的达特茅斯会议上，人工智能这个术语首次被正式提出。这标志着AI作为一个独立学...\n",
      "块 2: 80年代末到90年代初，由于技术限制和过高期望，AI进入了所谓的\"AI冬天\"。\n",
      "\n",
      "    21世纪以来，随着大数据、云计算和深度学习的发展，AI迎来了新的春天。\n",
      "\n",
      "    今天，AI已经在图像识别、自...\n",
      "\n",
      "2.2 CharacterTextSplitter\n",
      "字符分割块数: 1\n",
      "\n",
      "2.3 TokenTextSplitter\n",
      "Token分割块数: 7\n",
      "\n",
      "2.4 MarkdownHeaderTextSplitter\n",
      "Markdown分割块数: 4\n",
      "\n",
      "============================================================\n",
      "3. Embedding Models 嵌入模型示例\n",
      "============================================================\n",
      "\n",
      "3.1 Ollama嵌入模型\n",
      "Ollama嵌入模型初始化失败: Failed to connect to Ollama. Please check that Ollama is downloaded, running and accessible. https://ollama.com/download\n",
      "请确保Ollama服务正在运行并安装了嵌入模型\n",
      "\n",
      "============================================================\n",
      "4. Vector Stores 向量存储示例\n",
      "============================================================\n",
      "跳过向量存储示例（嵌入模型不可用）\n",
      "\n",
      "============================================================\n",
      "5. Retrievers 检索器示例\n",
      "============================================================\n",
      "\n",
      "5.1 向量存储检索器\n",
      "\n",
      "5.2 BM25检索器\n",
      "BM25检索结果数量: 2\n",
      "1. 80年代末到90年代初，由于技术限制和过高期望，AI进入了所谓的\"AI冬天\"。\n",
      "\n",
      "    21世纪以来，随着大数据、云计算和深度学习的发展，AI迎来了新的春天。\n",
      "\n",
      "    今天，AI已经在图像识别、自...\n",
      "2. 人工智能的发展历程可以追溯到20世纪50年代。当时，计算机科学家开始探索让机器模拟人类智能的可能性。\n",
      "\n",
      "    在1956年的达特茅斯会议上，人工智能这个术语首次被正式提出。这标志着AI作为一个独立学...\n",
      "\n",
      "5.3 集成检索器\n",
      "\n",
      "5.4 多查询检索器\n",
      "\n",
      "============================================================\n",
      "6. 完整RAG流程示例\n",
      "============================================================\n",
      "RAG流程执行失败: Failed to connect to Ollama. Please check that Ollama is downloaded, running and accessible. https://ollama.com/download\n",
      "\n",
      "============================================================\n",
      "7. 高级功能示例\n",
      "============================================================\n",
      "\n",
      "7.1 自定义文档加载器\n",
      "自定义加载器文档数量: 3\n",
      "\n",
      "7.2 文档过滤和预处理\n",
      "预处理后文档数量: 1\n",
      "\n",
      "🎉 所有示例运行完成！\n",
      "🧹 临时文件已清理\n"
     ]
    }
   ],
   "execution_count": 11
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
