{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 3. Embedding Models ç¤ºä¾‹",
   "id": "8c936db5daa65279"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T04:52:14.218680Z",
     "start_time": "2025-07-23T04:52:13.636641Z"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": 34,
   "source": [
    "\"\"\"\n",
    "LangChain 0.3 Embedding Models å®Œæ•´ç¤ºä¾‹\n",
    "åŒ…å«æ‰€æœ‰ä¸»è¦åµŒå…¥æ¨¡å‹å’Œé«˜çº§ç”¨æ³•\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import asyncio\n",
    "from typing import List, Dict, Any, Optional\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import time\n",
    "\n",
    "# æ ¸å¿ƒåµŒå…¥æ¨¡å‹å¯¼å…¥\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.embeddings import (\n",
    "    HuggingFaceEmbeddings,\n",
    "    HuggingFaceInstructEmbeddings,\n",
    "    SentenceTransformerEmbeddings,\n",
    "    CohereEmbeddings,\n",
    "    BedrockEmbeddings\n",
    ")\n",
    "from langchain_core.documents import Document"
   ],
   "id": "d4fbb25d56977fe7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### OllamaåµŒå…¥æ¨¡å‹ç¤ºä¾‹",
   "id": "1478a9e2b009c891"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T04:57:51.681456Z",
     "start_time": "2025-07-23T04:57:51.478789Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "1. OllamaåµŒå…¥æ¨¡å‹ç¤ºä¾‹\n",
      "============================================================\n",
      "\n",
      "1.1 åŸºç¡€OllamaåµŒå…¥æ¨¡å‹\n",
      "ç”Ÿæˆæ–‡æ¡£åµŒå…¥...\n",
      "æ–‡æ¡£åµŒå…¥æ•°é‡: 5\n",
      "åµŒå…¥å‘é‡ç»´åº¦: 768\n",
      "æŸ¥è¯¢åµŒå…¥ç»´åº¦: 768\n",
      "\n",
      "1.2 è¯­ä¹‰ç›¸ä¼¼åº¦è®¡ç®—\n",
      "æŸ¥è¯¢: 'ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½æŠ€æœ¯ï¼Ÿ'\n",
      "ä¸å„æ–‡æ¡£çš„ç›¸ä¼¼åº¦:\n",
      "1. 0.8601 - äººå·¥æ™ºèƒ½æ˜¯è®¡ç®—æœºç§‘å­¦çš„åˆ†æ”¯\n",
      "2. 0.5262 - æœºå™¨å­¦ä¹ æ˜¯AIçš„é‡è¦ç»„æˆéƒ¨åˆ†\n",
      "3. 0.5862 - æ·±åº¦å­¦ä¹ ä½¿ç”¨ç¥ç»ç½‘ç»œè¿›è¡Œå­¦ä¹ \n",
      "4. 0.7732 - è‡ªç„¶è¯­è¨€å¤„ç†è®©è®¡ç®—æœºç†è§£äººç±»è¯­è¨€\n",
      "5. 0.5400 - ä»Šå¤©å¤©æ°”å¾ˆå¥½ï¼Œé€‚åˆå‡ºé—¨æ•£æ­¥\n",
      "\n",
      "æœ€ç›¸ä¼¼æ–‡æ¡£: äººå·¥æ™ºèƒ½æ˜¯è®¡ç®—æœºç§‘å­¦çš„åˆ†æ”¯ (ç›¸ä¼¼åº¦: 0.8601)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "OllamaEmbeddings(model='nomic-embed-text:latest', validate_model_on_init=False, base_url='http://localhost:11434', client_kwargs={}, async_client_kwargs={}, sync_client_kwargs={}, mirostat=None, mirostat_eta=None, mirostat_tau=None, num_ctx=None, num_gpu=None, keep_alive=None, num_thread=None, repeat_last_n=None, repeat_penalty=None, temperature=None, stop=None, tfs_z=None, top_k=None, top_p=None)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 41,
   "source": [
    "\n",
    "def ollama_embeddings_example():\n",
    "    \"\"\"OllamaåµŒå…¥æ¨¡å‹ç¤ºä¾‹ - æœ¬åœ°éƒ¨ç½²\"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"1. OllamaåµŒå…¥æ¨¡å‹ç¤ºä¾‹\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    try:\n",
    "        # 1.1 åŸºç¡€OllamaåµŒå…¥\n",
    "        print(\"\\n1.1 åŸºç¡€OllamaåµŒå…¥æ¨¡å‹\")\n",
    "        embeddings = OllamaEmbeddings(\n",
    "            base_url=\"http://localhost:11434\",\n",
    "            model=\"nomic-embed-text:latest\",  # æ¨èçš„åµŒå…¥æ¨¡å‹\n",
    "        )\n",
    "\n",
    "        # æµ‹è¯•æ–‡æœ¬\n",
    "        texts = [\n",
    "            \"äººå·¥æ™ºèƒ½æ˜¯è®¡ç®—æœºç§‘å­¦çš„åˆ†æ”¯\",\n",
    "            \"æœºå™¨å­¦ä¹ æ˜¯AIçš„é‡è¦ç»„æˆéƒ¨åˆ†\",\n",
    "            \"æ·±åº¦å­¦ä¹ ä½¿ç”¨ç¥ç»ç½‘ç»œè¿›è¡Œå­¦ä¹ \",\n",
    "            \"è‡ªç„¶è¯­è¨€å¤„ç†è®©è®¡ç®—æœºç†è§£äººç±»è¯­è¨€\",\n",
    "            \"ä»Šå¤©å¤©æ°”å¾ˆå¥½ï¼Œé€‚åˆå‡ºé—¨æ•£æ­¥\"\n",
    "        ]\n",
    "\n",
    "        # ç”Ÿæˆæ–‡æ¡£åµŒå…¥\n",
    "        print(\"ç”Ÿæˆæ–‡æ¡£åµŒå…¥...\")\n",
    "        doc_embeddings = embeddings.embed_documents(texts)\n",
    "        print(f\"æ–‡æ¡£åµŒå…¥æ•°é‡: {len(doc_embeddings)}\")\n",
    "        print(f\"åµŒå…¥å‘é‡ç»´åº¦: {len(doc_embeddings[0])}\")\n",
    "\n",
    "        # ç”ŸæˆæŸ¥è¯¢åµŒå…¥\n",
    "        query = \"ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½æŠ€æœ¯ï¼Ÿ\"\n",
    "        query_embedding = embeddings.embed_query(query)\n",
    "        print(f\"æŸ¥è¯¢åµŒå…¥ç»´åº¦: {len(query_embedding)}\")\n",
    "\n",
    "        # 1.2 è®¡ç®—ç›¸ä¼¼åº¦\n",
    "        print(\"\\n1.2 è¯­ä¹‰ç›¸ä¼¼åº¦è®¡ç®—\")\n",
    "\n",
    "        def cosine_similarity(a: List[float], b: List[float]) -> float:\n",
    "            \"\"\"è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦\"\"\"\n",
    "            a_np = np.array(a)\n",
    "            b_np = np.array(b)\n",
    "            return np.dot(a_np, b_np) / (np.linalg.norm(a_np) * np.linalg.norm(b_np))\n",
    "\n",
    "        print(f\"æŸ¥è¯¢: '{query}'\")\n",
    "        print(\"ä¸å„æ–‡æ¡£çš„ç›¸ä¼¼åº¦:\")\n",
    "        similarities = []\n",
    "        for i, text in enumerate(texts):\n",
    "            similarity = cosine_similarity(query_embedding, doc_embeddings[i])\n",
    "            similarities.append((text, similarity))\n",
    "            print(f\"{i+1}. {similarity:.4f} - {text}\")\n",
    "\n",
    "        # æ’åºæ˜¾ç¤ºæœ€ç›¸ä¼¼çš„æ–‡æ¡£\n",
    "        similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "        print(f\"\\næœ€ç›¸ä¼¼æ–‡æ¡£: {similarities[0][0]} (ç›¸ä¼¼åº¦: {similarities[0][1]:.4f})\")\n",
    "\n",
    "        # # 1.3 ä¸åŒOllamaæ¨¡å‹å¯¹æ¯”\n",
    "        # print(\"\\n1.3 ä¸åŒOllamaåµŒå…¥æ¨¡å‹å¯¹æ¯”\")\n",
    "        # ollama_models = [\n",
    "        #     \"nomic-embed-text\",\n",
    "        #     \"mxbai-embed-large\",\n",
    "        #     \"all-minilm\"\n",
    "        # ]\n",
    "        #\n",
    "        # for model_name in ollama_models:\n",
    "        #     try:\n",
    "        #         model_embeddings = OllamaEmbeddings(\n",
    "        #             base_url=\"http://localhost:11434\",\n",
    "        #             model=model_name\n",
    "        #         )\n",
    "        #         test_embedding = model_embeddings.embed_query(\"æµ‹è¯•æ–‡æœ¬\")\n",
    "        #         print(f\"{model_name}: ç»´åº¦ {len(test_embedding)}\")\n",
    "        #     except Exception as e:\n",
    "        #         print(f\"{model_name}: ä¸å¯ç”¨ ({str(e)[:50]}...)\")\n",
    "\n",
    "        return embeddings\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"OllamaåµŒå…¥æ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {e}\")\n",
    "        print(\"è¯·ç¡®ä¿OllamaæœåŠ¡æ­£åœ¨è¿è¡Œå¹¶å®‰è£…äº†åµŒå…¥æ¨¡å‹\")\n",
    "        print(\"å®‰è£…å‘½ä»¤: ollama pull nomic-embed-text\")\n",
    "        return None\n",
    "ollama_embeddings_example()"
   ],
   "id": "769df0a1bb7091b0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### OpenAIåµŒå…¥æ¨¡å‹ç¤ºä¾‹",
   "id": "29bacfbeaad20aea"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "def openai_embeddings_example():\n",
    "    \"\"\"OpenAIåµŒå…¥æ¨¡å‹ç¤ºä¾‹\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"2. OpenAIåµŒå…¥æ¨¡å‹ç¤ºä¾‹\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    try:\n",
    "        # 2.1 åŸºç¡€OpenAIåµŒå…¥\n",
    "        print(\"\\n2.1 åŸºç¡€OpenAIåµŒå…¥\")\n",
    "        embeddings = OpenAIEmbeddings(\n",
    "            model=\"text-embedding-3-small\",  # æ–°ç‰ˆæœ¬æ¨¡å‹\n",
    "            api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "            dimensions=1536  # å¯é€‰ï¼šæŒ‡å®šç»´åº¦\n",
    "        )\n",
    "\n",
    "        texts = [\n",
    "            \"Artificial intelligence is a branch of computer science\",\n",
    "            \"Machine learning is a subset of AI\",\n",
    "            \"Deep learning uses neural networks\",\n",
    "            \"Natural language processing enables computers to understand human language\"\n",
    "        ]\n",
    "\n",
    "        doc_embeddings = embeddings.embed_documents(texts)\n",
    "        query_embedding = embeddings.embed_query(\"What is artificial intelligence?\")\n",
    "\n",
    "        print(f\"OpenAIåµŒå…¥ç»´åº¦: {len(doc_embeddings[0])}\")\n",
    "\n",
    "        # 2.2 ä¸åŒOpenAIæ¨¡å‹å¯¹æ¯”\n",
    "        print(\"\\n2.2 OpenAIæ¨¡å‹å¯¹æ¯”\")\n",
    "        openai_models = [\n",
    "            (\"text-embedding-3-small\", 1536),\n",
    "            (\"text-embedding-3-large\", 3072),\n",
    "            (\"text-embedding-ada-002\", 1536)\n",
    "        ]\n",
    "\n",
    "        for model_name, default_dim in openai_models:\n",
    "            try:\n",
    "                model_embeddings = OpenAIEmbeddings(\n",
    "                    model=model_name,\n",
    "                    api_key=os.getenv(\"OPENAI_API_KEY\")\n",
    "                )\n",
    "                test_embedding = model_embeddings.embed_query(\"test\")\n",
    "                print(f\"{model_name}: ç»´åº¦ {len(test_embedding)}\")\n",
    "            except Exception as e:\n",
    "                print(f\"{model_name}: ä¸å¯ç”¨ ({str(e)[:50]}...)\")\n",
    "\n",
    "        # 2.3 è‡ªå®šä¹‰ç»´åº¦ï¼ˆä»…æ”¯æŒtext-embedding-3ç³»åˆ—ï¼‰\n",
    "        print(\"\\n2.3 è‡ªå®šä¹‰åµŒå…¥ç»´åº¦\")\n",
    "        try:\n",
    "            custom_embeddings = OpenAIEmbeddings(\n",
    "                model=\"text-embedding-3-large\",\n",
    "                dimensions=1024,  # è‡ªå®šä¹‰ç»´åº¦\n",
    "                api_key=os.getenv(\"OPENAI_API_KEY\")\n",
    "            )\n",
    "            custom_embedding = custom_embeddings.embed_query(\"è‡ªå®šä¹‰ç»´åº¦æµ‹è¯•\")\n",
    "            print(f\"è‡ªå®šä¹‰ç»´åº¦åµŒå…¥: {len(custom_embedding)}\")\n",
    "        except Exception as e:\n",
    "            print(f\"è‡ªå®šä¹‰ç»´åº¦å¤±è´¥: {e}\")\n",
    "\n",
    "        return embeddings\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"OpenAIåµŒå…¥æ¨¡å‹å¤±è´¥: {e}\")\n",
    "        print(\"è¯·è®¾ç½®OPENAI_API_KEYç¯å¢ƒå˜é‡\")\n",
    "        return None"
   ],
   "id": "ff95b065bdae4c17"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### HuggingFaceåµŒå…¥æ¨¡å‹ç¤ºä¾‹",
   "id": "334b02a06201fb7a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "def huggingface_embeddings_example():\n",
    "    \"\"\"HuggingFaceåµŒå…¥æ¨¡å‹ç¤ºä¾‹\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"3. HuggingFaceåµŒå…¥æ¨¡å‹ç¤ºä¾‹\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # 3.1 åŸºç¡€HuggingFaceåµŒå…¥\n",
    "    print(\"\\n3.1 åŸºç¡€HuggingFaceåµŒå…¥\")\n",
    "    try:\n",
    "        # ä½¿ç”¨é¢„è®­ç»ƒçš„sentence-transformersæ¨¡å‹\n",
    "        embeddings = HuggingFaceEmbeddings(\n",
    "            model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "            model_kwargs={'device': 'cpu'},  # æˆ– 'cuda' å¦‚æœæœ‰GPU\n",
    "            encode_kwargs={'normalize_embeddings': True}  # æ ‡å‡†åŒ–åµŒå…¥\n",
    "        )\n",
    "\n",
    "        texts = [\n",
    "            \"è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•æ–‡æ¡£\",\n",
    "            \"äººå·¥æ™ºèƒ½æŠ€æœ¯å‘å±•è¿…é€Ÿ\",\n",
    "            \"æœºå™¨å­¦ä¹ ç®—æ³•å¾ˆé‡è¦\"\n",
    "        ]\n",
    "\n",
    "        doc_embeddings = embeddings.embed_documents(texts)\n",
    "        query_embedding = embeddings.embed_query(\"AIæŠ€æœ¯\")\n",
    "\n",
    "        print(f\"HuggingFaceåµŒå…¥ç»´åº¦: {len(doc_embeddings[0])}\")\n",
    "\n",
    "        # 3.2 ä¸­æ–‡ä¼˜åŒ–æ¨¡å‹\n",
    "        print(\"\\n3.2 ä¸­æ–‡ä¼˜åŒ–åµŒå…¥æ¨¡å‹\")\n",
    "        chinese_models = [\n",
    "            \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\",\n",
    "            \"sentence-transformers/distiluse-base-multilingual-cased\",\n",
    "            \"BAAI/bge-small-zh-v1.5\"  # ä¸­æ–‡ä¼˜åŒ–æ¨¡å‹\n",
    "        ]\n",
    "\n",
    "        for model_name in chinese_models:\n",
    "            try:\n",
    "                chinese_embeddings = HuggingFaceEmbeddings(\n",
    "                    model_name=model_name,\n",
    "                    model_kwargs={'device': 'cpu'}\n",
    "                )\n",
    "                test_embedding = chinese_embeddings.embed_query(\"ä¸­æ–‡æµ‹è¯•\")\n",
    "                print(f\"{model_name}: ç»´åº¦ {len(test_embedding)}\")\n",
    "            except Exception as e:\n",
    "                print(f\"{model_name}: åŠ è½½å¤±è´¥ ({str(e)[:50]}...)\")\n",
    "\n",
    "        # 3.3 æŒ‡ä»¤ä¼˜åŒ–åµŒå…¥\n",
    "        print(\"\\n3.3 æŒ‡ä»¤ä¼˜åŒ–åµŒå…¥æ¨¡å‹\")\n",
    "        try:\n",
    "            instruct_embeddings = HuggingFaceInstructEmbeddings(\n",
    "                model_name=\"hkunlp/instructor-xl\",\n",
    "                model_kwargs={'device': 'cpu'}\n",
    "            )\n",
    "\n",
    "            # ä½¿ç”¨æŒ‡ä»¤å‰ç¼€\n",
    "            query_instruction = \"ä¸ºè¿™ä¸ªæŸ¥è¯¢æ‰¾åˆ°æœ€ç›¸å…³çš„æ–‡æ¡£: \"\n",
    "            doc_instruction = \"è¿™æ˜¯ä¸€ä¸ªå…³äºæŠ€æœ¯çš„æ–‡æ¡£: \"\n",
    "\n",
    "            instruct_query = instruct_embeddings.embed_query(\n",
    "                query_instruction + \"äººå·¥æ™ºèƒ½åº”ç”¨\"\n",
    "            )\n",
    "            print(f\"æŒ‡ä»¤åµŒå…¥ç»´åº¦: {len(instruct_query)}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"æŒ‡ä»¤åµŒå…¥æ¨¡å‹åŠ è½½å¤±è´¥: {e}\")\n",
    "\n",
    "        return embeddings\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"HuggingFaceåµŒå…¥æ¨¡å‹å¤±è´¥: {e}\")\n",
    "        return None"
   ],
   "id": "395eab448816770b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### SentenceTransformersåµŒå…¥æ¨¡å‹ç¤ºä¾‹",
   "id": "b75cf90894756952"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "def sentence_transformers_example():\n",
    "    \"\"\"SentenceTransformersåµŒå…¥æ¨¡å‹ç¤ºä¾‹\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"4. SentenceTransformersåµŒå…¥æ¨¡å‹ç¤ºä¾‹\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    try:\n",
    "        # 4.1 å¤šè¯­è¨€æ¨¡å‹\n",
    "        print(\"\\n4.1 å¤šè¯­è¨€SentenceTransformers\")\n",
    "        multilingual_embeddings = SentenceTransformerEmbeddings(\n",
    "            model_name=\"sentence-transformers/paraphrase-multilingual-mpnet-base-v2\"\n",
    "        )\n",
    "\n",
    "        # å¤šè¯­è¨€æµ‹è¯•\n",
    "        multilingual_texts = [\n",
    "            \"Hello, how are you?\",\n",
    "            \"ä½ å¥½ï¼Œä½ å¥½å—ï¼Ÿ\",\n",
    "            \"Hola, Â¿cÃ³mo estÃ¡s?\",\n",
    "            \"Bonjour, comment allez-vous?\"\n",
    "        ]\n",
    "\n",
    "        multi_embeddings = multilingual_embeddings.embed_documents(multilingual_texts)\n",
    "        print(f\"å¤šè¯­è¨€åµŒå…¥ç»´åº¦: {len(multi_embeddings[0])}\")\n",
    "\n",
    "        # è®¡ç®—è·¨è¯­è¨€ç›¸ä¼¼åº¦\n",
    "        english_query = multilingual_embeddings.embed_query(\"greeting\")\n",
    "        chinese_query = multilingual_embeddings.embed_query(\"é—®å€™\")\n",
    "\n",
    "        def cosine_similarity(a, b):\n",
    "            return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "\n",
    "        cross_lang_similarity = cosine_similarity(english_query, chinese_query)\n",
    "        print(f\"è·¨è¯­è¨€ç›¸ä¼¼åº¦ (greeting vs é—®å€™): {cross_lang_similarity:.4f}\")\n",
    "\n",
    "        # 4.2 ä¸“ä¸šé¢†åŸŸæ¨¡å‹\n",
    "        print(\"\\n4.2 ä¸“ä¸šé¢†åŸŸåµŒå…¥æ¨¡å‹\")\n",
    "        domain_models = [\n",
    "            \"sentence-transformers/all-mpnet-base-v2\",  # é€šç”¨\n",
    "            \"sentence-transformers/msmarco-distilbert-base-v4\",  # æœç´¢ä¼˜åŒ–\n",
    "            \"sentence-transformers/nli-mpnet-base-v2\"  # è‡ªç„¶è¯­è¨€æ¨ç†\n",
    "        ]\n",
    "\n",
    "        for model_name in domain_models:\n",
    "            try:\n",
    "                domain_embeddings = SentenceTransformerEmbeddings(model_name=model_name)\n",
    "                test_embedding = domain_embeddings.embed_query(\"domain test\")\n",
    "                print(f\"{model_name.split('/')[-1]}: ç»´åº¦ {len(test_embedding)}\")\n",
    "            except Exception as e:\n",
    "                print(f\"{model_name}: ä¸å¯ç”¨\")\n",
    "\n",
    "        return multilingual_embeddings\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"SentenceTransformerså¤±è´¥: {e}\")\n",
    "        return None"
   ],
   "id": "8abba28e47559ba8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### äº‘ç«¯åµŒå…¥æ¨¡å‹ç¤ºä¾‹",
   "id": "6b986e96796163bb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "def cloud_embeddings_example():\n",
    "    \"\"\"äº‘ç«¯åµŒå…¥æ¨¡å‹ç¤ºä¾‹\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"5. äº‘ç«¯åµŒå…¥æ¨¡å‹ç¤ºä¾‹\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # 5.1 CohereåµŒå…¥\n",
    "    print(\"\\n5.1 CohereåµŒå…¥æ¨¡å‹\")\n",
    "    try:\n",
    "        cohere_embeddings = CohereEmbeddings(\n",
    "            cohere_api_key=os.getenv(\"COHERE_API_KEY\"),\n",
    "            model=\"embed-english-v3.0\"  # æˆ– embed-multilingual-v3.0\n",
    "        )\n",
    "\n",
    "        cohere_texts = [\"AI technology\", \"Machine learning algorithms\"]\n",
    "        cohere_embeds = cohere_embeddings.embed_documents(cohere_texts)\n",
    "        print(f\"CohereåµŒå…¥ç»´åº¦: {len(cohere_embeds[0])}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"CohereåµŒå…¥å¤±è´¥: {e}\")\n",
    "\n",
    "    # 5.2 AWS BedrockåµŒå…¥\n",
    "    print(\"\\n5.2 AWS BedrockåµŒå…¥æ¨¡å‹\")\n",
    "    try:\n",
    "        bedrock_embeddings = BedrockEmbeddings(\n",
    "            credentials_profile_name=\"default\",\n",
    "            region_name=\"us-east-1\",\n",
    "            model_id=\"amazon.titan-embed-text-v1\"\n",
    "        )\n",
    "\n",
    "        bedrock_texts = [\"Cloud computing\", \"Serverless architecture\"]\n",
    "        bedrock_embeds = bedrock_embeddings.embed_documents(bedrock_texts)\n",
    "        print(f\"BedrockåµŒå…¥ç»´åº¦: {len(bedrock_embeds[0])}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"BedrockåµŒå…¥å¤±è´¥: {e}\")"
   ],
   "id": "f315da9b9cabc7d3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### åµŒå…¥æ¨¡å‹æ€§èƒ½å¯¹æ¯”",
   "id": "3ebebe9c3f803e5e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "def embedding_performance_comparison():\n",
    "    \"\"\"åµŒå…¥æ¨¡å‹æ€§èƒ½å¯¹æ¯”\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"6. åµŒå…¥æ¨¡å‹æ€§èƒ½å¯¹æ¯”\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # æµ‹è¯•æ–‡æœ¬\n",
    "    test_texts = [\n",
    "        \"äººå·¥æ™ºèƒ½æŠ€æœ¯æ­£åœ¨å¿«é€Ÿå‘å±•\",\n",
    "        \"æœºå™¨å­¦ä¹ ç®—æ³•åœ¨å„ä¸ªé¢†åŸŸéƒ½æœ‰åº”ç”¨\",\n",
    "        \"æ·±åº¦å­¦ä¹ æ¨¡å‹éœ€è¦å¤§é‡çš„è®­ç»ƒæ•°æ®\",\n",
    "        \"è‡ªç„¶è¯­è¨€å¤„ç†è®©è®¡ç®—æœºç†è§£äººç±»è¯­è¨€\",\n",
    "        \"è®¡ç®—æœºè§†è§‰æŠ€æœ¯å¯ä»¥è¯†åˆ«å›¾åƒä¸­çš„ç‰©ä½“\"\n",
    "    ]\n",
    "\n",
    "    test_query = \"AIæŠ€æœ¯çš„åº”ç”¨é¢†åŸŸ\"\n",
    "\n",
    "    # å®šä¹‰è¦æµ‹è¯•çš„æ¨¡å‹\n",
    "    models_to_test = []\n",
    "\n",
    "    # Ollamaæ¨¡å‹\n",
    "    try:\n",
    "        ollama_model = OllamaEmbeddings(\n",
    "            base_url=\"http://localhost:11434\",\n",
    "            model=\"nomic-embed-text\"\n",
    "        )\n",
    "        models_to_test.append((\"Ollama-nomic\", ollama_model))\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # HuggingFaceæ¨¡å‹\n",
    "    try:\n",
    "        hf_model = HuggingFaceEmbeddings(\n",
    "            model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "        )\n",
    "        models_to_test.append((\"HF-MiniLM\", hf_model))\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # æ€§èƒ½æµ‹è¯•\n",
    "    results = {}\n",
    "\n",
    "    for model_name, model in models_to_test:\n",
    "        try:\n",
    "            print(f\"\\næµ‹è¯• {model_name}...\")\n",
    "\n",
    "            # æµ‹è¯•æ–‡æ¡£åµŒå…¥æ—¶é—´\n",
    "            start_time = time.time()\n",
    "            doc_embeddings = model.embed_documents(test_texts)\n",
    "            doc_time = time.time() - start_time\n",
    "\n",
    "            # æµ‹è¯•æŸ¥è¯¢åµŒå…¥æ—¶é—´\n",
    "            start_time = time.time()\n",
    "            query_embedding = model.embed_query(test_query)\n",
    "            query_time = time.time() - start_time\n",
    "\n",
    "            # è®¡ç®—ç›¸ä¼¼åº¦\n",
    "            similarities = []\n",
    "            for doc_emb in doc_embeddings:\n",
    "                sim = np.dot(query_embedding, doc_emb) / (\n",
    "                    np.linalg.norm(query_embedding) * np.linalg.norm(doc_emb)\n",
    "                )\n",
    "                similarities.append(sim)\n",
    "\n",
    "            results[model_name] = {\n",
    "                \"dimension\": len(doc_embeddings[0]),\n",
    "                \"doc_time\": doc_time,\n",
    "                \"query_time\": query_time,\n",
    "                \"avg_similarity\": np.mean(similarities),\n",
    "                \"max_similarity\": np.max(similarities)\n",
    "            }\n",
    "\n",
    "        except Exception as e:\n",
    "            results[model_name] = {\"error\": str(e)}\n",
    "\n",
    "    # æ˜¾ç¤ºç»“æœ\n",
    "    print(\"\\næ€§èƒ½å¯¹æ¯”ç»“æœ:\")\n",
    "    print(f\"{'æ¨¡å‹':<15} {'ç»´åº¦':<8} {'æ–‡æ¡£æ—¶é—´(s)':<12} {'æŸ¥è¯¢æ—¶é—´(s)':<12} {'å¹³å‡ç›¸ä¼¼åº¦':<12} {'æœ€é«˜ç›¸ä¼¼åº¦':<12}\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "    for model_name, result in results.items():\n",
    "        if \"error\" not in result:\n",
    "            print(f\"{model_name:<15} {result['dimension']:<8} {result['doc_time']:<12.4f} \"\n",
    "                  f\"{result['query_time']:<12.4f} {result['avg_similarity']:<12.4f} \"\n",
    "                  f\"{result['max_similarity']:<12.4f}\")\n",
    "        else:\n",
    "            print(f\"{model_name:<15} é”™è¯¯: {result['error'][:50]}...\")"
   ],
   "id": "1b314261935393b3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### é«˜çº§åµŒå…¥æŠ€æœ¯",
   "id": "d7337cc0e609f98d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "def advanced_embedding_techniques():\n",
    "    \"\"\"é«˜çº§åµŒå…¥æŠ€æœ¯\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"7. é«˜çº§åµŒå…¥æŠ€æœ¯\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # 7.1 åµŒå…¥ç¼“å­˜\n",
    "    print(\"\\n7.1 åµŒå…¥ç¼“å­˜æœºåˆ¶\")\n",
    "\n",
    "    class CachedEmbeddings:\n",
    "        \"\"\"å¸¦ç¼“å­˜çš„åµŒå…¥æ¨¡å‹\"\"\"\n",
    "\n",
    "        def __init__(self, base_embeddings):\n",
    "            self.base_embeddings = base_embeddings\n",
    "            self.cache = {}\n",
    "\n",
    "        def embed_query(self, text: str) -> List[float]:\n",
    "            if text in self.cache:\n",
    "                print(f\"ç¼“å­˜å‘½ä¸­: {text[:30]}...\")\n",
    "                return self.cache[text]\n",
    "\n",
    "            embedding = self.base_embeddings.embed_query(text)\n",
    "            self.cache[text] = embedding\n",
    "            print(f\"æ–°è®¡ç®—: {text[:30]}...\")\n",
    "            return embedding\n",
    "\n",
    "        def embed_documents(self, texts: List[str]) -> List[List[float]]:\n",
    "            embeddings = []\n",
    "            for text in texts:\n",
    "                embeddings.append(self.embed_query(text))\n",
    "            return embeddings\n",
    "\n",
    "    # ä½¿ç”¨ç¼“å­˜åµŒå…¥\n",
    "    try:\n",
    "        base_model = HuggingFaceEmbeddings(\n",
    "            model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "        )\n",
    "        cached_model = CachedEmbeddings(base_model)\n",
    "\n",
    "        # ç¬¬ä¸€æ¬¡è®¡ç®—\n",
    "        test_texts = [\"AIæŠ€æœ¯\", \"æœºå™¨å­¦ä¹ \", \"AIæŠ€æœ¯\"]  # é‡å¤æ–‡æœ¬\n",
    "        embeddings1 = cached_model.embed_documents(test_texts)\n",
    "\n",
    "        # ç¬¬äºŒæ¬¡è®¡ç®—ï¼ˆåº”è¯¥ä½¿ç”¨ç¼“å­˜ï¼‰\n",
    "        embeddings2 = cached_model.embed_documents(test_texts)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"ç¼“å­˜åµŒå…¥ç¤ºä¾‹å¤±è´¥: {e}\")\n",
    "\n",
    "    # 7.2 æ‰¹é‡å¤„ç†ä¼˜åŒ–\n",
    "    print(\"\\n7.2 æ‰¹é‡å¤„ç†ä¼˜åŒ–\")\n",
    "\n",
    "    def batch_embed_documents(embeddings_model, texts: List[str], batch_size: int = 32):\n",
    "        \"\"\"æ‰¹é‡å¤„ç†åµŒå…¥\"\"\"\n",
    "        all_embeddings = []\n",
    "\n",
    "        for i in range(0, len(texts), batch_size):\n",
    "            batch = texts[i:i + batch_size]\n",
    "            print(f\"å¤„ç†æ‰¹æ¬¡ {i//batch_size + 1}: {len(batch)} ä¸ªæ–‡æ¡£\")\n",
    "\n",
    "            batch_embeddings = embeddings_model.embed_documents(batch)\n",
    "            all_embeddings.extend(batch_embeddings)\n",
    "\n",
    "        return all_embeddings\n",
    "\n",
    "    # 7.3 å¼‚æ­¥åµŒå…¥å¤„ç†\n",
    "    print(\"\\n7.3 å¼‚æ­¥åµŒå…¥å¤„ç†\")\n",
    "\n",
    "    async def async_embed_documents(embeddings_model, texts: List[str]):\n",
    "        \"\"\"å¼‚æ­¥å¤„ç†åµŒå…¥\"\"\"\n",
    "        loop = asyncio.get_event_loop()\n",
    "\n",
    "        # å°†æ–‡æœ¬åˆ†ç»„\n",
    "        chunk_size = len(texts) // 4 + 1\n",
    "        tasks = []\n",
    "\n",
    "        for i in range(0, len(texts), chunk_size):\n",
    "            chunk = texts[i:i + chunk_size]\n",
    "            task = loop.run_in_executor(\n",
    "                None,\n",
    "                embeddings_model.embed_documents,\n",
    "                chunk\n",
    "            )\n",
    "            tasks.append(task)\n",
    "\n",
    "        # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ\n",
    "        results = await asyncio.gather(*tasks)\n",
    "\n",
    "        # åˆå¹¶ç»“æœ\n",
    "        all_embeddings = []\n",
    "        for result in results:\n",
    "            all_embeddings.extend(result)\n",
    "\n",
    "        return all_embeddings"
   ],
   "id": "13a8746b7d065d9f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### åµŒå…¥è´¨é‡è¯„ä¼°",
   "id": "c7f06da450b1cc92"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "def embedding_quality_evaluation():\n",
    "    \"\"\"åµŒå…¥è´¨é‡è¯„ä¼°\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"8. åµŒå…¥è´¨é‡è¯„ä¼°\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # 8.1 è¯­ä¹‰ç›¸ä¼¼åº¦æµ‹è¯•\n",
    "    print(\"\\n8.1 è¯­ä¹‰ç›¸ä¼¼åº¦æµ‹è¯•\")\n",
    "\n",
    "    # å®šä¹‰æµ‹è¯•ç”¨ä¾‹\n",
    "    similarity_tests = [\n",
    "        (\"äººå·¥æ™ºèƒ½\", \"AIæŠ€æœ¯\", \"é«˜ç›¸ä¼¼åº¦\"),\n",
    "        (\"æœºå™¨å­¦ä¹ \", \"æ·±åº¦å­¦ä¹ \", \"ä¸­ç­‰ç›¸ä¼¼åº¦\"),\n",
    "        (\"è®¡ç®—æœº\", \"è‹¹æœ\", \"ä½ç›¸ä¼¼åº¦\"),\n",
    "        (\"ç‹—\", \"çŒ«\", \"ä¸­ç­‰ç›¸ä¼¼åº¦\"),\n",
    "        (\"æ±½è½¦\", \"é£æœº\", \"ä½ç›¸ä¼¼åº¦\")\n",
    "    ]\n",
    "\n",
    "    try:\n",
    "        embeddings = HuggingFaceEmbeddings(\n",
    "            model_name=\"sentence-transformers/paraphrase-multilingual-mpnet-base-v2\"\n",
    "        )\n",
    "\n",
    "        print(\"è¯­ä¹‰ç›¸ä¼¼åº¦æµ‹è¯•ç»“æœ:\")\n",
    "        for text1, text2, expected in similarity_tests:\n",
    "            emb1 = embeddings.embed_query(text1)\n",
    "            emb2 = embeddings.embed_query(text2)\n",
    "\n",
    "            similarity = np.dot(emb1, emb2) / (np.linalg.norm(emb1) * np.linalg.norm(emb2))\n",
    "            print(f\"{text1} vs {text2}: {similarity:.4f} ({expected})\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"è¯­ä¹‰ç›¸ä¼¼åº¦æµ‹è¯•å¤±è´¥: {e}\")\n",
    "\n",
    "    # 8.2 èšç±»è´¨é‡è¯„ä¼°\n",
    "    print(\"\\n8.2 èšç±»è´¨é‡è¯„ä¼°\")\n",
    "\n",
    "    def evaluate_clustering_quality(embeddings_model, texts: List[str], labels: List[str]):\n",
    "        \"\"\"è¯„ä¼°èšç±»è´¨é‡\"\"\"\n",
    "        try:\n",
    "            from sklearn.cluster import KMeans\n",
    "            from sklearn.metrics import adjusted_rand_score\n",
    "\n",
    "            # ç”ŸæˆåµŒå…¥\n",
    "            embeddings = embeddings_model.embed_documents(texts)\n",
    "            embeddings_array = np.array(embeddings)\n",
    "\n",
    "            # æ‰§è¡Œèšç±»\n",
    "            n_clusters = len(set(labels))\n",
    "            kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "            predicted_labels = kmeans.fit_predict(embeddings_array)\n",
    "\n",
    "            # è®¡ç®—è°ƒæ•´å…°å¾·æŒ‡æ•°\n",
    "            ari_score = adjusted_rand_score(labels, predicted_labels)\n",
    "            print(f\"èšç±»è´¨é‡ (ARI): {ari_score:.4f}\")\n",
    "\n",
    "            return ari_score\n",
    "\n",
    "        except ImportError:\n",
    "            print(\"éœ€è¦å®‰è£…scikit-learn: pip install scikit-learn\")\n",
    "        except Exception as e:\n",
    "            print(f\"èšç±»è¯„ä¼°å¤±è´¥: {e}\")"
   ],
   "id": "62de51fc2e10a4e0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### è‡ªå®šä¹‰åµŒå…¥åŒ…è£…å™¨",
   "id": "66c3a63584771c0f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "def custom_embedding_wrapper():\n",
    "    \"\"\"è‡ªå®šä¹‰åµŒå…¥åŒ…è£…å™¨\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"9. è‡ªå®šä¹‰åµŒå…¥åŒ…è£…å™¨\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    from langchain_core.embeddings import Embeddings\n",
    "\n",
    "    class MultiModelEmbeddings(Embeddings):\n",
    "        \"\"\"å¤šæ¨¡å‹é›†æˆåµŒå…¥\"\"\"\n",
    "\n",
    "        def __init__(self, models: List[Embeddings], weights: Optional[List[float]] = None):\n",
    "            self.models = models\n",
    "            self.weights = weights or [1.0] * len(models)\n",
    "\n",
    "            # æ ‡å‡†åŒ–æƒé‡\n",
    "            total_weight = sum(self.weights)\n",
    "            self.weights = [w / total_weight for w in self.weights]\n",
    "\n",
    "        def embed_documents(self, texts: List[str]) -> List[List[float]]:\n",
    "            \"\"\"é›†æˆå¤šä¸ªæ¨¡å‹çš„æ–‡æ¡£åµŒå…¥\"\"\"\n",
    "            all_embeddings = []\n",
    "\n",
    "            # è·å–æ¯ä¸ªæ¨¡å‹çš„åµŒå…¥\n",
    "            model_embeddings = []\n",
    "            for model in self.models:\n",
    "                embeddings = model.embed_documents(texts)\n",
    "                model_embeddings.append(embeddings)\n",
    "\n",
    "            # åŠ æƒå¹³å‡\n",
    "            for i in range(len(texts)):\n",
    "                combined_embedding = np.zeros(len(model_embeddings[0][i]))\n",
    "\n",
    "                for j, (embeddings, weight) in enumerate(zip(model_embeddings, self.weights)):\n",
    "                    combined_embedding += np.array(embeddings[i]) * weight\n",
    "\n",
    "                all_embeddings.append(combined_embedding.tolist())\n",
    "\n",
    "            return all_embeddings\n",
    "\n",
    "        def embed_query(self, text: str) -> List[float]:\n",
    "            \"\"\"é›†æˆå¤šä¸ªæ¨¡å‹çš„æŸ¥è¯¢åµŒå…¥\"\"\"\n",
    "            embeddings = self.embed_documents([text])\n",
    "            return embeddings[0]\n",
    "\n",
    "    # ä½¿ç”¨ç¤ºä¾‹\n",
    "    try:\n",
    "        # åˆ›å»ºå¤šä¸ªåŸºç¡€æ¨¡å‹\n",
    "        model1 = HuggingFaceEmbeddings(\n",
    "            model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "        )\n",
    "\n",
    "        # å¦‚æœæœ‰å¤šä¸ªæ¨¡å‹å¯ç”¨\n",
    "        models = [model1]  # å¯ä»¥æ·»åŠ æ›´å¤šæ¨¡å‹\n",
    "        weights = [1.0]    # å¯¹åº”çš„æƒé‡\n",
    "\n",
    "        multi_embeddings = MultiModelEmbeddings(models, weights)\n",
    "\n",
    "        test_text = \"å¤šæ¨¡å‹åµŒå…¥æµ‹è¯•\"\n",
    "        result = multi_embeddings.embed_query(test_text)\n",
    "        print(f\"å¤šæ¨¡å‹åµŒå…¥ç»´åº¦: {len(result)}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"å¤šæ¨¡å‹åµŒå…¥å¤±è´¥: {e}\")"
   ],
   "id": "3cbc9a21bdf18a08"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "def main():\n",
    "    \"\"\"è¿è¡Œæ‰€æœ‰åµŒå…¥æ¨¡å‹ç¤ºä¾‹\"\"\"\n",
    "    print(\"ğŸš€ LangChain 0.3 Embedding Models å®Œæ•´ç¤ºä¾‹\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    # è¿è¡Œæ‰€æœ‰ç¤ºä¾‹\n",
    "    ollama_embeddings = ollama_embeddings_example()\n",
    "    openai_embeddings = openai_embeddings_example()\n",
    "    hf_embeddings = huggingface_embeddings_example()\n",
    "    st_embeddings = sentence_transformers_example()\n",
    "    cloud_embeddings_example()\n",
    "    embedding_performance_comparison()\n",
    "    advanced_embedding_techniques()\n",
    "    embedding_quality_evaluation()\n",
    "    custom_embedding_wrapper()\n",
    "\n",
    "    print(\"\\nğŸ‰ æ‰€æœ‰åµŒå…¥æ¨¡å‹ç¤ºä¾‹è¿è¡Œå®Œæˆï¼\")\n",
    "\n",
    "    # æœ€ä½³å®è·µå»ºè®®\n",
    "    print(\"\\nğŸ“‹ åµŒå…¥æ¨¡å‹é€‰æ‹©å»ºè®®:\")\n",
    "    print(\"1. æœ¬åœ°éƒ¨ç½²ï¼šOllama + nomic-embed-text\")\n",
    "    print(\"2. äº‘ç«¯æœåŠ¡ï¼šOpenAI text-embedding-3-small\")\n",
    "    print(\"3. å¼€æºæ–¹æ¡ˆï¼šHuggingFace sentence-transformers\")\n",
    "    print(\"4. ä¸­æ–‡ä¼˜åŒ–ï¼šBAAI/bge-small-zh-v1.5\")\n",
    "    print(\"5. å¤šè¯­è¨€ï¼šparaphrase-multilingual-mpnet-base-v2\")\n",
    "    print(\"6. é«˜æ€§èƒ½ï¼štext-embedding-3-large\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "id": "31b1c1b39e98e6bc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T06:54:26.718459Z",
     "start_time": "2025-07-23T06:54:26.199599Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "3. Embedding Models åµŒå…¥æ¨¡å‹ç¤ºä¾‹\n",
      "============================================================\n",
      "\n",
      "3.1 OllamaåµŒå…¥æ¨¡å‹\n",
      "æ–‡æ¡£åµŒå…¥æ•°é‡: 4\n",
      "åµŒå…¥å‘é‡ç»´åº¦: 768\n",
      "æŸ¥è¯¢åµŒå…¥ç»´åº¦: 768\n",
      "\n",
      "ç›¸ä¼¼åº¦è®¡ç®—:\n",
      "'äººå·¥æ™ºèƒ½æ˜¯è®¡ç®—æœºç§‘å­¦çš„åˆ†æ”¯' ç›¸ä¼¼åº¦: 0.8551\n",
      "'æœºå™¨å­¦ä¹ æ˜¯AIçš„å­é›†' ç›¸ä¼¼åº¦: 0.6135\n",
      "'æ·±åº¦å­¦ä¹ ä½¿ç”¨ç¥ç»ç½‘ç»œ' ç›¸ä¼¼åº¦: 0.5818\n",
      "'ä»Šå¤©å¤©æ°”å¾ˆå¥½' ç›¸ä¼¼åº¦: 0.5851\n"
     ]
    }
   ],
   "execution_count": 42,
   "source": [
    "# 3. Embedding Models ç¤ºä¾‹\n",
    "def embedding_models_example():\n",
    "    \"\"\"åµŒå…¥æ¨¡å‹ç¤ºä¾‹\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"3. Embedding Models åµŒå…¥æ¨¡å‹ç¤ºä¾‹\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # 3.1 OllamaåµŒå…¥æ¨¡å‹\n",
    "    print(\"\\n3.1 OllamaåµŒå…¥æ¨¡å‹\")\n",
    "    try:\n",
    "        embeddings = OllamaEmbeddings(\n",
    "            base_url=\"http://localhost:11434\",\n",
    "            model=\"nomic-embed-text\"  # æˆ–ä½¿ç”¨å…¶ä»–åµŒå…¥æ¨¡å‹\n",
    "        )\n",
    "\n",
    "        # æµ‹è¯•æ–‡æœ¬\n",
    "        texts = [\n",
    "            \"äººå·¥æ™ºèƒ½æ˜¯è®¡ç®—æœºç§‘å­¦çš„åˆ†æ”¯\",\n",
    "            \"æœºå™¨å­¦ä¹ æ˜¯AIçš„å­é›†\",\n",
    "            \"æ·±åº¦å­¦ä¹ ä½¿ç”¨ç¥ç»ç½‘ç»œ\",\n",
    "            \"ä»Šå¤©å¤©æ°”å¾ˆå¥½\"\n",
    "        ]\n",
    "\n",
    "        # ç”ŸæˆåµŒå…¥å‘é‡\n",
    "        text_embeddings = embeddings.embed_documents(texts)\n",
    "        query_embedding = embeddings.embed_query(\"ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿ\")\n",
    "\n",
    "        print(f\"æ–‡æ¡£åµŒå…¥æ•°é‡: {len(text_embeddings)}\")\n",
    "        print(f\"åµŒå…¥å‘é‡ç»´åº¦: {len(text_embeddings[0])}\")\n",
    "        print(f\"æŸ¥è¯¢åµŒå…¥ç»´åº¦: {len(query_embedding)}\")\n",
    "\n",
    "        # è®¡ç®—ç›¸ä¼¼åº¦\n",
    "        import numpy as np\n",
    "\n",
    "        def cosine_similarity(a, b):\n",
    "            return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "\n",
    "        print(\"\\nç›¸ä¼¼åº¦è®¡ç®—:\")\n",
    "        for i, text in enumerate(texts):\n",
    "            similarity = cosine_similarity(query_embedding, text_embeddings[i])\n",
    "            print(f\"'{text}' ç›¸ä¼¼åº¦: {similarity:.4f}\")\n",
    "\n",
    "        return embeddings\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"OllamaåµŒå…¥æ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {e}\")\n",
    "        print(\"è¯·ç¡®ä¿OllamaæœåŠ¡æ­£åœ¨è¿è¡Œå¹¶å®‰è£…äº†åµŒå…¥æ¨¡å‹\")\n",
    "        return None\n",
    "# 3. åµŒå…¥æ¨¡å‹\n",
    "embeddings = embedding_models_example()"
   ],
   "id": "70e94f0eb1a52a05"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
