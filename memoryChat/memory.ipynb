{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 使用传统的chains",
   "id": "f1f4619d0c40e1df"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-21T09:07:04.247450Z",
     "start_time": "2025-07-21T09:06:48.302932Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始对话测试:\n",
      "用户: 你好！\n",
      "AI: 你好！很高兴见到你。有什么问题或话题想要讨论吗？我可以帮你解答关于科技、文化、日常生活等方面的问题，或者我们一起聊聊最近的新闻和趣闻。如果你有任何疑问或是想了解的事情，都可以随时问我哦。\n",
      "用户: 我的名字是小明\n",
      "AI: 很高兴认识你，小明！请问有什么问题或话题想要讨论吗？无论是科技、文化、日常生活方面的问题，还是最近的新闻和趣闻，我都很乐意与你分享。如果你有任何疑问或是想了解的事情，都可以随时问我哦。\n",
      "用户: 我的名字是什么？\n",
      "AI: 你的名字是小明。如果你有其他问题或话题想要讨论，也可以随时告诉我哦！\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import Ollama\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "# 初始化Ollama模型\n",
    "llm = Ollama(model=\"qwen2.5:3b\")  # 可以替换为您安装的任何模型，如\"gemma:2b\"、\"mistral\"等\n",
    "\n",
    "# 创建对话记忆\n",
    "memory = ConversationBufferMemory(return_messages=True)\n",
    "\n",
    "# 创建对话链\n",
    "conversation = ConversationChain(\n",
    "    llm=llm,\n",
    "    memory=memory,\n",
    "    # verbose=True\n",
    ")\n",
    "\n",
    "# 测试多轮对话\n",
    "print(\"开始对话测试:\")\n",
    "\n",
    "# 第一轮对话\n",
    "user_input = \"你好！\"\n",
    "print(f\"用户: {user_input}\")\n",
    "response = conversation.predict(input=user_input)\n",
    "print(f\"AI: {response}\")\n",
    "\n",
    "# 第二轮对话\n",
    "user_input = \"我的名字是小明\"\n",
    "print(f\"用户: {user_input}\")\n",
    "response = conversation.predict(input=user_input)\n",
    "print(f\"AI: {response}\")\n",
    "\n",
    "# 第三轮对话 - 测试记忆功能\n",
    "user_input = \"我的名字是什么？\"\n",
    "print(f\"用户: {user_input}\")\n",
    "response = conversation.predict(input=user_input)\n",
    "print(f\"AI: {response}\")\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 各种记忆组件",
   "id": "e887fada4775f506"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4141dc81",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-21T09:09:50.697538Z",
     "start_time": "2025-07-21T09:09:43.309808Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "使用 buffer 记忆类型进行对话测试:\n",
      "\n",
      "用户: 我的名字叫小明\n",
      "AI: 好的，我已经记住了您提供的信息。您可以继续与我交流，我会记得之前的信息来帮助我们更好地互动。无论是聊天、提供信息还是其他任何你能想到的事情，请随时告诉我！\n",
      "\n",
      "用户: 我的名字是什么?\n",
      "AI: 你好，小明！很高兴认识你。你可以开始和我分享你的想法或者问题，我会尽力帮助你。\n",
      "\n",
      "记忆中存储的内容:\n",
      "{'history': [HumanMessage(content='我的名字叫小明', additional_kwargs={}, response_metadata={}), AIMessage(content='好的，我已经记住了您提供的信息。您可以继续与我交流，我会记得之前的信息来帮助我们更好地互动。无论是聊天、提供信息还是其他任何你能想到的事情，请随时告诉我！', additional_kwargs={}, response_metadata={}), HumanMessage(content='我的名字是什么?', additional_kwargs={}, response_metadata={}), AIMessage(content='你好，小明！很高兴认识你。你可以开始和我分享你的想法或者问题，我会尽力帮助你。', additional_kwargs={}, response_metadata={})]}\n"
     ]
    }
   ],
   "source": [
    "# 高级多轮对话记忆示例\n",
    "\n",
    "from langchain_ollama import OllamaLLM  # 使用新的推荐导入方式\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.memory import ConversationBufferMemory, ConversationSummaryMemory, ConversationBufferWindowMemory\n",
    "\n",
    "# 初始化Ollama模型\n",
    "llm = OllamaLLM(model=\"qwen2.5:3b\")\n",
    "\n",
    "# 创建自定义提示模板\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    SystemMessage(content=\"你是一个有记忆能力的智能助手。请记住用户告诉你的信息，并在后续对话中使用这些信息。\"),\n",
    "    MessagesPlaceholder(variable_name=\"history\"),\n",
    "    HumanMessage(content=\"{input}\")\n",
    "])\n",
    "\n",
    "# 1. 标准对话缓冲记忆 - 保存所有对话历史\n",
    "buffer_memory = ConversationBufferMemory(return_messages=True, memory_key=\"history\")\n",
    "\n",
    "# 2. 对话摘要记忆 - 保存对话摘要而不是完整历史\n",
    "summary_memory = ConversationSummaryMemory(llm=llm, return_messages=True, memory_key=\"history\")\n",
    "\n",
    "# 3. 对话窗口记忆 - 只保留最近的k轮对话\n",
    "window_memory = ConversationBufferWindowMemory(k=2, return_messages=True, memory_key=\"history\")\n",
    "\n",
    "# 创建对话链 - 选择一种记忆类型\n",
    "memory_type = \"buffer\"  # 可选: \"buffer\", \"summary\", \"window\"\n",
    "\n",
    "if memory_type == \"buffer\":\n",
    "    memory = buffer_memory\n",
    "elif memory_type == \"summary\":\n",
    "    memory = summary_memory\n",
    "else:\n",
    "    memory = window_memory\n",
    "\n",
    "conversation = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=prompt,\n",
    "    memory=memory,\n",
    "    # verbose=True\n",
    ")\n",
    "\n",
    "# 测试多轮对话\n",
    "print(f\"\\n使用 {memory_type} 记忆类型进行对话测试:\")\n",
    "\n",
    "# 模拟多轮对话\n",
    "conversations = [\n",
    "    \"我的名字叫小明\",\n",
    "    \"我的名字是什么?\",\n",
    "]\n",
    "\n",
    "for user_input in conversations:\n",
    "    print(f\"\\n用户: {user_input}\")\n",
    "    response = conversation.predict(input=user_input)\n",
    "    print(f\"AI: {response}\")\n",
    "    \n",
    "# 查看内存中存储的内容\n",
    "print(\"\\n记忆中存储的内容:\")\n",
    "print(memory.load_memory_variables({}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84aea41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FastAPI集成示例 - 多轮对话记忆\n",
    "\n",
    "import os\n",
    "import json\n",
    "from typing import Dict, List, Any, Optional\n",
    "from pydantic import BaseModel, Field\n",
    "from fastapi import FastAPI, HTTPException, Depends\n",
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "from langchain_ollama import OllamaLLM\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "# 数据模型\n",
    "class Message(BaseModel):\n",
    "    role: str\n",
    "    content: str\n",
    "\n",
    "class ChatRequest(BaseModel):\n",
    "    user_id: str\n",
    "    message: str\n",
    "    conversation_id: Optional[str] = None\n",
    "\n",
    "class ChatResponse(BaseModel):\n",
    "    conversation_id: str\n",
    "    message: str\n",
    "    history: List[Message]\n",
    "\n",
    "# 内存中的会话存储\n",
    "conversations = {}\n",
    "\n",
    "# 创建一个简单的会话管理器\n",
    "class ConversationManager:\n",
    "    def __init__(self, model_name: str = \"qwen2.5:3b\"):\n",
    "        self.model_name = model_name\n",
    "        self.llm = OllamaLLM(model=model_name)\n",
    "        self.conversations = {}\n",
    "    \n",
    "    def _get_or_create_conversation(self, conversation_id: str, user_id: str) -> Dict:\n",
    "        \"\"\"获取或创建会话\"\"\"\n",
    "        if conversation_id not in self.conversations:\n",
    "            memory = ConversationBufferMemory(return_messages=True, memory_key=\"history\")\n",
    "            \n",
    "            prompt = ChatPromptTemplate.from_messages([\n",
    "                SystemMessage(content=\"你是一个有记忆能力的智能助手。请记住用户告诉你的信息，并在后续对话中使用这些信息。\"),\n",
    "                MessagesPlaceholder(variable_name=\"history\"),\n",
    "                HumanMessage(content=\"{input}\")\n",
    "            ])\n",
    "            \n",
    "            conversation = LLMChain(\n",
    "                llm=self.llm,\n",
    "                prompt=prompt,\n",
    "                memory=memory,\n",
    "                verbose=True\n",
    "            )\n",
    "            \n",
    "            self.conversations[conversation_id] = {\n",
    "                \"user_id\": user_id,\n",
    "                \"conversation\": conversation,\n",
    "                \"memory\": memory,\n",
    "                \"messages\": []\n",
    "            }\n",
    "        \n",
    "        return self.conversations[conversation_id]\n",
    "    \n",
    "    def process_message(self, user_id: str, message: str, conversation_id: Optional[str] = None) -> Dict:\n",
    "        \"\"\"处理用户消息并返回响应\"\"\"\n",
    "        # 如果没有提供会话ID，则创建一个新的\n",
    "        if not conversation_id:\n",
    "            conversation_id = f\"conv_{len(self.conversations) + 1}\"\n",
    "        \n",
    "        # 获取或创建会话\n",
    "        conv_data = self._get_or_create_conversation(conversation_id, user_id)\n",
    "        \n",
    "        # 获取对话链\n",
    "        conversation = conv_data[\"conversation\"]\n",
    "        \n",
    "        # 获取AI响应\n",
    "        response = conversation.predict(input=message)\n",
    "        \n",
    "        # 更新消息历史\n",
    "        conv_data[\"messages\"].append(Message(role=\"user\", content=message))\n",
    "        conv_data[\"messages\"].append(Message(role=\"assistant\", content=response))\n",
    "        \n",
    "        return {\n",
    "            \"conversation_id\": conversation_id,\n",
    "            \"message\": response,\n",
    "            \"history\": conv_data[\"messages\"]\n",
    "        }\n",
    "    \n",
    "    def get_conversation_history(self, conversation_id: str) -> List[Message]:\n",
    "        \"\"\"获取会话历史\"\"\"\n",
    "        if conversation_id not in self.conversations:\n",
    "            return []\n",
    "        \n",
    "        return self.conversations[conversation_id][\"messages\"]\n",
    "    \n",
    "    def delete_conversation(self, conversation_id: str) -> bool:\n",
    "        \"\"\"删除会话\"\"\"\n",
    "        if conversation_id in self.conversations:\n",
    "            del self.conversations[conversation_id]\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "# 创建FastAPI应用和会话管理器实例\n",
    "app = FastAPI(title=\"对话记忆API\")\n",
    "conversation_manager = ConversationManager()\n",
    "\n",
    "# 添加CORS中间件\n",
    "app.add_middleware(\n",
    "    CORSMiddleware,\n",
    "    allow_origins=[\"*\"],\n",
    "    allow_credentials=True,\n",
    "    allow_methods=[\"*\"],\n",
    "    allow_headers=[\"*\"],\n",
    ")\n",
    "\n",
    "# API端点\n",
    "@app.post(\"/chat\", response_model=ChatResponse)\n",
    "async def chat(request: ChatRequest):\n",
    "    \"\"\"处理聊天请求\"\"\"\n",
    "    response = conversation_manager.process_message(\n",
    "        user_id=request.user_id,\n",
    "        message=request.message,\n",
    "        conversation_id=request.conversation_id\n",
    "    )\n",
    "    return response\n",
    "\n",
    "@app.get(\"/conversations/{conversation_id}\")\n",
    "async def get_conversation(conversation_id: str):\n",
    "    \"\"\"获取会话历史\"\"\"\n",
    "    history = conversation_manager.get_conversation_history(conversation_id)\n",
    "    if not history:\n",
    "        raise HTTPException(status_code=404, detail=\"Conversation not found\")\n",
    "    return {\"conversation_id\": conversation_id, \"history\": history}\n",
    "\n",
    "@app.delete(\"/conversations/{conversation_id}\")\n",
    "async def delete_conversation(conversation_id: str):\n",
    "    \"\"\"删除会话\"\"\"\n",
    "    success = conversation_manager.delete_conversation(conversation_id)\n",
    "    if not success:\n",
    "        raise HTTPException(status_code=404, detail=\"Conversation not found\")\n",
    "    return {\"message\": \"Conversation deleted successfully\"}\n",
    "\n",
    "# FastAPI应用启动代码\n",
    "'''\n",
    "if __name__ == \"__main__\":\n",
    "    import uvicorn\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n",
    "'''\n",
    "\n",
    "# 模拟API调用\n",
    "print(\"\\n模拟FastAPI对话记忆API调用:\")\n",
    "\n",
    "# 创建一个会话\n",
    "user_id = \"user123\"\n",
    "conversation_id = None\n",
    "\n",
    "# 第一条消息\n",
    "request1 = ChatRequest(user_id=user_id, message=\"你好，我是小明\", conversation_id=conversation_id)\n",
    "response1 = conversation_manager.process_message(\n",
    "    user_id=request1.user_id,\n",
    "    message=request1.message,\n",
    "    conversation_id=request1.conversation_id\n",
    ")\n",
    "conversation_id = response1[\"conversation_id\"]\n",
    "print(f\"\\n用户: {request1.message}\")\n",
    "print(f\"AI: {response1['message']}\")\n",
    "\n",
    "# 第二条消息\n",
    "request2 = ChatRequest(user_id=user_id, message=\"我今年25岁，喜欢打篮球\", conversation_id=conversation_id)\n",
    "response2 = conversation_manager.process_message(\n",
    "    user_id=request2.user_id,\n",
    "    message=request2.message,\n",
    "    conversation_id=request2.conversation_id\n",
    ")\n",
    "print(f\"\\n用户: {request2.message}\")\n",
    "print(f\"AI: {response2['message']}\")\n",
    "\n",
    "# 第三条消息 - 测试记忆\n",
    "request3 = ChatRequest(user_id=user_id, message=\"你还记得我的名字和爱好吗？\", conversation_id=conversation_id)\n",
    "response3 = conversation_manager.process_message(\n",
    "    user_id=request3.user_id,\n",
    "    message=request3.message,\n",
    "    conversation_id=request3.conversation_id\n",
    ")\n",
    "print(f\"\\n用户: {request3.message}\")\n",
    "print(f\"AI: {response3['message']}\")\n",
    "\n",
    "# 显示会话历史\n",
    "print(\"\\n会话历史:\")\n",
    "history = conversation_manager.get_conversation_history(conversation_id)\n",
    "for msg in history:\n",
    "    print(f\"{msg.role.capitalize()}: {msg.content}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bc68490007a4539",
   "metadata": {},
   "source": [
    "# 多轮对话记忆功能总结\n",
    "\n",
    "## 记忆类型比较\n",
    "\n",
    "| 记忆类型 | 描述 | 适用场景 | 优点 | 缺点 |\n",
    "|---------|------|---------|------|------|\n",
    "| ConversationBufferMemory | 保存完整对话历史 | 短期对话，需要完整上下文 | 实现简单，保留所有信息 | 随着对话增长，上下文窗口可能溢出 |\n",
    "| ConversationBufferWindowMemory | 只保留最近k轮对话 | 长期对话，但只关注最近上下文 | 控制上下文大小，减少token消耗 | 可能丢失早期重要信息 |\n",
    "| ConversationSummaryMemory | 保存对话摘要而非完整历史 | 长期对话，需要压缩历史信息 | 大幅减少token消耗 | 可能丢失细节信息，依赖LLM摘要质量 |\n",
    "| VectorStoreRetrieverMemory | 使用向量存储和检索相关记忆 | 长期对话，需要选择性记忆检索 | 可扩展性强，适合大规模对话 | 实现复杂，依赖嵌入质量 |\n",
    "| EntityMemory | 专门记住实体信息 | 需要记住用户特定属性信息 | 结构化存储用户信息 | 需要额外的实体提取逻辑 |\n",
    "| PersistentConversationMemory | 将对话历史保存到文件中 | 需要跨会话保持记忆 | 支持持久化，可恢复对话 | 需要文件I/O操作 |\n",
    "\n",
    "## 最佳实践\n",
    "\n",
    "1. **选择合适的记忆类型**：根据应用场景选择合适的记忆类型，或组合多种记忆类型\n",
    "2. **优化提示模板**：在提示中明确指导模型如何使用记忆信息\n",
    "3. **记忆管理**：实现记忆清理和过期机制，避免记忆过载\n",
    "4. **持久化存储**：对重要对话实现持久化存储\n",
    "5. **记忆压缩**：对长期记忆进行摘要或压缩\n",
    "6. **相关性检索**：使用向量存储实现相关性记忆检索\n",
    "\n",
    "## 实现步骤\n",
    "\n",
    "1. 选择并初始化记忆组件\n",
    "2. 创建适当的提示模板\n",
    "3. 配置对话链或代理\n",
    "4. 实现记忆管理逻辑\n",
    "5. 处理用户输入并生成响应\n",
    "6. 保存对话历史到记忆中\n",
    "\n",
    "## 进阶功能\n",
    "\n",
    "1. **多用户记忆隔离**：为不同用户维护独立的记忆空间\n",
    "2. **记忆优先级**：对重要信息赋予更高的记忆优先级\n",
    "3. **记忆检索增强**：结合关键词和语义检索\n",
    "4. **记忆反思**：让AI定期反思和总结记忆内容\n",
    "5. **记忆可视化**：为用户提供记忆内容的可视化界面\n",
    "\n",
    "## 注意事项\n",
    "\n",
    "1. 注意隐私保护，不要存储敏感信息\n",
    "2. 实现记忆过期和清理机制\n",
    "3. 控制记忆大小，避免token消耗过大\n",
    "4. 定期评估记忆质量和相关性\n",
    "5. 提供用户控制记忆的选项（如删除特定记忆）\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "a8a4180994bf6148",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-21T09:18:25.498343Z",
     "start_time": "2025-07-21T09:18:22.727084Z"
    }
   },
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage\n",
    "from langchain_ollama import OllamaLLM\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
    "\n",
    "# 初始化模型\n",
    "llm = OllamaLLM(model=\"qwen2.5:3b\")\n",
    "\n",
    "# 创建记忆组件\n",
    "memory = ConversationBufferMemory(return_messages=True, memory_key=\"history\")\n",
    "\n",
    "# 创建提示模板\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    SystemMessage(content=\"你是一个有记忆能力的智能助手。请记住用户告诉你的信息，并在后续对话中使用这些信息。\"),\n",
    "    MessagesPlaceholder(variable_name=\"history\"),\n",
    "    HumanMessage(content=\"{input}\")\n",
    "])\n",
    "\n",
    "# 使用LCEL构建对话链\n",
    "chain = (\n",
    "    RunnablePassthrough.assign(\n",
    "        history=lambda _: memory.load_memory_variables({})[\"history\"]\n",
    "    )\n",
    "    | prompt\n",
    "    | llm\n",
    ")\n",
    "\n",
    "# 处理对话函数\n",
    "def process_message(user_input):\n",
    "    response = chain.invoke({\"input\": user_input})\n",
    "    # 修复：直接使用response，不访问content属性\n",
    "    memory.save_context({\"input\": user_input}, {\"output\": response})\n",
    "    return response\n",
    "\n",
    "# 测试对话\n",
    "print(\"开始对话测试:\")\n",
    "\n",
    "# 第一轮对话\n",
    "user_input = \"你好，我叫小明\"\n",
    "print(f\"用户: {user_input}\")\n",
    "response = process_message(user_input)\n",
    "print(f\"AI: {response}\")\n",
    "\n",
    "# 第二轮对话\n",
    "user_input = \"我今年25岁\"\n",
    "print(f\"用户: {user_input}\")\n",
    "response = process_message(user_input)\n",
    "print(f\"AI: {response}\")\n",
    "\n",
    "# 第三轮对话 - 测试记忆功能\n",
    "user_input = \"你还记得我的名字吗？\"\n",
    "print(f\"用户: {user_input}\")\n",
    "response = process_message(user_input)\n",
    "print(f\"AI: {response}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始对话测试:\n",
      "用户: 你好，我叫小明\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'content'",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mAttributeError\u001B[39m                            Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[11]\u001B[39m\u001B[32m, line 43\u001B[39m\n\u001B[32m     41\u001B[39m user_input = \u001B[33m\"\u001B[39m\u001B[33m你好，我叫小明\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m     42\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m用户: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00muser_input\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m)\n\u001B[32m---> \u001B[39m\u001B[32m43\u001B[39m response = \u001B[43mprocess_message\u001B[49m\u001B[43m(\u001B[49m\u001B[43muser_input\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     44\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mAI: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mresponse\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m)\n\u001B[32m     46\u001B[39m \u001B[38;5;66;03m# 第二轮对话\u001B[39;00m\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[11]\u001B[39m\u001B[32m, line 32\u001B[39m, in \u001B[36mprocess_message\u001B[39m\u001B[34m(user_input)\u001B[39m\n\u001B[32m     31\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mprocess_message\u001B[39m(user_input):\n\u001B[32m---> \u001B[39m\u001B[32m32\u001B[39m     response = \u001B[43mchain\u001B[49m\u001B[43m.\u001B[49m\u001B[43minvoke\u001B[49m\u001B[43m(\u001B[49m\u001B[43m{\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43minput\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43muser_input\u001B[49m\u001B[43m}\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     33\u001B[39m     \u001B[38;5;66;03m# 修复：直接使用response，不访问content属性\u001B[39;00m\n\u001B[32m     34\u001B[39m     memory.save_context({\u001B[33m\"\u001B[39m\u001B[33minput\u001B[39m\u001B[33m\"\u001B[39m: user_input}, {\u001B[33m\"\u001B[39m\u001B[33moutput\u001B[39m\u001B[33m\"\u001B[39m: response})\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/FastAPIProject/.venv/lib/python3.11/site-packages/langchain_core/runnables/base.py:3046\u001B[39m, in \u001B[36mRunnableSequence.invoke\u001B[39m\u001B[34m(self, input, config, **kwargs)\u001B[39m\n\u001B[32m   3044\u001B[39m                 input_ = context.run(step.invoke, input_, config, **kwargs)\n\u001B[32m   3045\u001B[39m             \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m3046\u001B[39m                 input_ = context.run(step.invoke, input_, config)\n\u001B[32m   3047\u001B[39m \u001B[38;5;66;03m# finish the root run\u001B[39;00m\n\u001B[32m   3048\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/FastAPIProject/.venv/lib/python3.11/site-packages/langchain_core/runnables/base.py:4775\u001B[39m, in \u001B[36mRunnableLambda.invoke\u001B[39m\u001B[34m(self, input, config, **kwargs)\u001B[39m\n\u001B[32m   4761\u001B[39m \u001B[38;5;250m\u001B[39m\u001B[33;03m\"\"\"Invoke this Runnable synchronously.\u001B[39;00m\n\u001B[32m   4762\u001B[39m \n\u001B[32m   4763\u001B[39m \u001B[33;03mArgs:\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m   4772\u001B[39m \u001B[33;03m    TypeError: If the Runnable is a coroutine function.\u001B[39;00m\n\u001B[32m   4773\u001B[39m \u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m   4774\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[33m\"\u001B[39m\u001B[33mfunc\u001B[39m\u001B[33m\"\u001B[39m):\n\u001B[32m-> \u001B[39m\u001B[32m4775\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_with_config\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   4776\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_invoke\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   4777\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m   4778\u001B[39m \u001B[43m        \u001B[49m\u001B[43mensure_config\u001B[49m\u001B[43m(\u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   4779\u001B[39m \u001B[43m        \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   4780\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   4781\u001B[39m msg = \u001B[33m\"\u001B[39m\u001B[33mCannot invoke a coroutine function synchronously.Use `ainvoke` instead.\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m   4782\u001B[39m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(msg)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/FastAPIProject/.venv/lib/python3.11/site-packages/langchain_core/runnables/base.py:1939\u001B[39m, in \u001B[36mRunnable._call_with_config\u001B[39m\u001B[34m(self, func, input_, config, run_type, serialized, **kwargs)\u001B[39m\n\u001B[32m   1935\u001B[39m     child_config = patch_config(config, callbacks=run_manager.get_child())\n\u001B[32m   1936\u001B[39m     \u001B[38;5;28;01mwith\u001B[39;00m set_config_context(child_config) \u001B[38;5;28;01mas\u001B[39;00m context:\n\u001B[32m   1937\u001B[39m         output = cast(\n\u001B[32m   1938\u001B[39m             \u001B[33m\"\u001B[39m\u001B[33mOutput\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m-> \u001B[39m\u001B[32m1939\u001B[39m             \u001B[43mcontext\u001B[49m\u001B[43m.\u001B[49m\u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1940\u001B[39m \u001B[43m                \u001B[49m\u001B[43mcall_func_with_variable_args\u001B[49m\u001B[43m,\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# type: ignore[arg-type]\u001B[39;49;00m\n\u001B[32m   1941\u001B[39m \u001B[43m                \u001B[49m\u001B[43mfunc\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1942\u001B[39m \u001B[43m                \u001B[49m\u001B[43minput_\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1943\u001B[39m \u001B[43m                \u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1944\u001B[39m \u001B[43m                \u001B[49m\u001B[43mrun_manager\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1945\u001B[39m \u001B[43m                \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1946\u001B[39m \u001B[43m            \u001B[49m\u001B[43m)\u001B[49m,\n\u001B[32m   1947\u001B[39m         )\n\u001B[32m   1948\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[32m   1949\u001B[39m     run_manager.on_chain_error(e)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/FastAPIProject/.venv/lib/python3.11/site-packages/langchain_core/runnables/config.py:428\u001B[39m, in \u001B[36mcall_func_with_variable_args\u001B[39m\u001B[34m(func, input, config, run_manager, **kwargs)\u001B[39m\n\u001B[32m    426\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m run_manager \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m accepts_run_manager(func):\n\u001B[32m    427\u001B[39m     kwargs[\u001B[33m\"\u001B[39m\u001B[33mrun_manager\u001B[39m\u001B[33m\"\u001B[39m] = run_manager\n\u001B[32m--> \u001B[39m\u001B[32m428\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/FastAPIProject/.venv/lib/python3.11/site-packages/langchain_core/runnables/base.py:4633\u001B[39m, in \u001B[36mRunnableLambda._invoke\u001B[39m\u001B[34m(self, input_, run_manager, config, **kwargs)\u001B[39m\n\u001B[32m   4631\u001B[39m                 output = chunk\n\u001B[32m   4632\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m4633\u001B[39m     output = \u001B[43mcall_func_with_variable_args\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   4634\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mfunc\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minput_\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrun_manager\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\n\u001B[32m   4635\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   4636\u001B[39m \u001B[38;5;66;03m# If the output is a Runnable, invoke it\u001B[39;00m\n\u001B[32m   4637\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(output, Runnable):\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/FastAPIProject/.venv/lib/python3.11/site-packages/langchain_core/runnables/config.py:428\u001B[39m, in \u001B[36mcall_func_with_variable_args\u001B[39m\u001B[34m(func, input, config, run_manager, **kwargs)\u001B[39m\n\u001B[32m    426\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m run_manager \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m accepts_run_manager(func):\n\u001B[32m    427\u001B[39m     kwargs[\u001B[33m\"\u001B[39m\u001B[33mrun_manager\u001B[39m\u001B[33m\"\u001B[39m] = run_manager\n\u001B[32m--> \u001B[39m\u001B[32m428\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[11]\u001B[39m\u001B[32m, line 27\u001B[39m, in \u001B[36m<lambda>\u001B[39m\u001B[34m(msg)\u001B[39m\n\u001B[32m     14\u001B[39m prompt = ChatPromptTemplate.from_messages([\n\u001B[32m     15\u001B[39m     SystemMessage(content=\u001B[33m\"\u001B[39m\u001B[33m你是一个有记忆能力的智能助手。请记住用户告诉你的信息，并在后续对话中使用这些信息。\u001B[39m\u001B[33m\"\u001B[39m),\n\u001B[32m     16\u001B[39m     MessagesPlaceholder(variable_name=\u001B[33m\"\u001B[39m\u001B[33mhistory\u001B[39m\u001B[33m\"\u001B[39m),\n\u001B[32m     17\u001B[39m     HumanMessage(content=\u001B[33m\"\u001B[39m\u001B[38;5;132;01m{input}\u001B[39;00m\u001B[33m\"\u001B[39m)\n\u001B[32m     18\u001B[39m ])\n\u001B[32m     20\u001B[39m \u001B[38;5;66;03m# 使用LCEL构建对话链\u001B[39;00m\n\u001B[32m     21\u001B[39m chain = (\n\u001B[32m     22\u001B[39m     RunnablePassthrough.assign(\n\u001B[32m     23\u001B[39m         history=\u001B[38;5;28;01mlambda\u001B[39;00m _: memory.load_memory_variables({})[\u001B[33m\"\u001B[39m\u001B[33mhistory\u001B[39m\u001B[33m\"\u001B[39m]\n\u001B[32m     24\u001B[39m     )\n\u001B[32m     25\u001B[39m     | prompt\n\u001B[32m     26\u001B[39m     | llm\n\u001B[32m---> \u001B[39m\u001B[32m27\u001B[39m     | RunnableLambda(\u001B[38;5;28;01mlambda\u001B[39;00m msg: \u001B[43mmsg\u001B[49m\u001B[43m.\u001B[49m\u001B[43mcontent\u001B[49m)  \u001B[38;5;66;03m# 提取 LLM 回复文本\u001B[39;00m\n\u001B[32m     28\u001B[39m )\n\u001B[32m     30\u001B[39m \u001B[38;5;66;03m# 处理对话函数\u001B[39;00m\n\u001B[32m     31\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mprocess_message\u001B[39m(user_input):\n",
      "\u001B[31mAttributeError\u001B[39m: 'str' object has no attribute 'content'"
     ]
    }
   ],
   "execution_count": 11
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
